# Список задач по курсу Моя первая научная статья, весна 2025

 ## Задача 125 (была)
* **Название:** Изучение влияния гиперпараметров на точность алгоритма агрегирования экспертных прогнозов с переменным числом экспертов
* **Задача:** Рассматривается прогнозирование локально стационарных  временных рядов в режиме онлайн при помощи алгоритма на базе Fixed Share. Данный подход предполагает теоретико-игровую постановку.    Задача заключается в изучении влияния гиперпараметров на качество работы алгоритма в зависимости от свойств временного ряда. 
* **Данные:** Модельные и синтетические временные ряды. 
* **Литература:** 
    - В.В. Вьюгин. Математические основы машинного обучения и прогнозирования, 2022, Изд.3-е, МЦНМО, 400с. (глава 4, в частности разделы 4.7, 4.8)
    - M. Herbster, M. Warmuth. Tracking the best expert.  Machine Learning, 32(2) 151--178, 1998.
    - O. Bousquet, M. Warmuth. Tracking a small set of experts by mixing past posteriors.  Journal of Machine Learning Research. 3:363--396, 2002.
* **Базовый алгоритм:** Метод агрегирования прогнозов [Fixed Share]( http://www.jip.ru/2023/470-487-2023.pdf), метод смешивания апостериорных распределений Mixing Past Posteriors
* **Решение: ** Предлагается написать генератор моделирующий (или синтезирующий) временные ряды с заданными свойствами: количеством промежутков локальной стационарности, частотой переключения и т.д.  На полученных данных поставить ряд экспериментов в с различными гиперпараметрами алгоритма. Сделать вывод об их влиянии. 
* **Авторы:** В.В.  Вьюгин, Р. Д. Зухба
* **Консультант:** А.В. Зухба
* 
## Задача 118  (была) (renew for 2024)
* __Название__ Непрерывное время при построении нейроинтерфейса BCI 
* __Задача__: В задачах декодирования сигнала данные представляются как многомерные временные ряды. При решении задач используется дискретное представление
времени. Однако недавние работы по нейронным обыкновенным дифференциальным уравнениям иллюстрируют возможность работать со скрытым состоянием рекуррентных нейронных сетей, как с решениями дифференциальных уравнений. Это позволяет рассматривать временные ряды как непрерывные по времени. 
* __Данные__: Для классификации:
  * датасет P300, по которому писали статью c Алиной 
  * похожий на него по формату записей датасет DEAPdataset. 
  * найти современный датасет, спросить в U.Grenoble-Alpes
* __Литература__: [Neural CDE](https://bit.ly/NeuroCDE)
* __Базовой алгоритм__: Алгоритм Алины Самохиной, S4, S5, D4 (порождающий)
* __Решение__: Использование вариаций  NeurODE для аппроксимации исходного сигнала. Сравнительный анализ существующих подходов к применению дифференциальных уравнений для классификации EEG. (***Энкодер-тензорное разложение, декодер NeuroCDE***)
* __Новизна__: предлагается способ построения непрервыного представления сигнала. Работа с функциональным пространством сигнала, а не его дискретным представлением. Использование параметров полученной функции в качестве признакового пространства результирующей модели. Основная задача - построить обращаемый поток и указать оптимальные размерности на каждом слое нейросети. 
* **Авторы:** Алина Самохина,   

## Задача 159 (была)
* **Название:** Восстановление функциональных групп головного мозга с помощью графовых диффузных моделей
* **Описание проблемы:** Решается задача построения модели анализа активности головного мозга, учитывающей пространственную структуру сигнала. Данные об активности мозга представлены в виде многомерных временных рядов, считываемых
электродами, расположенными на голове испытуемого одним из универсальных стандартов размещения. Из-за отсутствия регулярного определения окрестности на сферической поверхности мозга классические сверточные нейронные
сети не могут быть эффективно применены для учета пространственной информации. Предлагается использовать графовое представление сигнала, что позволит выявить более сложные взаимосвязи различных областей активности в пространстве и провести нейробиологическую интерпретацию функциональных связей мозга. 
* **Данные:** Юлия Березуцкая, код загрузки у четвертого курса
	- Berezutskaya J., et al Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film // Sci Data 9, 91, 2022.
	- [Код предшественников](https://github.com/intsystems/
* **Литература** Магистерская работа Наталии Вареник
* **Базовый алгоритм:** Graph Neural Diffusion: https://github.com/twitter-research/graph-neural-pde
* **Новизна:** Построить карту функциональных групп с изменением во времени в зависимости от внешнего воздействия (видео Пеппи)
* **Авторы:** Святослав Панченко

## Задача 159 (???) (отложена на 2025-2026 в связи с тем, что нет разработанных методов решения этой задачи)
* **Название:** Прогнозирование деформаций и напряжений в биологических тканях и органах
* **Описание проблемы:** Требуется выбрать модель аппроксимации поля поле деформаций и напряжений в материале под разными условиями. Целью исследования является разработка модели и метода, которые по имеющимся ограниченным экспериментальным данным достоверно восстанавливает поля в материале.
* **Данные:** Измерения деформаций и напряжений, полученные с помощью сенсоров. Данные будут представлены в виде тензора, где каждый элемент соответствует определенному измерению деформации или напряжения в конкретном месте и времени.
* **Литература**
	- https://arxiv.org/abs/2302.06594
	- Что было опубликовано после этой работы вместе со словами Physics-informed learning + tensor analysis
* **Авторы:**

## Задача 157 (???)
* **Название:** Приближение временных рядов стохастическими дифференциальными уравнениями (вставить мотивацию)
* **Описание проблемы:** Требуется построить прогноз набора временных рядов (с высокой ковариацией), вплоть до редсказания аномалий (аномалия означает отказ от прогнозирования, она не относится ни к детерминированой, ни к стохастической составляющей). Требуется декомпозировать реализзцию случайного процесса (временного ряда) и восстановить модель случайного процесса (со сносом, скачками). Требуется выбать адекватную порождабщую нейросеть, указать способ прореживания числа слоёв и снижения их размерности.
* **Данные:**
Выборка: набор фазовых траекторий, реализаций с.п.  (было: Т х Т - временной диапазон х З - значение, которое случ. величина принимала в это время). Требуется построить прогноз. Критерий – минимум свертки мсв с функцией ошибки, адекватной гипотезе порождения данных, или построенной под прикладную задачу.
- EEG PyRiemann https://pyriemann.readthedocs.io
- LOBSTER Trades Quotes and Prices https://lobsterdata.com/tradesquotesandprices
* **Литература**
  - про Neural SDE: [SDE as a GAN](https://arxiv.org/pdf/2102.03657.pdf), [signature kernel scores](https://arxiv.org/pdf/2305.16274.pdf), [gradients](https://arxiv.org/pdf/2105.13493.pdf)
  - обсудить da Prato–Debussche trick (возмущения решений посредством порождающих моделей)
* **Базовый алгоритм:** 
* **Новизна:**  
* **Авторы:** Эдуард Владимиров, Иван Папай 

## Задача 158 (???) (индустриальная)
* **Описание проблемы:**
	- Ранжирование риск-сигналов о признаках развития неправомерной торговой стратегии на финансовом рынке. Задача выявления ранних признаков аномального поведения участника торгов [Данные: набор агрегатов, построенных на обезличенных данных торгов].
	- Классификация субъектов ПНИИИМР, ПОД/ФТ, … . Выявление аномального поведения субъектов, классификация, кластеризация стратегий или профилей финансового поведения [Данные: набор агрегатов, построенных на обезличенных данных торгов].
* **Данные:** ВАЖНО! Требуется найти или синтезировать открытые данные
* **Авторы:** Андрей Сергеевич Инякин

## Задача 157 (???) (индустриальная)
* **Описание проблемы:**
	- Использование (дообучение, «компрессия» / дистиляция, прунинг, квантизация) SOTA LLM/GAN (генеративных моделей) для формирования последовательности тестовых сценариев (тест-кейсов) по заданному набору функциональных требований [Данные: реестр требований, тест-кейсы].
	- Использование (дообучение, «компрессия» / дистиляция, прунинг, квантизация) SOTA LLM/GAN для формирования и актуализации реестра «атомарных» и непротиворечивых функциональных / нефункциональных требований к программной Системе на основе набора функциональных, технических заданий и иных документов [Данные: реестр требований, реестр ФЗ, реестр ТЗ].
* **Данные:** ВАЖНО! Требуется найти или синтезировать открытые данные
* **Авторы:** Андрей Сергеевич Инякин

## Задача 156 (была)
* **Название:** Optimal Gradient Methods with Relative Inexactness
* **Описание проблемы:** A description of your problem, its motivation  and goals. An optimization-style problem statement is welcome
Задача: построить оптимальный метод оптимизации первого порядка с помощью градиентов, зашумлённых относительным шумом. Для этого мы будем использовать технику Programming Performance Estimate, которая позволяет строить такого рода алгоритмы через анализ численного решения задачи Полуопределённого программирования. Необходимые знания: 1) Понимать, что такое выпуклые функции,  и как работаю градиентные методы для поиска их минимумов 2) Опыт Python с numpy
Ожидаемые результаты: Доказанные теоремы сходимости для предложенного методы и экспериментальное подтверждение результатов, публикация в журнал уровня Q1,Q2. Комментарий от автора: 
Конечно, тема не самая простая и результаты не факты, что будут, но я готов уверить, что будет релевантный опыт написание введения, анализа работ и  теории для статьи, постановки и презентации экспериментов и представления научной ценности. Вся необходимая помощь по материалу будет предоставлена.
* **Данные:** Synthetic data 
* **Литература** Рарерs with
	1-2) Основная статья, продолжения первой части которой мы будем писать https://arxiv.org/abs/2310.00506
	3) Введение в PEP https://francisbach.com/computer-aided-analyses/
* **Базовый алгоритм:**  https://github.com/Jhomanik/InterRel
* **Авторы:** Корнилов Никита
  
## Задача 155 (???) (была закомментирована)
* **Название:** Identification of the relationship between labels using an algorithm based on one's own attention for the classification problem with multiple labels, justifying the connection with Hawkes processes.
* **Описание проблемы:** Most of the available user information can be represented as a sequence of events with timestamps. Each event is assigned a set of categorical labels, the future structure of which is of great interest. This is a temporal sets prediction problem for sequential data. Modern approaches focus on the transformation architecture for sequential data, introducing independent attention to the elements in the sequence. In this case, we take into account the temporal interactions of events, but lose information about the interdependencies of labels. Motivated by this disadvantage, we propose to use the mechanism of independent attention to the marks preceding the predicted step. Since our approach is a network of attention to labels, we call it a LANET.  We also justify this aggregation method, it affects the intensity of the event positively, assuming that the intensity is represented by the basic Hawkes process.
* **Данные:** A brief description of data in the computational experiment and. Links to the datasets. The datasets shall be open-source. The data shall be ready-to-model
	- https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data
	- https://www.kaggle.com/c/python-and-analyze-data-final-project/data
Based on the dataset data, we will compare the state of the art solutions in this area with our solution in this problem statement.
* **Литература** 1-2 Predicting Temporal Sets with Deep Neural Networks, Predicting Temporal Sets with Simplified Fully Connected Networks  3 Transformer Hawkes Process, The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process - introduction to idea with process Hawkes
* **Базовый алгоритм:**
	- https://github.com/yule-BUAA/SFCNTSP
	- https://github.com/yule-BUAA/DNNTSP The state of the art methods for temporal sets prediction
* **Новизна:** The idea of the proposed solution and methods for conducting the research. Ways of visualizing data and error analysis
Most of the transformer-related models used for temporal sets prediction use self-attention computation between consecutive input timestamps representations. The LANET instead uses the self-attention between label representations. So, it has the input that consists of K vectors. Below, we describe how to aggregate a sequence of size τ to K vectors via an Embedding layer. Then we define the Self-attention layer. To get the predictions, we apply a Prediction layer.  Also, to justify such aggregation by labels, instead of time dependence, we introduce the concept of Cox processes, which describe the probability of an event at a given time through the integral of the intensity function. And we can show that such aggregation can either not worsen it, or make a positive contribution to probability. LANET will train with the cross-entropy loss adapted for the multi-label task through
independent consideration of each label score. Comparison tables with basic approaches will be carried out, as well as visualization of quality changes from selected parameters, visualization of attention for analyzing the relationship of label representations, as well as tables comparing the effect of vector representations on the result of the model.
* **Авторы:**  Consultant - Andrey Grabovoy, Expert - Alexey Zaytsev, Author of research - Galina Boeva 

## Задача 154 (была)
* **Название:** Декодирования сигналов головного мозга в аудиоданные
* **Описание проблемы:** Декодирования сигналов головного мозга в аудиоданные (чтение мыслей) на выбор из двух математических постановок:
1. Classification problem in a match-mismatch paradigm: the task of the model is to determine which of the input stimulus segments of audio corresponds to the EEG https://exporl.github.io/auditory-eeg-challenge-2024/task1/description/
2. Regression problem: to reconstruct the mel spectrogram from the EEG https://exporl.github.io/auditory-eeg-challenge-2024/task2/description/ 
* **Данные:** Датасет был собран на базе Лёвенского университета - https://rdr.kuleuven.be/dataset.xhtml?persistentId=doi:10.48804/K3VSND Для сбора датаесета были приглашены 85 человек без проблем со слухом и нервной системой, носители бельгийского голландского языка. Измерения производились в звуконепроницаемой лаборатории с помощью высокоточных приборов для снятия ЭЭГ с 64 электродами. Частота дискретезации даных 8192 Гц. Каждому учатнику предлагалось послушать отрывок подкаста или аудиокниги (случайно) длиной до 15 минут. Всего имеем 668 пар ЭЭГ-стимул (прослушанный отрывок) общей продолжительностью 9431 минута
* **Литература**
	1. Pre-LN FFT: Baseline решение с использованием иной архитектуры обработки аудио https://arxiv.org/pdf/2305.06806.pdf  (код https://github.com/jkyunnng/HappyQuokka_system_for_EEG_Challenge)
	2. Линейные модели, FCCN (2022) - https://www.researchgate.net/publication/361380348_Robust_decoding_of_the_speech_envelope_from_EEG_recordings_through_deep_neural_networks
	3. VLAAI (2023, Nature) https://www.nature.com/articles/s41598-022-27332-2
	4. Decoding speech perception from non-invasive brain recordings (2023) https://arxiv.org/pdf/2208.12266.pdf
* **Базовый алгоритм:** Описание baseline решения предоставлено на сайте соревнования: https://exporl.github.io/auditory-eeg-challenge-2024/task2/description/
* **Новизна:** Попробовать современные методы получения эмбеддингов аудиоданных для улучшения бейзлайн решения (например wav2vec, fastspeech2). Цель: показать, что модели, учитывающие законы физики/мира (как например fastspeech2) улучшают качество декодирования сигналов головного мозга в аудиоданные
* **Авторы:** Павел Северилов pseverilov@gmail.com

## Задача 117 (???) (Полностью задача не решена в 2024)
* **Название:** Поиск зависимостей биомеханических системах и (Метод Convergence Cross-Mpping, теорема Такенса)
* **Задача**:  При прогнозировании сложноорганизованных временных рядов, зависящих от экзогенных факторов и имеющих множественную периодичность, требуется решить задачу выявления связанных пар рядов. Предполагается, что добавление этих рядов в модель повышает качество прогноза. В данной работе для обнаружения связей между временными рядами предлагается использовать метод сходящегося перекрестного отображения. При таком подходе два временных ряда связаны, если существуют их траекторные подпространства, проекции на которые связаны. В свою очередь, проекции рядов на траекторные подпространства связаны, если окрестность фазовой траектории одного ряда отображается в окрестность фазовой траектории другого ряда. Ставится задача отыскания траекторных подпространств, обнаруживающих связь рядов. Анализ зависимости между показаниями датчиков и восприятиям внешнего мира человеком. Требуется проверить гипотезу зависимости между данными, а также предложить метод апроксимации показаний FMRI по прослушиваемому звуковому ряду.
* **Литература**
	- Все, что написал Сугихара в Science и в Nature (спросить коллекцию)
	- Усманова К.Р., Стрижов В.В. Обнаружение зависимостей во временных рядах в задачах построения прогностических моделей // Системы и средства информатики, 2019, 29(2) 
	- [Neural CDE](https://bit.ly/NeuroCDE)
* __Данные__: Видео, его разметка и ECoG, EEG, движение, глаз из работы Nature, Березуцкая, люди смотрят фильм
   	- Berezutskaya J., et al Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film // Sci Data 9, 91, 2022.
	- [Код предшественников](https://github.com/intsystems/
* Решение 
	* Базовое в работе Карины
	* Наше построить Neural ODE для обеих сигналов и решить, относятся ли обе модели к одной динамической системе. Требуется построить модель зависимости показания датчиков FMRI и звуковому сопровождению, который в этот момент прослушивает человек.
	* Построен [метод апроксимации показаний FMRI по прослушиваемому звуковому ряду](https://github.com/intsystems/2024-Project-117/tree/master).
* **Авторы:** Денис Тихонов, Даниил Дорин

## Задача 153 (была)
* **Название:**  Погружение временных рядов с высокой волатильностью в метрическое пространство
* **Описание проблемы:**  Решается задача выбора оптимального порфеля финансовых инструментов по результатам прогноза наборов временных рядов. Проблема при постановке задачи выбора заключается в том, что оценка ковариационной матрицы пар временных рядов не имеет требуемых статистических свойств (устойчивость, состоятельность, несмещенность). Предлагается выполнить сравнительный анализ различных методов вычисления парных расстояний между временными рядами. 
* **Данные:**
	- Финансовые ряды https://www.cambridge.org/core/books/trades-quotes-and-prices/029A71078EE4C41C0D5D4574211AB1B5
	- Trades Quotes and Prices https://lobsterdata.com/tradesquotesandprices
* **Литература** 
	- Multi-Period Trading via Convex Optimization by Stephen Boyd  Enzo Busseti
	- диссертация А.А. Адуенко
 * **Базовый алгоритм:** Задача квадратичного программирования с расстоянием между временными рядами в виде ковариационной матрицы из книги Бойда. Прогностическая модель произвольна, начиная с линейной регрессии. 
* **Новизна:** Задача выбора метрики между сильно зашумленными временными рядами является открытой. Функицю выбора предлагается строить исходя из Бойдовского критерия, против свертки прогноза с функцией доходности. 
* **Авторы:**  Яковлев

## Задача 152
* **Название:**  Метрическое прогнозирование временных рядов с высокой ковариацией
* **Описание проблемы:**  Решается задача прогнозирования наборов временных рядов. Каждый ряд имеет высокую дисперсию, ряды имеют высокую ковариацию. Такие свойства рядов наблюдаются в сигналах головного мозга и в ценах биржевых активов. Предлагается построить пространство парных растояний (метрическую конфигурацию временных рядов), выполнить прогноз в пространстве парных расстояний, и вернуть прогноз в исходное пространство, используя метод многомерного шкалирования. 
* **Данные:**
	- EEG, Данные удобные для загрузки https://pyriemann.readthedocs.io/en/latest/auto_examples/ERP/plot_classify_EEG_tangentspace.html#sphx-glr-auto-examples-erp-plot-classify-eeg-tangentspace-py
	- Финансовые ряды https://www.cambridge.org/core/books/trades-quotes-and-prices/029A71078EE4C41C0D5D4574211AB1B5
	- Trades Quotes and Prices https://lobsterdata.com/tradesquotesandprices
* **Литература** 
	- Singular Spectrum Analysis
	- https://pyriemann.readthedocs.io/en/latest/index.html
	- Multidimensional scaling
* **Базовый алгоритм:** Базовый прогноз выполнятется методами Singular Spectrum Analysis, LSTM+attention, Transformer
* **Новизна:** Предлагаемое решение использует 1) Римановы модели но не для классификации, а для арегрессии, 2) Римановы генеративные диффузные модели. 
* **Авторы:**  Яковлев
  
## Задача 150 (была)
* **Название:** Tree-width Driven SDP for The Max-Cut Problem
* **Описание проблемы:** The Max Cut problem is computationally intractable (NP hard) over general graphs; however, for trees and graphs with small tree-width it is easy to solve exactly in polynomial time. Furthermore, the SDP or Lovász-Schrijver relaxations allows to approximate the Max-Cut value over general graphs. The contribution is to combine both the tree-width and relaxation approaches to improve (empirically) the Max-Cut approximation quality. 
* **Данные:**
	- [1] Texas Data Repository https://dataverse.tdl.org/dataset.xhtml?persistentId=doi:10.18738/T8/VLTIVC
	- [2] Biq Mac Library https://biqmac.aau.at/biqmaclib.html
* **Литература**
	- [1] Intro and Problem setup: https://medium.com/toshiba-sbm/benchmarking-the-max-cut-problem-on-the-simulated-bifurcation-machine-e26e1127c0b0
	- [2] The Lovasz-Schrijver https://home.ttic.edu/~madhurt/Papers/ls.pdf
	- [3] The SDP https://ocw.mit.edu/courses/15-084j-nonlinear-programming-spring-2004/a632b565602fd2eb3be574c537eea095_lec23_semidef_opt.pdf
	- [4] Treewidth https://www.cs.cmu.edu/~odonnell/toolkit13/lecture17.pdf
* **Базовый алгоритм:**
	(a) The SDP/Semi-Definite Programming relaxation (b) The Lovasz Schrijver relaxation
* **Новизна:**
	(c) find a matrix k-diagonal C dominating (in a spectral sence) the laplacian of the initial graph. Use the treewidth max-cut over a "dominating" graph
	(d*) Use graph sparsification [5] to create a nice approximation to the initial graph, but having lower number of edges and treewidth
	- [5] A nice course of 3 lectures on Graph sparsification: https://simons.berkeley.edu/graph-sparsification
* **Авторы:** Alex Bulkin
 
## Задача 149 (???)
* **Название:** The Optimal Binning Problem: A Statistical ViewPoint
* **Описание проблемы:** The Optimal Binning problem is the optimal discretization of a variable into bins given a discrete or continuous numeric target. Given a dataset of N samples in [0,1] we are looking for a binning on M bins maximizing the weight-of-evidence metric [1]. The latter metric allows to understand the predictive power of an independent variable. Weight-of-evidence helps to understand if a particular class of an independent variable has a higher distribution of good or bad. Our problem is for a sufficiently large N and constant M to find such an optimal binning.  [1] https://medium.com/mlearning-ai/weight-of-evidence-woe-and-information-value-iv-how-to-use-it-in-eda-and-model-building-3b3b98efe0e8
* **Данные:** the baseline experiment over simulated datasets or any of the Kaggle datasets would be ok
* **Литература**
  	- [0] http://www.c4st.org/images/hesa-2015/submissions/Weight-of-Evidence-A-Review-of-Concept-and-Methods-E.pdf
  	- [1] https://medium.com/mlearning-ai/weight-of-evidence-woe-and-information-value-iv-how-to-use-it-in-eda-and-model-building-3b3b98efe0e8
  	- [2] https://arxiv.org/pdf/2001.08025.pdf
  	- [3] https://iopscience.iop.org/article/10.1088/0266-5611/18/4/201/meta
* **Базовый алгоритм:** The baseline algorithms are (a) CART and similar techniques (b) convex relaxation
* **Новизна:** quantile splitting of the inverse transform to the empirical distribution; probably with some a-posteriori empirical tuning
* **Авторы:** Alex Bulkin
* 
## Задача 148 (была)
* **Название:** Средневзвешенная когерентность как мера интерпретируемости тематических моделей
* **Описание проблемы:** Тематическое моделирование широко используется в социо-гуманитарных исследованиях для понимания тематической структуры больших текстовых коллекций. Типичный сценарий предполагает, что пользователь сам разделяет найденные моделью темы на "хорошие" (интерпретируемые) и "плохие". Для упрощения этой работы можно использовать ряд автоматически вычисляемых критериев качества, один из которых — когерентность (мера "согласованности" слов темы). Однако проблема когерентности в том, что при её вычислении игнорируется бòльшая часть текста, что делает оценку качества темы по когерентности ненадёжной. Задача в том, чтобы проверить новый способ вычисления когерентности, обобщающий классический подход, но при этом учитывающий распределение темы во всём тексте.
* **Данные:** В качестве данных подойдёт любая коллекция текстов на естественном языке, про которую известна тематическая структура (сколько примерно тем, сколько документов по разным темам). Например, можно взять коллекцию статей с ПостНауки, новостей Lenta, дамп Википедии, посты с Хабрахабра, 20 Newsgroups, Reuters.
* **Литература**
  - Воронцов К. В. "Вероятностное тематическое моделирование: теория, модели, алгоритмы и проект BigARTM" (https://web.archive.org/web/20230520153443/http://machinelearning.ru/wiki/images/d/d5/Voron17survey-artm.pdf)
  - Воронцов К. В. "Оценивание качества тематических моделей" (из курса лекций "Вероятностные тематические модели"; https://web.archive.org/web/20230811052505/http://www.machinelearning.ru/wiki/images/a/a7/Voron23ptm-quality.pdf
  - Alekseev V. A., Bulatov V. G., Vorontsov K. V. Intra-text coherence as a measure of topic models' interpretability //Komp'juternaja Lingvistika i Intellektual'nye Tehnologii. – 2018. – С. 1-13 (https://www.dialog-21.ru/media/4281/alekseevva.pdf)
  - Newman D. et al. Automatic evaluation of topic coherence //Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics. – 2010. – С. 100-108. (https://aclanthology.org/N10-1012.pdf)
* **Базовый алгоритм:** Когерентность Ньюмана по топ словам, внутритекстовая когерентность
* **Новизна:** Использование библиотек тематического моделирование BigARTM и TopicNet. Разработка нового способа вычисления когерентности тем. Предложение и реализация методики измерения интерпретируемости тем (чтобы проверить "адекватность" новой когерентности: в самом ли деле для заведомо хороших тем она показывает качество выше, чем для плохих тем).
* **Авторы:** Василий Алексеев, Константин Воронцов

## Задача 147 (???)
* **Название:** Нижние оценки для min max задач с разной размерностью блоков переменных (Проект 1.)
* **Описание проблемы:** Для задач малоразмерной выпуклой оптимизации нижние оценки получаются с помощью сопротивляющегося оракула https://www2.isye.gatech.edu/~nemirovs/Lect_EMCO.pdf (3 Methods with linear convergence, II, но начать лучше прямо с самого первого раздела Lecture 1 - на одномерном случае все попонятнее). В то время как для задач большой размерности -  c помощью “худшей в мире функции” - см., например, указания к упражнения 1.3 и 2.1 пособия МЦНМО https://opt.mipt.ru/posobie.pdf В работе https://arxiv.org/pdf/2010.02280.pdf, исследуются задачи min max, в которых одна из групп min переменных имеет небольшую размерность, а другая группа, напротив, большую. Получены верхние оценки. Интересно было бы попробовать получить нижние оценки, путем комбинации двух конструкций. Кажется, что в математическом плане пример построения нижней оценки будет содержать новые интересные идеи.
В развитие этого проекта интересно было бы подумать и о нижних оценках для min min задач, в которых по одной из групп переменных (негладких) имеется малая размерность. Верхние оценки имеются в работах https://arxiv.org/pdf/2102.00584.pdf и  https://arxiv.org/pdf/2103.00434.pdf
* **Авторы:** Александр Владимирович Гасников

## Задача 146 (???)
* **Название:** Слайдинг с редукцией дисперсии (Проект 2.)
* **Описание проблемы:** Направление градиентного слайдинга стало популярно в последнее время. Активно в этом направлении работает Джордж Лан https://arxiv.org/pdf/1406.0919.pdf https://arxiv.org/pdf/1609.04905.pdf https://arxiv.org/pdf/2111.00996.pdf Некоторые результаты по слайдингу есть и у нас https://arxiv.org/pdf/1906.03620.pdf https://arxiv.org/pdf/2002.02706.pdf https://arxiv.org/pdf/2103.09344.pdf https://arxiv.org/pdf/2205.15136.pdf https://arxiv.org/pdf/2307.12946.pdf стали появляться и статьи других коллективов https://arxiv.org/pdf/2201.01169.pdf.
Однако, полноценного слайдинга для задач вида f(x) + g(x), где g(x) имеет структуру суммы и для g(x) используется инкрементальный оракул (рандомизация суммы m слагаемых) не известно. Все что известно, описано тут https://arxiv.org/pdf/1912.11632.pdf
Задача заключается в обосновании оценки числа вызовов градиента f \tilde{O}(\sqrt{L_f/\mu}), и инкрементального оракула (градиентов слагаемых) g \tilde{O}(m+\sqrt{mL_{g}/\mu}). В общем случае такая оценка не получена до сих пор.
P.S. Были надежды на редукцию дисперсии с importance sampling с правильным выбором вероятностей, но пока они не оправдались…
* **Авторы:** Александр Владимирович Гасников 

## Задача 145  (была) (индустриальная)
  * **Название**: Создание персонализированных генераций изображений
  * **Описание проблемы**: Генеративные модели добились высокого качества генераций в общем домене. Однако, когда возникает запрос на генерацию специфичного объекта, в нашем случае человека, то модель не может сгенерировать человека с необходимой точностью и передать его идентичность. В этой задаче вам предлагается предложить решение, которое будет способно генерировать изображения заданного человека в различных варияциях в высоком разрешении.
  * **Данные**: CelebA (датасет изображений знаменитостей в высоком разрешении)
  * **Литература**: 
	- [1] [Создание изображений через механизм внимания](https://ip-adapter.github.io)
	- [2] [Создание специальных токенов для генерации специфичного изображения](https://dreambooth.github.io)
	- [3] [Latent Diffusion Model](https://arxiv.org/pdf/2112.10752.pdf)
  * **Базовой алгоритм**: Базовый алгоритм состоит в обучении метода IP-Adapter[1] на модели Stable Diffusion [3].
  * **Новизна**: Индустриальная
 * **Авторы**: Андрей Филатов
* **Контакт**: [TG: @anvilarth](https://t.me/anvilarth)


## Задача 144 (была)
* **Название:** Многократное обучение в рекомендательных системах
* **Описание проблемы:** Метрики качества рекомендательных систем P@k, NDCG, MRR и пр. обычно учитывают, насколько хорошие рекомендации были даны для рассматриваемого пользователя, при этом текущее и долгосрочное влияние применяемых алгоритмов на окружающую аудиторию потребителей и ассортимент товаров не учитывается. Предлагается рассмотреть рекомендательную систему, в которой товары W и потребители C меняются со временем, как процесс многократного машинного обучения. Пусть заданы начальные плотности распределений признаков f_0(с) и f_0(w) на X = C U W. Рассмотрим динамическую систему вида f_{t+1}(x) = D_t(f_t)(x) с оператором эволюции D_t [4], где переход к шагу t+1 состоит в формировании рекомендации потребителю (c, z) ~ f(c) алгоритмом товара w = h(c, f(c), f(w)), заключением сделки потребителем с вероятностью ~ u(c,w,z), обновлении f(c), f(w) и h(…) по истории предложений и сделок. В частности выяснить  1) При каких условиях в такой системе при t к бесконечности возникает вырождение аудитории, ассортимента или распределения сделок, и как зависит такое вырождение от алгоритма обучения и вида рекомендательной модели? 2) Предложить такой обучаемый рекомендательный алгоритм, который улучшает любую из стандартных метрик без вырождения C и G при t к бесконечности, или доказать, что это невозможно.
* **Данные:** Для инициализации используем синтетические данные, созданные по аналогии со статьей [1] или MovieLens 100K Dataset по аналогии со статьей [2]
* **Литература**
	- [1] Debiasing the Human-Recommender System Feedback Loop in Collaborative Filtering
	- [2] Breaking Feedback Loops in Recommender Systems with Causal Inference
	- [3] Positive feedback loops lead to concept drift in machine learning systems
	- [4] Задача 119, Моя первая научная статья 2023
	- [5] Python LibMF
* **Базовый алгоритм:** Исходный код - доработанная версия кода из [3] при решении [4], базовые алгоритмы рекомендаций - TopPop, Random, Oracle, SGD MF [5]
* **Новизна:** Предлагается исследовать свойства оператора D в зависимости от возможных предположений и ограничений, и применить ранее полученные в [4] результаты в предлагаемой постановке. Затем для простых моделей рекомендаций провести вычислительный эксперимент с симуляцией работы системы во времени с использованием базового кода для сравнения теоретических результатов по вопросам 1)-2) с фактическими наблюдениями.
* **Авторы:**  консультант - Веприков А.С., эксперт - Хританков А.С.

## Задача 143 (???) (индустриальная)
* **Название:** Интерпретируемая иерархическая кластеризация объектов
* **Описание проблемы:** Требуется на обучающей выборке физических лиц (ФЛ) с меткой принадлежности классу построить такую иерархическую кластеризацию данных, что для каждого кластера можно наиболее полно интерпретировать причину его попадания в конкретный кластер.
* **Данные:** Для исследования предлагается использовать данные из соревнования Kaggle по классификации мошенников в автостраховании на основе их признакового описания. https://www.kaggle.com/datasets/khusheekapoor/vehicle-insurance-fraud-detection?resource=download 
* **Литература** 
	- https://habr.com/ru/companies/otus/articles/782862/
	- https://paperswithcode.com/paper/interpretable-clustering-on-dynamic-graphs 
	- https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.53
* **Базовый алгоритм:** Базовый алгоритм решения задачи состоит в применении методов из пакетов sklearn, umap языка Python. Интерпретируемым алгоритмом классификации является дерево решений (sklearn.tree.DecisionTree).
* **Новизна:** Индустриальная задача
* **Авторы:** Старожилец Всеволод Михайлович

## Задача 142 (была) (индустриальная)
* ** **Название:**  Классификация товаров по ОКПД2 кодам.
* **Описание проблемы:**  Требуется по краткому текстовому описанию товара классифицировать товар по кодам ОКПД2 классификатора. Предполагается исследование зависимости точности и полноты классификации от рассматриваемой глубины ОКПД2 классификатора (например, для двух первых чисел классификатора задача практически тривиальна). 
* **Данные:** Около 40% данных госзакупок из открытых источников за 2022 год. https://drive.google.com/file/d/1bg1wdpsxv797dB8RBWaq8JLp_qQp_uc0/view?usp=drive_link
* **Литература** Рарерs with 1) the formulation of the problem, 2) baseline and new results, 3) fast introduction https://www.kaggle.com/competitions/ndsc-beginner/overview
* **Базовый алгоритм:** Построение текстовых эмбеддингов каким-либо открытым переобученным пакетом. Например, spaCy (у него есть русский язык). Далее решается задача классификации.
* **Новизна:** Индустриальная задача
* **Авторы:** Старожилец Всеволод Михайлович

## Задача (???) 140 
* **Название**: Адаптация архитектуры модели глубокого обучения с контролем эксплуатационных характеристик
*  **Задача:** рассматривается задача адаптация структуры обученной модели глубокого обучения для ограниченных вычислителньых ресурсов. Предполагается, что полученная архитектура (или несколько архитектур) должны работать эффективно на нескольких типах вычислительных серверов (например, на разных моделях GPU или различных мобильных устройствах). Требуется предложить метод поиска модели, позволяющий контролировать её сложность учетом целевых эксплуатационных характеристик.
*  **Данные:** MNIST, CIFAR
*  **Литература:**
    * [Yakovlev K. D. et al. Neural Architecture Search with Structure Complexity Control //Recent Trends in Analysis of Images, Social Networks and Texts: 10th International Conference, AIST 2021, Tbilisi, Georgia, December 16–18, 2021, Revised Selected Papers. – Cham : Springer International Publishing, 2022. – С. 207-219.](https://yahootechpulse.easychair.org/publications/preprint_download/H5MC)
    * [FBNet: выбор архитектуры модели с учетом целевых характеристик](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_FBNet_Hardware-Aware_Efficient_ConvNet_Design_via_Differentiable_Neural_Architecture_Search_CVPR_2019_paper.pdf)  
*  **Базовый алгоритм:** FBNet и случайный поиск подструктуры модели
*  **Решение:** Предлагаемый метод заключается в использовании дифференцируемого алгоритма поиска архитектуры нейросети(FBNet) с контролем сложности параметров при помощи гиперсети. Гиперсеть - это модель, порождающая структуру модели в зависимости от входных параметров. В качестве параметров гиперсети предлагается использовать нормированное время работы базовых операций на целевых вычислительных ресурсах. Таким образом, полученная модель позволит адаптировать архитектуру модели для произвольного устройства.
Новизна: предложенный метод позволяет контролировать сложность модели, в процессе поиска архитектуры без дополнительных эвристик.
*  **Авторы:** Константин Яковлев, Олег Бахтеев
  

# Проекты пятикурсников и назначенные проекты=

## Задача 160 (???) (Алексей Ребриков)
* __Название__: Ускоренные методы нулевого порядка с одноточечным фидбэком
* __Задача__: Обычно для целевых функций в задачах оптимизации мы можем считать градиенты и даже информацию более высоких порядков. Но также существуют приложения, в которых подсчет градиента или затруднен или вообще невозможен. В таких приложениях на помощь приходят безградиентные методы. Довольно натуральной идеей в данном случае является аппроксимация градиента через конечные разности (https://arxiv.org/pdf/2211.13566.pdf): $\nabla f_\gamma(x, e) = \frac{f(x + \gamma e) - f(x - \gamma e)}{2 \gamma} e.$ Более того, в более близких для практики приложениях мы имеем доступ к зашумленной версии функции. Это порождает дополнительные проблемы. В более простом с точки зрения теоретического анализа, но менее приближенным к реальности случае можно рассматривать так называемый two-point feedback: $\nabla f_\gamma(x, \xi, e) = \frac{f(x + \gamma e, \xi) - f(x - \gamma e, \xi)}{2 \gamma} e.$ Ключевая особенность – одна и та же случайность в обеих точках. Более сложный и неприятный случай – one-point feedback: $\nabla f_\gamma(x, \xi^+, \xi^-, e) = \frac{f(x + \gamma e, \xi^+) - f(x - \gamma e, \xi^-)}{2 \gamma} e,$ который и будет рассматриваться в работе.
* __Данные__:  1) Датасет mushroom 2) Квадратичная задача
* __Литература__: Randomized gradient-free methods in convex optimization (https://arxiv.org/pdf/2211.13566.pdf) AN ACCELERATED METHOD FOR DERIVATIVE-FREE SMOOTH STOCHASTIC CONVEX OPTIMIZATION (https://arxiv.org/pdf/1802.09022.pdf)
* __Базовой алгоритм__:   Результаты для two-point feedback (https://arxiv.org/pdf/1802.09022.pdf)
* __Решение__: В данной работе предлагается разработать и проанализировать сходимость ускоренный безградиентные методы для выпуклой гладкой стохастической задачи минимизации. В частности предлагается адаптировать (или оттолкнуться в качестве стартовой точки) для этого уже существующие результаты в случае two-point feedback (https://arxiv.org/pdf/1802.09022.pdf).
* __Авторы__: Консультант - Александр Богданов, эксперт - Александр Безносиков

## Задача 161 (была) (Фанис)
* __Название__: Методы малоранговых разложений в распределенном и федеративном обучении
* __Задача__: Подходы распределенного и федеративного обучения становятся все более популярными в обучении современных SOTA моделей машинного обучения. При этом на первый план выходит вопрос организации эффективных коммуникаций, так как процесс передачи информации занимает слишком много времени даже в случае кластерных вычислений. Из-за этого может теряться смысл в распределении/распараллеливании процесса обучения. Одной из ключевой техник  борьбы с коммуникационными затратами является использование сжатий передаваемой информации. На данный момент в литературе предлагаются различные техники сжатия (https://arxiv.org/abs/2002.12410, https://arxiv.org/abs/1610.02132, https://arxiv.org/abs/1905.10988), но потенциал в этом вопросе явно не исчерпан. В частности, довольно большой потенциал кроется в малоранговых разложениях (https://gregorygundersen.com/blog/2019/01/17/randomized-svd/). В рамках проекта предлагается сконструировать операторы сжатия на основе данных разложений и встроить в методы распределенной оптимизации (https://arxiv.org/abs/2106.05203).
* __Данные__: LibSVM https://www.csie.ntu.edu.tw/~cjlin/libsvm/ CIFAR 10
https://www.cs.toronto.edu/~kriz/cifar.html В экспериментах предлагается суммулировать на одном устройстве распределенное обучение 1) логистической регресии на датасетах из LibSVM, 2) ResNet18 на CIFAR 10
* __Литература__: https://arxiv.org/abs/2002.12410, https://arxiv.org/abs/1610.02132, https://arxiv.org/abs/1905.10988 https://gregorygundersen.com/blog/2019/01/17/randomized-svd/ https://arxiv.org/abs/2106.05203
* __Базовой алгоритм__: https://arxiv.org/abs/2106.05203 + https://arxiv.org/abs/2002.12410 или https://arxiv.org/abs/1905.13727
* __Решение__: В рамках проекта предлагается сконструировать операторы сжатия на основе малоранговых разложений (https://gregorygundersen.com/blog/2019/01/17/randomized-svd/) и встроить в методы современные методы распределенной оптимизации (https://arxiv.org/abs/2106.05203).
* __Авторы__: Безносиков А.Н., Зыль А.В.

## Задача 162 (была) (Проект 5-ого курса, Марат Хусаинов)
* __Название__: Адаптивные методы генерации с использованием диффузионных моделей
* __Задача__: Дана многомерная функция распределения p(x) = f(x) / Z.  Задача состоит как в вычислении нормализационной константы — Z, так и в получении объектов из распределения p(x).
* __Данные__: В качестве целевого распределения могут быть взяты многомерная гауссова смесь, Funnel, Manywell и др. 
* __Литература__:  https://arxiv.org/pdf/2302.13834.pdf https://arxiv.org/pdf/2310.02679.pdf https://arxiv.org/pdf/2208.01893.pdf
* __Базовой алгоритм__: https://github.com/lollcat/fab-torch
* __Решение__: The idea of the proposed solution and methods for conducting the research. Ways of visualizing data and error analysis
* __Новизна__: Обоснование новизны и значимости идей (для редколлегии и рецензентов журнала). 
* __Авторы__:  Сергей Самсонов 

## Задача 163 (была) (Проект пятого курса, Баир Михайлов)
* __Название__:  Undersampled MRI reconstruction 
* __Задача__: Magnetic Resonance Imaging (MRI) examination times can vary from fifteen minutes to one hour, which is inconvenient for both the doctor and the patient. Additionally, human motion during the scan can significantly decrease the quality of the images. Undersampled MRI allows for fewer measurements in Fourier-space, thereby reducing the scan time by 4-8 times. However, in this approach, some information is lost according to the Nyquist-Shannon sampling theorem. The main hypothesis of this study is the possibility of using general information from the scan space through machine learning to mitigate this problem. An optimization-style problem statement can be seen in Supplementary.
* __Данные__: fastMRI, a large-scale dataset of both raw MRI measurements and clinical MRI images. https://fastmri.med.nyu.edu/
* __Литература__:
	1. fastMRI: An Open Dataset and Benchmarks for Accelerated MRI - formulation of the problem, dataset (https://arxiv.org/abs/1811.08839)
	2. Fill the K-Space and Refine the Image: Prompting for Dynamic and Multi-Contrast MRI Reconstruction - SOTA (https://arxiv.org/abs/2309.13839)
	3. An Adaptive Intelligence Algorithm for Undersampled Knee MRI Reconstruction - top submission of the original competition (https://arxiv.org/abs/2004.07339)
	4. Deep Cardiac MRI Reconstruction with ADMM - top submission from other competition (https://arxiv.org/abs/2310.06628)
	5. A review and experimental evaluation of deep learning methods for MRI reconstruction - introduction to the problem (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9202830/)
* __Базовой алгоритм__:  A link to the code of the baseline algorithm. It shows the state of the art and will be compared with the proposed solution https://github.com/hellopipu/PromptMR
* __Решение__: My idea is to potentially develop a solution without several cascade steps like PromptMR to spend less time on each prediction, also it is possible to experiment with filter patterns. Additionally in the original SOTA paper there is no experiments with real radiologists, authors only measure PSNR, SSIM and NMSE and don't consider the opinion of the specialists and it is entirely possible to make a direct comparison between visualizations with and without undersampling to measure the drop in quality if any.
* __Авторы__:   Expert: Dmitry Dylov Consultant: Artem Razumov  Me: Bair Mikhailov


## Задача 155 (???)
* __Название__:  Identification of the relationship between labels using an algorithm based on one's own attention for the classification problem with multiple labels, justifying the connection with Hawkes processes.
* __Задача__: A description of your problem, its motivation  and goals. An optimization-style problem statement is welcome
Most of the available user information can be represented as a sequence of events with timestamps. Each event is assigned a set of categorical labels, the future structure of which is of great interest. This is a temporal sets prediction problem for sequential data. Modern approaches focus on the transformation architecture for sequential data, introducing independent attention to the elements in the sequence. In this case, we take into account the temporal interactions of events, but lose information about the interdependencies of labels. Motivated by this disadvantage, we propose to use the mechanism of independent attention to the marks preceding the predicted step. Since our approach is a network of attention to labels, we call it a LANET.  We also justify this aggregation method, it affects the intensity of the event positively, assuming that the intensity is represented by the basic Hawkes process.
* __Данные__: Based on the dataset data, we will compare the state of the art solutions in this area with our solution in this problem statement. https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data https://www.kaggle.com/c/python-and-analyze-data-final-project/data
* __Литература__:
	- 1-2 Predicting Temporal Sets with Deep Neural Networks, Predicting Temporal Sets with Simplified Fully Connected Networks 
	- 3 Transformer Hawkes Process, The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process - introduction to idea with process Hawkes
* __Базовой алгоритм__:  https://github.com/yule-BUAA/SFCNTSP https://github.com/yule-BUAA/DNNTSP The state of the art methods for temporal sets prediction
* __Решение__:  Most of the transformer-related models used for temporal sets prediction use self-attention computation between consecutive input timestamps representations. The LANET instead uses the self-attention between label representations. So, it has the input that consists of K vectors. Below, we describe how to aggregate a sequence of size τ to K vectors via an Embedding layer. Then we define the Self-attention layer. To get the predictions, we apply a Prediction layer.  Also, to justify such aggregation by labels, instead of time dependence, we introduce the concept of Cox processes, which describe the probability of an event at a given time through the integral of the intensity function. And we can show that such aggregation can either not worsen it, or make a positive contribution to probability. LANET will train with the cross-entropy loss adapted for the multi-label task through
independent consideration of each label score. Comparison tables with basic approaches will be carried out, as well as visualization of quality changes from selected parameters, visualization of attention for analyzing the relationship of label representations, as well as tables comparing the effect of vector representations on the result of the model. 
* __Авторы__:  Consultant - Andrey Grabovoy Expert - Alexey Zaytsev Author of research - Galina Boeva
  
## Задача 1XX (???) (Who are the problem owners?)
* __Название__:  Support strategies for advanced Post-Training Quantization
* __Задача__: Advanced PTQ methods do not require a lot of computing resources, demonstrate high quality and work quickly. Modern approaches tend to be consistent and optimize the model block-by-block or layer-by-layer. However, these approaches have several fundamental problems. The first of these is the poor correlation of reconstruction losses, which are used for optimization, with target losses. This leads to a decrease in quality, especially for models with a sharp loss landscape. In sequential approaches, the optimization of the following blocks or layers is based on the optimization of all previous blocks or layers. Because of this design, the second problem is an incorrectly set optimization task for the last blocks or layers in the network.
* __Данные__: Cifar-10, ImageNet
* __Литература__:  https://arxiv.org/abs/2203.05740, https://arxiv.org/abs/2312.07950
* __Базовой алгоритм__:  https://github.com/wimh966/QDrop
* __Решение__:   Theoretical justification of mentioned problems, generalization and ablation study of basic solutions and their modification.
* __Авторы__: 

## Задача 164 (была) (проект пятого курса, Парвиз Каримов)
* __Название__:  Методы составления эмбеддингов коллекций
* __Задача__: Пусть дан датасет $\mathfrak{G} = \{(x_i, y_i)\}_{i = 1}^{n}$, $x_i \in X$, $y_i \in \{1, ..., K\}$. Составим из этих точек данных множества: $$G_{j, k} = \{x_i | (x_i, y_i) \in \mathfrak{G} \wedge y_i = k \forall i \} : \forall j_1, j_2 G_{j_1, k} \cap G_{j_2, k} = \emptyset$$ю Задача состоит в том, чтобы сопоставить каждой коллекции $G_{j, k}$ эмбеддинг $f_{\theta}(G_{j, k})$, представляющий собой информативное векторное представление $G_{j, k}$.
* __Данные__:  Omniglot (https://github.com/brendenlake/omniglot), Face datasets (https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/, http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)
* __Литература__:  https://arxiv.org/pdf/1206.5538.pdf, https://arxiv.org/pdf/2207.06167v1.pdf, https://arxiv.org/pdf/2005.10497v2.pdf
* __Базовой алгоритм__:  Taking instance-level trained pre-trained model & using centroid as group embedding (https://arxiv.org/pdf/1908.05257v1.pdf, https://github.com/tiangeluo/fsl-global)
* __Решение__:  Смешивание эмбеддингов уровня инстансов с эмбеддингами уровня коллекций при обучении: https://arxiv.org/pdf/2207.06167v1.pdf Определение принадлежности к коллекции с учётом неравномерности распределения датасета по ним: https://arxiv.org/pdf/2005.10497v2.pdf Получить некоторые теоретические результаты относительно качества эмбеддингов коллекций (напр. Теорема 1 из https://github.com/intsystems/Karimov_2023_NIR/blob/master/paper/paper.pdf).
* __Авторы__:  Roman Isachenko

## Задача 165 (проект пятого курса, Дмитрий Протасов)
* __Название__: Automatic Music Transcription
* __Задача__:  Automatic music transcription (AMT) remains an important but challenging task in music information retrieval, hampered by limited MIDI datasets and the poor quality of existing models. This research aims to improve transcription accuracy by using specialized models to extract distinct musical features such as chord progressions, tonality, rhythm, and instrument types. To address the scarcity of MIDI datasets, we propose the use of synthetic
data to augment training resources. This approach offers a new way to potentially enrich AMT models and advance the field.
* __Данные__: https://github.com/KinWaiCheuk/AudioLoader/tree/master (MAPS, Maestro, MusicNet)
* __Литература__:
	1) MT3 (sota 2022): https://arxiv.org/pdf/2111.03017.pdf
	2) Splitter: https://arxiv.org/pdf/2305.07489.pdf
	3) Basic Pitch (lightweight model, good detection for one instrument, fast intro): https://arxiv.org/pdf/2203.09893.pdf
	4) Synthetic data: https://arxiv.org/pdf/2312.10402.pdf
* __Базовой алгоритм__: Solution1: https://github.com/magenta/mt3/tree/main (sota 2022), Solutions2: https://github.com/vpavlenko/study-music/blob/main/parts/transcription.d
* __Решение__:  The idea of the proposed solution and methods for conducting the research. Ways of visualizing data and error analysisю Extracting individual musical characteristics (chord-progression, tonality, bpm, instrument), and using synthetic datasets for data augmentation.
Authors: Матвеев, Протасов


## Задача 166 
* __Название__: Machine learning methods for functional brain mapping
* __Задача__: Segmentation of functional areas of fMRI brain scans ([article with problem statement](https://www.sciencedirect.com/science/article/pii/S1053811922005353)) 
* __Данные__: [Dataset](https://www.humanconnectome.org/): Human Connectome Project: ≈ 1200 healthy subjects with both resting-state and task fMRI data; 4D (3D time-dependent data) $1.5 \times 10^6$ -dimensional MRI-measurements received per a few seconds
* __Литература__:
	1. [Accurate predictions of individual differences in task-evoked brain activity from resting-state fMRI using a sparse ensemble learner](https://www.sciencedirect.com/science/article/pii/S1053811922005353)
        2. [Predicting individual task contrasts from resting ‐state functional connectivity](https://www.sciencedirect.com/science/article/pii/S1053811921011204)
        3. [Task-free MRI Predicts Individual Differences in Brain Activity During Task Performance](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6309730/)
        4. [Semiblind Spatial ICA of fMRI Using Spatial Constraints](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2891131/)
* __Базовой алгоритм__: [Method 1](https://github.com/yingqiuz/predict-task-individual-variability), [Method 2](https://github.com/sabunculab/brainsurfcnn) 
* __Решение__: The proposed solution is a machine learning algorithm developed based on existing techniques.

## Задача 167
* __Название__: Прогнозирования временных рядов социальных трендов и общественных интересов с высокой волатильностью
* __Описание проблемы__: Анализ и прогноз трендов в медиапространстве является важной задачей для различных сфер, таких как маркетинг, медиапроизводство, связи с общественностью, инновационные исследования и разработки. Эта задача является сложной из-за волатильности и неустойчивости социальных тенденций и общественных интересов. Цель предлагаемого исследования - изучить подходы к решению задачи и разработать базовый алгоритм, способный предсказать, о чем будут говорить люди, в конкретный исторический период в будущем на горизонте нескольких месяцев. Проблема заключается не только в высокой размерности и неустойчивости тематического пространства, но и в необходимости выделения тематик релевантных для конкретных сообществ на высоком уровне обобщения: спорт, политика, бизнес, технологии и др. так и на низком уровне: профессиональные сообщества вокруг конкретной технологии или целевая аудитория определенного сегмента рынка.
* __Данные__: Будут изучены общедоступные сообщения на платформах социальных сетей, таких как Twitter, за несколько лет. Затем набор данных преобразуется во временные ряды тематических кластеров с помощью тематического моделирования. Кластеры формируются с учетом значимости событий, рассчитанной по позиции новостной темы в топе. Наборы данных для справки: [Twitter trending tweets](https://www.kaggle.com/datasets/rsrishav/twitter-trending-tweets/), [Youtube trending video dataset](https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset).
* __Литература__:
	1. [Taylor, S. J., & Letham, B. (2018). Forecasting at scale. The American Statistician, 72(1), 37–45.](https://peerj.com/preprints/3190/)
        2. [Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint arXiv:1908.10084.](https://arxiv.org/abs/1908.10084)
        3. [Motrenko, A., & Strijov, V. (2014). Extracting fundamental periods to segment biomedical signals.](https://m1p.org/papers/MotrenkoStrijov2014RV2.pdf)
* __Базовой алгоритм__: ARIMA и Exponential Smoothing 
* __Решение__: Для решения данной задачи предлагается снизить волатильность домена путем построения пространства кластеров интересов. Прогноз осуществляется в этом пространстве, чтобы определить сезонность и жизненные циклы тем и предсказать их динамику в будущих периодах. В данной статье сравниваются методы кластеризации социальных трендов, а также алгоритмы их предсказания. Помимо этого, приводится оценка качества прогноза.
* __Новизна__: Новизна работы заключается в уникальном сочетании методов прогнозирования временных рядов и кластеризации трендов. Это позволяет применить научный подход к задаче, которая в силу высокой размерности ранее в зависела от человеческой интуиции и традиционного выявления тенденций. Этот подход может дать как прикладное ( внести вклад в область прогнозирования интересов аудитории ), так и научное значение ( перенеся алгоритм на домен трендов научных статей )
* __Авторы__: Задворнов Егор

## Задача 1XX (новая, выгрузил из формы)
* __Название__: Средневзвешенная когерентность как мера интерпретируемости тематических моделей
* __Задача__: Тематическое моделирование широко используется в социо-гуманитарных исследованиях для понимания тематической структуры больших текстовых коллекций. Типичный сценарий предполагает, что пользователь сам разделяет найденные моделью темы на "хорошие" (интерпретируемые) и "плохие". Для упрощения этой работы можно использовать ряд автоматически вычисляемых критериев качества, один из которых — когерентность (мера "согласованности" слов темы). Однако проблема когерентности в том, что при её вычислении игнорируется бОльшая часть текста, что делает оценку качества темы по когерентности ненадёжной. Задача в том, чтобы проверить новый способ вычисления когерентности, обобщающий классический подход, но при этом учитывающий распределение темы во всём тексте.
* __Данные__: В качестве данных подойдёт любая коллекция текстов на естественном языке, про которую известна тематическая структура (сколько примерно тем, сколько документов по разным темам). Например, можно взять коллекцию статей с ПостНауки, новостей Lenta, дамп Википедии, посты с Хабрахабра, 20 Newsgroups, Reuters. Тематика коллекции должна быть интересна самому исследователю, чтобы была мотивация оценивать темы вручную.
* __Литература__: * Воронцов К. В. "Вероятностное тематическое моделирование: теория, модели, алгоритмы и проект BigARTM" (https://web.archive.org/web/20230520153443/http://machinelearning.ru/wiki/images/d/d5/Voron17survey-artm.pdf)
* Воронцов К. В. "Оценивание качества тематических моделей" (из курса лекций "Вероятностные тематические модели"; https://web.archive.org/web/20230811052505/http://www.machinelearning.ru/wiki/images/a/a7/Voron23ptm-quality.pdf)
* Alekseev V. A., Bulatov V. G., Vorontsov K. V. Intra-text coherence as a measure of topic models' interpretability //Komp'juternaja Lingvistika i Intellektual'nye Tehnologii. – 2018. – С. 1-13 (https://www.dialog-21.ru/media/4281/alekseevva.pdf)
* Newman D. et al. Automatic evaluation of topic coherence //Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics. – 2010. – С. 100-108. (https://aclanthology.org/N10-1012.pdf)
* __Базовой алгоритм__: Когерентность Ньюмана по топ словам, внутритекстовая когерентность
* __Решение__: Использование библиотек тематического моделирование BigARTM и TopicNet. Разработка нового способа вычисления когерентности тем. Предложение и реализация методики измерения интерпретируемости тем (чтобы проверить "адекватность" новой когерентности: в самом ли деле для заведомо хороших тем она показывает качество выше, чем для плохих тем).
* __Авторы__: Старожилец Всеволод Михайлович

## Задача 1XX (новая, выгрузил из формы)
* __Название__: Интерпретируемая иерархическая кластеризация объектов.
* __Задача__: Требуется на обучающей выборке физических лиц (ФЛ) с меткой принадлежности классу построить такую иерархическую кластеризацию данных, что для каждого кластера можно наиболее полно интерпретировать причину его попадания в конкретный кластер.
* __Данные__: Для исследования предлагается использовать данные из соревнования [Kaggle](https://www.kaggle.com/datasets/khusheekapoor/vehicle-insurance-fraud-detection?resource=download) по классификации мошенников в автостраховании на основе их признакового описания.
* __Литература__: [Список научных работ, дополненный 1) формулировкой решаемой задачи, 2) ссылками на новые результаты, 3) основной информацией об исследуемой проблеме. ](https://habr.com/ru/companies/otus/articles/782862/
https://paperswithcode.com/paper/interpretable-clustering-on-dynamic-graphs
https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.53)
* __Базовой алгоритм__: Описание baseline решения предоставлено на сайте соревнования: https://exporl.github.io/auditory-eeg-challenge-2024/task2/description/
* __Решение__: Базовый алгоритм решения задачи состоит в применении методов из пакетов sklearn, umap языка Python. Например, классическим интерпретируемым алгоритмом классификации является дерево решений (sklearn.tree.DecisionTree).
* __Авторы__: Василий Алексеев, Константин Воронцов

## Задача 1XX (новая) (проект пятого курса, Дорин Даниил)
* __Title__: Detecting Manual Alterations in Biological Image Data Using Contrastive Learning and Pairwise Image Comparison
* __Problem__: This project aims to develop a model for pairwise comparison of biological and medical images, demonstrating high pairwise comparison metrics to detect manual modifications such as cropping, rotation, duplication, color distortion, or more subtle alterations. These alterations can compromise dataset validity and lead to erroneous conclusions, posing unique detection challenges due to the thematic and structural similarities in medical images. Additionally, the reproducibility of scientific findings is often undermined by the copying of images used as evidence or achievements, highlighting the need for robust detection methods to maintain data integrity in scientific research.
* __Data__:
	- [BioImage Archive](https://www.ebi.ac.uk/bioimage-archive/): A repository of biological images, including microscopy and experimental images.
	- [Cell Image Library](https://www.cellimagelibrary.org/home): High-resolution cell microscopy images from various biological experiments.
	- [Haxby Dataset](http://data.pymvpa.org/datasets/haxby2001/): The fMRI images dataset, the data are third-order tensors, so it is suggested to take slices.
	- [Visible Human Project](https://www.nlm.nih.gov/research/visible/visible_human.html): Detailed anatomical datasets of the human body, including CT and MRI scans.
* __Reference__:
	- [1] Melekhov I., Kannala J., Rahtu E. Siamese network features for image matching. ICPR, 2016.
	- [2] Chen T. et al. A simple framework for contrastive learning of visual representations. PMLR, 2020.
	- [3] Radford, A. et al. Learning Transferable Visual Models from Natural Language Supervision (CLIP). ICML, 2021.
	- [4] Zbontar, J. et al. Barlow Twins: Self-Supervised Learning via Redundancy Reduction. ICML, 2021.
* __Baseline__: Use the [Barlow Twins](https://github.com/facebookresearch/barlowtwins) for self-supervised learning of image features. Next, train the head for matching, or train the entire pipeline. It is intended to use parallel augmentations, which you can read more about in the work [SimCLR](https://arxiv.org/abs/2002.05709). To simulate manual modifications, [augment](https://explore.albumentations.ai/) datasets with: Cropping, resizing, flipping. Brightness/contrast adjustments. Cloning or duplicating parts of an image. Adding artificial noise or subtle distortions. 
* __Proposed solution__: To refine the basic solution, it is possible to retrain the full pipeline for biological data.
* __Novelty__: At present, the problem of matching biological and medical images has not yet been solved.
* __Authors__:
  	- Expert: Andrey Grabovoy
	- Consultant: Daniil Dorin (tg: @danulkin)

## Задача 1XX (новая) (проект пятого курса, Дорин Даниил)
* __Title__: Robust Detection of AI-Generated Images
* __Problem__: The rapid evolution of generative models, such as Gangs, VAEs, and diffusion-based models, has enabled the creation of highly realistic synthetic images, driving innovation in entertainment, art, and content creation. However, this has also introduced significant challenges in digital trust and authenticity, making the detection of machine-generated images crucial for combating misinformation and ensuring visual data integrity. This paper addresses the problem of identifying whether an image is machine-generated or real, aiming to develop a lightweight, efficient, and interpretable detection framework. The goal is to optimize detection accuracy while minimizing computational complexity, formulated as a binary classification problem. Key challenges include the diversity of generative models and the evolving quality of synthetic images, which increasingly mimic real-world characteristics. 
* __Data__:
	- Real Images:  
   		- [COCO](https://cocodataset.org/#home): A large-scale dataset with natural images of various objects and scenes.  
   		- [Flickr-Faces-HQ (FFHQ)](https://github.com/NVlabs/ffhq-dataset): A high-quality dataset of human faces for training and evaluation.
		- [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/): Standardised image datasets for object class recognition.
	- Hybrid Datasets:
		- [CIFAKE](https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images): CIFAKE is a dataset that contains 60,000 synthetically-generated images and 60,000 real images.
		- [ArtiFact](https://paperswithcode.com/dataset/artifact): large-scale image dataset that aims to include a diverse collection of real and synthetic images from multiple categories.
* __Reference__:
	- [1] [Zero-Shot Detection of AI-Generated Images](https://arxiv.org/abs/2409.15875)
	- [2] [AI vs. AI: Can AI Detect AI-Generated Images?](https://www.mdpi.com/2313-433X/9/10/199)
	- [3] [GenImage: A Million-Scale Benchmark for Detecting AI-Generated Image](https://proceedings.neurips.cc/paper_files/paper/2023/file/f4d4a021f9051a6c18183b059117e8b5-Paper-Datasets_and_Benchmarks.pdf)
* __Baseline__: Use a robust encoder, such as [CLIP ViT](https://arxiv.org/abs/2103.00020) or [Barlow Twins](https://github.com/facebookresearch/barlowtwins), as a frozen model and train only the last classification layer.
* __Authors__:
  	- Expert: Andrey Grabovoy
	- Consultant: Daniil Dorin (tg: @danulkin)

## Задача 1XX (Новая, ранее была частью задачи 117) (проект пятого курса, Дорин Даниил)
* __Название__: Восстановление показаний датчиков фМРТ по прослушиваемому звуковому ряду
* __Задача__: Рассматривается задача восстановления зависимости между показаниями датчиков функциональной магнитно-резонансной томографии (фМРТ) и восприятием внешнего мира человеком. Основная цель заключается в анализе зависимости между последовательностью снимков фМРТ и звуковым рядом, а также в разработке метода прогнозирования показаний фМРТ на основе прослушиваемого звукового ряда. Кроме того, хочется проверить гипотезу о влиянии параметра задержки BOLD (Blood-oxygen-level-dependent) на качество аппроксимации.
* __Литература__:
	- Вся литература из работы [Forecasting fMRI images from video sequences: linear model analysis](https://link.springer.com/epdf/10.1007/s13755-024-00315-5?sharing_token=lmbWsrIhGUoHaL75ub7etfe4RwlQNchNByi7wbcMAY438_8ojgqthAfg8Q2YiNthCEtJoSCkYUusKUpq6L-34kwAeXd5dq2ckTk8iYugJ0VtvTwPUiCF7XX9yGXw-8rdq5DB9u-Y4sNf-in0p3Zc5IbybvEDqh7v-BeAmfkPkuI%3D). [Код данного исследования](https://github.com/DorinDaniil/Forecasting-fMRI-Images).
	- [Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film](https://www.nature.com/articles/s41597-022-01173-0)
* __Данные__: 
   	- Berezutskaya J., et al Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film // Sci Data 9, 91, 2022. Открытый мультимодальный набор данных, включающий записи внутричерепной электроэнцефалографии (iEEG) и функциональной магнитно-резонансной томографии (фМРТ), полученные во время просмотра короткометражного аудиовизуального фильма. Исследование проводилось на группе участников в возрасте от 7 до 47 лет. Данные собраны с использованием богатого аудиовизуального стимула, что позволяет анализировать реакции мозга на естественные условия. 
* __Решение__
  	* Предлагается использовать простейшую линейную модель для прогнозирования независимо значений в каждом вокселе по данным звукового ряда.
	* [Код предшественников](https://github.com/intsystems/2024-Project-117).
* __Новизна__: Анализ зависимости между показаниями датчиков и восприятиям внешнего мира человеком. Требуется проверить гипотезу зависимости между данными.
* __Авторы__:
	* Консультант: Даниил Дорин (tg: @danulkin)
	* Эксперт: Вадим Стрижов, Андрей Грабовой

## Задача 1XX (новая) (проект пятого курса, Веприков Андрей)
* __Название__: Дообучение LLM с помощью оптимизации нулевого порядка 
* __Задача__: Описание проблемы: В области обработки естественного языка стандартным подходом является дообучение больших языковых моделей (LLM) с использованием методов оптимизации первого порядка, таких как SGD и Adam. Однако с увеличением размеров LLM существенные затраты памяти, связанные с обратным проходом (back-propagation) для вычисления градиентов, становятся серьезной проблемой из-за нехватки памяти для обучения. Именно поэтому развивается все больше методов оптимизации нулевого порядка (ZO) [1, 2], которые для вычисления градиентов требуют только прямого (forward) прохода модели. В данной работе предлагается придумать новые или модифицировать уже известные ZO подходы [3] для дообучения LLM, таких как LoRA [4]. 
* __Литература__:
	- [1] Fine-tuning language models with just forward passes
	- [2] Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for Fine-Tuning Large Language Models
	- [3] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark
	- [4] Lora: Low-rank adaptation of large language models
	- [5] New aspects of black box conditional gradient: Variance reduction and one point feedback
* __Базовый алгоритм__: В статье [2] применяется базовый ZO-SGD на рассматриваемой задаче. В статье [3] рассмотрены более сложные алгоритмы, также в этой статье есть гитхаб с кодом
* __Новизна__: Предлагается применить уже полученные результаты в области безградиентных методов [5] в реальной задаче дообучения LLM. Идеальным результатом было бы повышение качества относительно других Zero-order методов и снижение памяти относительно First-order методов.
* __Авторы__: консультант: Веприков А.С., эксперт: Безносиков А.Н.

## Задача 1XX (новая) (проект пятого курса, Веприков Андрей)
* **Название:** 
Эффекты самоорганизации в рекомендательных системах
* **Описание проблемы:** 
Предлагается исследовать эффекты самоорганизации в рекомендательной системе, в которой товары W и потребители C меняются со временем, как процесс многократного машинного обучения [4]. Пусть заданы начальные плотности распределений признаков f_0^c потребителей и f_0^w товаров на X = C U W. Рассмотрим динамическую систему вида f_{t+1} = D_t(f_t) с оператором эволюции D_t, где переход к шагу t+1 состоит в формировании рекомендации потребителю c ~ f_t^c алгоритмом рекомендаций товара w = h_u(c, f^c_t, f^w_t), заключением сделки потребителем с вероятностью ~ q(c,w,z) со скрытым от алгоритма параметром z, обновлении f^c_t и f^w_t по истории предложений и сделок. 
Можно показать, что при некоторых условиях в системе присутствует петля положительной обратной связи [3], то есть распределение данных системы меняется со временем вследствие искажений, вносимых алгоритмом рекомендаций. 
Используя нейронную коллаборативную фильтрацию (NCF) научиться восстанавливать и обновлять функцию q(c,w,z) по истории сделок и экспериментально уточнить условия возникновения положительной петли обратной связи в такой системе [3].
* **Данные:** 
Для инициализации используем MovieLens 100K Dataset по аналогии со статьей [2] 
* **Литература** 
	- [1] Debiasing the Human-Recommender System Feedback Loop in Collaborative Filtering 
	- [2] Breaking Feedback Loops in Recommender Systems with Causal Inference 
	- [3] Positive feedback loops lead to concept drift in machine learning systems 
	- [4] Задача 119, Моя первая научная статья 2023, https://arxiv.org/abs/2405.02726 
	- [5] PyTorch NCF, NeuMF etc, https://github.com/guoyang9/NCF, LibMF
	- [6] см. hidden feedback loop user drift recommender systems в scholar.google.com
* **Базовый алгоритм:** 
Исходный код - доработанная версия кода из [3, 4] при решении [5].
* **Новизна:** 
В сравнении с [6] впервые применяем многократное машинное обучение для изучения эффектов самоорганизации в многоагентных системах с рекомендательным алгоритмом. Разработанная имитационная модель и полученные условия существования скрытой петли обратной связи позволят предсказывать долгосрочное поведение такой системы. 
* **Авторы:**  
	* Консультант - Веприков А.С.,
	* Эксперт - Хританков А.С.

## Задача 1XX (новая) (проект пятого курса, Игнашин Игорь)
* __Title__: Adaptive Loss Scaling for Splitting Mods
* __Problem__: In machine learning, numerous challenges can degrade model performance, including noisy features in tabular data and incorrect labeling in reinforcement learning from human feedback. Various approaches exist to mitigate these issues; however, this project proposes addressing them through adaptive loss scaling.  

    The core idea is to weight the overall loss function at the sample level, with these weights being learned on a unit simplex. This approach formulates the problem as a minimax optimization task. As a result, the learned weights converge to a certain distribution, which may exhibit multiple modes corresponding, for example, to noisy samples or poorly labeled data.  
    
    To improve data quality, the project suggests correcting target labels or discarding samples associated with these problematic modes, followed by retraining on the refined dataset and potentially ensembling models. This strategy is expected to enhance model performance by generating a higher-quality dataset.
* __Data__:
	- Tabular Data
	- RLHF data
	- Some toys datasets
    
* __Reference__:
	- Mirror-Prox Algorithm with Linear Convergence Rate and its Application for Dynamic Loss Scaling.
	- Tabular DL reference:
		- [First paper about Tabular DL (On embeddings for numerical features, 2023 oct)](https://arxiv.org/pdf/2203.05556)
		- [Second paper (Tabred, 2024 jun)](https://arxiv.org/pdf/2406.19380v1)
		- [Third paper (TabM, 2024 nov)](https://arxiv.org/pdf/2410.24210)
* __Baseline__:  Some benchmark model corresponding to the selected dataset, trained on it.
* __Proposed solution__: Improvement of the acquisition pipelines of benchmark models: Apply ALSO optimizer to the benchmark model, discard some data according to the obtained weights, or replace incorrect targets, continue training the model on the corrected dataset but with a normal optimizer.
* __Novelty__: The novelty lies in the approach of discarding one or more data modes corresponding to different modes of weight distribution at losses in the minimax setting.
* __Authors__:
    - Ignashim Igor
    - Aleksandr Beznosikov

## Задача 1XX (новая) (проект пятого курса, Вознюк Анастасия)
* __Название__: Использование методов подсчета неопределенности для борьбы с атаками на детекторы машинно-сгенерированного текста
* __Задача__: Для того, чтобы обойти детекторы машинно-сгенерированного текста,  иногда используются различные атаки. Но с другой стороны, эти же атаки можно использовать, чтобы проверять устойчивость детекторов. Существуют несколько типов атак, некоторые из которых детекторы легко обнаружуивают. Многие атаки можно обходить если дооюучивать детектор на текстах с этими аатками, однако хотелось бы найти подход, который бы этого не требовал. Хочется проверить гипотезу, что мы можем использовать методы подсчета неопределенности для этого в случае, когда у нас есть только текст и в случае, когда еще есть и к доступ к внутренним состояниям модели.
* __Данные__: Датасет с атаками: https://github.com/liamdugan/raid ([paper](https://arxiv.org/pdf/2405.07940) )
* __Литература__:
	- **Методы подсчета определенности:** хорошая обзорная [статья](https://arxiv.org/pdf/2311.07383), в аппендиксе который приведены все текущие методы подсчета
	- **Статья-вдохновение1:** Как методы подсчета неопределенности работают для обнаружения изображений-дипфейков: [2412](https://arxiv.org/pdf/2412.05897)
	- **Статья-вдохновение2:** Как можно перевзвешивать матрицу внимания для детекции с учетом неопределенности [2501](https://arxiv.org/pdf/2501.03940)
	- При желании, данную проблему можно порассматривать больше с точки зрения математики (пример [pdf](https://openreview.net/pdf?id=jN5y-zb5Q7m)), так как подсчет неопределенности - это задача байесовского моделирования.
* __Авторы__:
	- консультант Вознюк Анастасия, 
	- эксперт Андрей Грабовой

## Задача 1XX (новая) (проект пятого курса, Никитина Мария)
* __Название__: Кодирование дискриминативных и генеративных моделей
* __Задача__: В работе исследуются различные методы энкодинга нейронных сетей, применяемые в дискриминативных и генеративных моделях. Основная цель проекта — имплементация и сравнительный анализ существующих методов энкодинга, представленных в научных статьях. Результатом проведенного исследования ожидается разработанная библиотека, объединяющая различные методы энкодинга, что позволит упростить их применение в практических задачах. В рамках проекта также предлагается изучить возможность комбинирования нескольких методов энкодинга и теоретически обосновать их совместную применимость. Например, рассмотреть ортогональности методов в функциональном пространстве, что может способствовать улучшению качества и эффективности кодирования нейронных сетей.
* __Данные__: CIFAR
* __Литература__:
	- [[1]](https://arxiv.org/pdf/2406.09997)
	- [[2]](https://arxiv.org/pdf/1703.03400)
	- [[3]](https://arxiv.org/pdf/2403.02484)
* __Базовый алгоритм__: [https://github.com/HSG-AIML/SANE](https://github.com/HSG-AIML/SANE) — интересный метод кодирования сеток, подходящий как для генеративных, так и дискриминативных моделей.
* __Авторы__:
	- Консультант: Никитина Мария
	- Эксперт: Бишук Антон

## Задача 1XX (новая) (проект пятого курса, Охотников Никита)
* __Title__: Low-rank self-play fine-tuning for small LLMs
* __Problem__: Fine-tuning of even relatively small LLM takes considerable resources. Different techniques has been proposed in the last years to accelerate that process. The most common approach for SFT stage nowadays is LoRA. However, LLMs greatly benefits from reinforcement learning, which requires human annotators and that requirement might be demanding. Recently RL-based approaches, that do not rely on human preferences were proposed for ~7B models. This project focuses on applying these to smaller ones with limited resources. The goal is to get some gains over traditional SFT without additional need of human annotation.
* __Baseline__:
Small pretrained LLM without RLHF tuning like Qwen2.5-0.5B/1.5B/....
* __Proposed solution__:
Self-Play fIne-tuNing [SPIN](https://arxiv.org/pdf/2401.01335) - an approach using just ground truth data from the dataset along with old model version replies and which is claimed to outperform DPO (common go-to RL-based method) along with [LoRA](https://arxiv.org/pdf/2106.09685).
* __Data__: 
	- [GSM8K](https://huggingface.co/datasets/openai/gsm8k)
	- [Hellaswag](https://huggingface.co/datasets/Rowan/hellaswag)
	- [Winogrande](https://leaderboard.allenai.org/winogrande/submissions/get-started)
	- [MMLU-Pro](https://github.com/TIGER-AI-Lab/MMLU-Pro)
	- [MMLU](https://huggingface.co/datasets/lukaemon/mmlu)
* __Reference__: 
	- [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/pdf/2401.01335)
	- [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685)
	- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)
	- [Qwen2.5 Technical Report](https://arxiv.org/pdf/2412.15115)
* __Authors__:
  	- Expert: Andrey Grabovoy
	- Consultant: Nikita Okhotnikov

## Задача 1XX (новая) (проект пятого курса, Латыпов Ильгам)
* **Title:** Бандиты для Query selection
* **Problem:** 
In today's world, tools are needed for efficient data processing. Databases underlie all such systems, but as their complexity increases, the task of **Query Optimization** arises. In this paper, you will have to figure out this problem, implement a solution based on the Multi-Armed Bandits method proposed in [4], and also suggest a way to improve the proposed algorithm.
* **Data:**
Will be determined after the algorithm is developed.
* **Reference:**
	- [1] Hazan E. et al. Introduction to online convex optimization //Foundations and Trends® in Optimization. – 2016. – Vol. 2. – No. 3-4. – Pp. 157-325.
	- [2] Cesa-Bianchi N., Lugosi G. Prediction, learning, and games. – Cambridge University Press, 2006.
	- [3] Bandits M. A. Introduction to Multi-Armed Bandits.
	- [4] Marcus R. et al. Bao: Making learned query optimization practical //Proceedings of the 2021 International Conference on Management of Data. – 2021. – С. 1275-1288.
* **Baseline:**
Implement [4] and check it in work.
* **Proposed solution:** 
To implement contextual bandit algorithm in [4]. Improve the quality using contextual bandits.
* **Novelty:**
There is a rumour, that the result in [4] is not reproduced in practice. It is necessary to check this and suggest an improvement.
* **Authors:**
	- Expert Yuriy Dorn
	- Consultant: Ilgam Latypov

## Задача 1XX (новая) (проект пятого курса, Zabarianska Iryna)
* **Title:** Game-Theoretic Approaches to Learning Generative Adversarial Networks
* **Problem:** 
	Generative adversarial networks (GANs) [4] are a type of generative model consisting of two opposing neural networks: a generator and a discriminator. These networks engage in competition through an adversarial process, which can be framed as a stochastic Nash 	equilibrium problem (SNEP). Given the difficulties associated with the training process, it is essential to develop robust algorithms to compute the equilibrium. 

	One effective method for solving a SNEP is to reformulate it as a Stochastic Variational Inequality (SVI) [2], [3]. The benefit of this approach lies in the availability of numerous algorithms designed to solve SVIs, some of which have already been applied to 	GANs [2], [1].

	The goal of this project is to investigate various algorithms for solving the SNEP problem. For many of these algorithms, convergence has been proven, but convergence rate estimates have not been obtained (e.g., the stochastic algorithms from paper [1]). It is 	proposed to model and compare different algorithms, formulate hypotheses, and obtain estimates for the convergence rate.
* **Reference:**
	- [1] B. Franci and S. Grammatico, Training generative adversarial networks via stochastic Nash games, IEEE Trans. Neural Netw. Learn. Syst., vol. 34, no. 3, pp. 1319–1328, Mar. 2023.
	- [2] G. Gidel et al., A variational inequality perspective on generative adversarial networks, arXiv:1802.10551, 2018.
	- [3] Q. Tao et al., Stochastic learning via optimizing the variational inequalities, IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 10, pp.1769–1778, 2014.
	- [4] I. Goodfellow et al., Generative adversarial nets, in Advances in Neural Inf. Processing Syst., 2014, pp. 2672–2680.
* **Authors:**
	- Consultant: Zabarianska Iryna

## Задача 1XX (новая) (проект пятого курса, Бабкин Петр)
* __Название__: Эволюционный метод создания ансамбля нейронных сетей с использованием предикаторных функций
* __Задача__: Ансамбли нейронных сетей обладают более высоким предсказательным потенциалом по сравнению с одиночными нейронными сетями. Однако пространство возможных архитектур нейронных сетей само по себе огромно, а пространство поиска ансамблей растет экспоненциально с увеличением размера ансамбля. Это делает задачу поиска оптимального ансамбля крайне ресурсоемкой. Таким образом, необходимо разработать методы эффективного поиска ансамблей, которые минимизируют вычислительные затраты.
* __Данные__:
	- [CIFAR-100](https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR100.html) — стандартный датасет для задач классификации изображений.
	- [AE-CNN-code](https://github.com/yn-sun/cnn-ga) — код эволюционного алгоритма, на базе которого предлагается проводить исследования.
* __Литература__:
	- [1] [AE-CNN](https://ieeexplore.ieee.org/abstract/document/9075201) — современный (SOTA) эволюционный алгоритм для поиска архитектуры нейронной сети.
	- [2] [E2EPP](https://ieeexplore.ieee.org/abstract/document/8744404) — пример эволюционного алгоритма с предикаторной функцией для поиска одной архитектуры.
	- [3] [SSENAS](https://www.aimspress.com/aimspress-data/era/2024/2/PDF/era-32-02-050.pdf) — использование предикаторной функции для оценки сходства моделей, однако в статье рассматривается поиск только одной модели.
* __Базовый алгоритм__: Сравнение ансамбля с одиночной моделью, полученной с помощью эволюционного алгоритма. Сравнение предложенного метода построения ансамбля с другими методами создания ансамблей.
* __Предлагаемое решение__: В работе предлагается разработать алгоритм создания ансамбля нейронных сетей на основе одного из современных подходов к поиску архитектур — эволюционного алгоритма с использованием суррогатных функций. Эти функции будут использоваться для оценки предсказательной способности моделей. Основной акцент будет сделан на исследовании суррогатной функции, которая сможет отбирать для следующего поколения не только архитектуры с высокой предсказательной способностью, но и достаточно разнообразные, чтобы обеспечить эффективность ансамбля.
* __Новизна__: Исследование свойств суррогатной функции для построения оптимального ансамбля.
* __Авторы__:
  	* Эксперт: Бахтеев Олег
  	* Консультант: Бабкин Пётр

## Задача 1XX (новая) (проект пятого курса, Киселев Никита)
* __Название__: Сходимость поверхности функции потерь в трансформерных архитектурах нейронных сетей
* __Задача__: Обучение нейронной сети подразумевает поиск точки минимума функции потерь, которая задает поверхность в пространстве параметров модели. Свойства этой поверхности определяются выбранной архитектурой, функцией потерь, а также данными для обучения. Существующие исследования показывают, что с ростом числа объектов в выборке поверхность функции потерь перестает значимо меняться. В работе предлагается получить оценку на сходимость поверхности функции потерь для трансформерной архитектуры нейронной сети со слоями внимания, а также провести вычислительные эксперименты, подтверждающие полученные теоретические результаты.
* __Литература__:
    - [1] [Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes](https://arxiv.org/abs/2409.11995): в работе предлагается использовать Гессиан нейронной сети для оценки сходимости, доказываются результаты для полносвязной сети
    - [2] [Attention Is All You Need](https://arxiv.org/abs/1706.03762): базовая работа по архитектуре трансформер и механизму внимания
    - [3] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929): применение трансформерных архитектур для классификации изоражений — Vision Transformer (ViT)
* __Данные__: Предлагается проводить вычислительный эксперимент на задаче классификации изображений [3], используя наборы данных
    - [MNIST](https://pytorch.org/vision/0.20/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST)
    - [FashionMNIST](https://pytorch.org/vision/0.20/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST)
    - [CIFAR10](https://pytorch.org/vision/0.20/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10)
    - [CIFAR100](https://pytorch.org/vision/0.20/generated/torchvision.datasets.CIFAR100.html#torchvision.datasets.CIFAR100)
* __Базовый алгоритм__: Использовать подход, предложенный в [1], при этом рассмотреть вместо последовательности линейных слоев один трансформерный блок с механизмом внимания [2]
* __Решение__:
    - Рассмотреть нейронную сеть, состояющую из последовательности трансформеных блоков с механизмом внимания, представить ее в виде произведения матричных преобразований
    - Используя полученное представление, вывести формулу для Гессиана функции потерь в задаче многоклассовой классификации
    - Свести оценку абсолютной разности функции потерь к норме разности Гессианов в точке локального минимума
* __Новизна__: Ранее никем не были получены такие оценки сходимости. Развитие исследований в этом направлении позволит связать размер модели и необходимое количество данных для ее обучения.
* __Авторы__:
    - Консультант: Никита Киселев (tg: [@kisnikser](https://t.me/kisnikser))
    - Эксперт: Андрей Грабовой

## Задача 1XX (новая) (проект пятого курса, Киселев Никита)
* __Название__: Сравнение Монте-Карло оценок в различных низкоразмерных подпространствах для оценки сходимости поверхности функции потерь
* __Задача__: Обучение нейронной сети подразумевает поиск точки минимума функции потерь, которая задает поверхность в пространстве параметров модели. Свойства этой поверхности определяются выбранной архитектурой, функцией потерь, а также данными для обучения. Существующие исследования показывают, что с ростом числа объектов в выборке поверхность функции потерь перестает значимо меняться. Один из возможных способов практической оценки сходимости — численно измерять изменение поверхности в окрестности локальных минимумов. Делать это можно, например, методом Монте-Карло, однако сэмплирование в пространстве параметров модели осложняется его большой размерностью. В работе предлагается сравнить несколько подпространств, в которых можно производить сэмплирование: натянутое на случайные вектора, собственное подпространство Гессиана нейронной сети и другие. Вычислительный эксперимент на задаче классификации изображений позволит обозначить практическое применение предложенных методов.
* __Литература__:
    - [1] [Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes](https://arxiv.org/abs/2409.11995): в работе предлагается использовать Гессиан нейронной сети для оценки сходимости, доказываются результаты для полносвязной сети
    - [2] Работа консультанта и эксперта, находящаяся на стадии подачи в журнал: наиболее релевантная к теме задачи
* __Данные__: Предлагается проводить вычислительный эксперимент на задаче классификации изображений, используя наборы данных
    - [MNIST](https://pytorch.org/vision/0.20/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST)
    - [FashionMNIST](https://pytorch.org/vision/0.20/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST)
    - [CIFAR10](https://pytorch.org/vision/0.20/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10)
    - [CIFAR100](https://pytorch.org/vision/0.20/generated/torchvision.datasets.CIFAR100.html#torchvision.datasets.CIFAR100)
* __Базовый алгоритм__: Случайно выбирать несколько направлений в пространстве параметров, а затем сэмплировать точки в подпространстве, натянутом на эти вектора. Теоретические результаты и код для базового эксперимента в определенном виде уже имеются.
* __Решение__:
    - Расширить уже полученные результаты [2] для сэмплирования во всем пространстве параметров, вводя матрицу проективного преобразования на выбранное подпространство
    - Рассматривая различные матрицы проекции, сравнить между собой эти способы с точки зрения оценки на скорость сходимости поверхности функции потерь [1]
* __Новизна__: Ранее никем не проводилось такое сравнение. Развитие исследований в этом направлении позволит связать размер модели и необходимое количество данных для ее обучения.
* __Авторы__:
    - Консультант: Никита Киселев (tg: [@kisnikser](https://t.me/kisnikser))
    - Эксперт: Андрей Грабовой

<!-- # Старые задачи -->

<!-- ## Задача 112  (OLD)
* **Название:** Моделирование показания FMRI по видео показанному человеку
* **Описание проблемы:** Требуется построить модель зависимости показания датчиков FMRI и видеоряду, который в этот момент просматривает человек.
* **Данные:** Выборка для аппроксимации представлена в работе J. Berezutskay, в которой присутствуют различные типы параллельных сигналов.
* **Литература**
	- Berezutskaya J., et al Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film // Sci Data 9, 91, 2022.
	- [Код предшественников](https://github.com/intsystems/CreationOfIntelligentSystems_video_fMRI)
* **Базовый алгоритм:** Запуск кода на основе трансформер моделей.
* **Новизна:** Анализ зависимости между показаниями датчиков и восприятиям внешнего мира человеком. Требуется проверить гипотезу зависимости между данными, а также предложить метод апроксимации показаний FMRI по просматриваемому видеоряду.
* **Авторы:** Эксперт Грабовой Андрей -->

<!-- ## Задача 113  (OLD)
* **Название:** Моделирование показания FMRI по звуковому ряду, который слышит человек 
* **Описание проблемы:** Требуется построить модель зависимости показания датчиков FMRI и звуковому сопровождению, который в этот момент прослушивает человек.
* **Данные:** Выборка для аппроксимации представлена в работе J. Berezutskay, в которой присутствуют различные типы параллельных сигналов.
* **Литература**
	- Berezutskaya J., et al Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film // Sci Data 9, 91, 2022.
	- [Код предшественников](https://github.com/intsystems/CreationOfIntelligentSystems_sound_fMRI)
* **Базовый алгоритм:** Запуск кода на основе трансформер моделей.
* **Новизна:** Анализ зависимости между показаниями датчиков и восприятиям внешнего мира человеком. Требуется проверить гипотезу зависимости между данными, а также предложить метод апроксимации показаний FMRI по прослушиваемому звуковому ряду.
* **Авторы:** Эксперт Грабовой Андрей -->

<!--## Задача 114   (OLD)
* **Название:** Моделирование динамики физических систем с помощью Physics-Informed Neural Networks
* **Задача** Решается задача выбора оптимальной модели предсказания динамики физической системы. Под динамикой системы понимается изменение во времени параметров системы. Нейронные сети не имеют априорных знаний о моделируемой системе, что не позволяет получить оптимальные параметры, учитывающие физические законы. Лагранжева нейронная сеть учитывает закон сохранения энергии при моделировании динамики. В данной работе предлагается Нётеровская агранжева нейронная сеть, учитывающая законы сохранения импульса и момента импульса в дополнение к закону сохранения энергии. Показано, что для данной задачи Нётеровская Лагранжева нейронная сеть является оптимальной среди полносвязной модели нейронной сети, нейронной сети с долговременной кратковременной памятью и Лагранжевой нейронной сетью. Сравнение моделирования проводилось на искусственно сгенерированных данных для системы двойного маятника, которая является простейшей хаотической системой. Результаты экспериментов подтверждают гипотезу, что внесение априорного знания о физике системы повышает качество модели.
* **Описание проблемы:** Породить набор сверток по имеющимся данным и выбрать лучшую с помощью методов снижения порядка и размерности пространств. 
* **Данные:** Биомедицинские данные акселерометра и гироскопа, океанические течения, движение барханов, воздушные потоки
* **Литература**
	- [Базовая работа содержит ссылки.](https://github.com/severilov/master-thesis/blob/main/doc/Severilov2022MasterThesis_rus.pdf)
* **Базовый алгоритм:** Нейросетевой, лагранжевы нейросети.
* **Решение:** Нетерова нейросеть. 
* **Новизна:** Предложенная сеть учитывает симметрию
* **Авторы:** Эксперты - Северилов, консультант - Панченко-->



<!--## Задача 119   (OLD)
* **Название** Анализ динамики многократного обучения 
* **Описание проблемы** Рассмотрим задачу многократного обучения с учителем, в которой обучающая выборка не фиксирована, а обновляется в зависимости от предсказаний обученной модели на тестовой выборке. Для процесса многократного обучения, предсказания и обновления выборки строим математическую модель и исследуем свойства этого процесса на основе построенной модели. Пусть f(x) - функция плотности распределения признаков, G - алгоритм обучения модели, формирования предсказаний на тестовой выборке и подмешивания предсказаний в обучающую выборку, в результате применения которого изменяется распределение признаков. Пусть задано пространство неотрицательных гладких функций F(x), интеграл которых на R^n равен единице Рассмотрим автономную (не зависит от времени явно) динамическую систему (есть выделенная переменная - номер шага, которая возрастает),  на шаге t и t+1 которой выполняется соотношение f_{t+1}(x) = G(f_{t})(x), где G(f) - оператор эволюции на пространстве указанных функций F и известна начальная функция f_0(x), Вообще говоря, G может быть произвольным оператором, не обязательно гладким и/или непрерывным.
    - **Вопрос 0.** Найти условия на оператор G, при которых образ G лежит в том же классе функций плотности распределений F. 
    В частности, должен ли G быть ограниченным, операторная норма ||G|| <= 1, для того, чтобы образ G(f) \in F также был функцией плотности распределения для любой f из F? Существует ли в пространстве F единица относительно оператора G и что будет единичной функцией f в таком F?
    - **Вопрос 1.** При каких условиях на G будет существовать такое t_0, что для всех t > t_0 хвост последовательности {f} будет ограничен?
    - **Вопрос 2.** При каких условиях оператор G будет иметь неподвижную точку?
* **Данные** В вычислительном эксперименте предлагается проверить существенность ограничения / значимость условий, при которых получен ответ на вопросы 0-2. Например, для задачи линейной регрессии и/или регрессии с многоуровневой полносвязной нейронной сетью при разных долях подмешиваемых в обучающую выборку предсказаниях на синтетических наборах данных.
* **Авторы** Ментор, эксперт - Хританков А.С., эксперт - Афанасьев А.П.
* **Литература**
    - Khritankov A., Hidden Feedback Loops in Machine Learning Systems: A Simulation Model and Preliminary Results, https://doi.org/10.1007/978-3-030-65854-0_5
    - Khritankov A.. Pilkevich A. Existence Conditions for Hidden Feedback Loops in Online Recommender Systems, https://doi.org/10.1007/978-3-030-91560-5_19
    - Каток А.Б., Хасселблат Б. Введение в современную теорию динамических систем.1999. 768 с. ISBN 5-88688-042-9.
    - Немыцкий В. В., Степанов В. В. Качественная теория дифференциальных уравнений, год изд.: 1974-->

<!--## Задача 122  (OLD)
* **Задача:** Снижение размерности пространства в задаче генеративного моделирования с помощью обратимых моделей.
* **Описание задачи:** Пример задачи генеративного моделирования - генерация изображений. Некоторые виды новых моделей, такие как нормализационные потоки или диффузионные модели, задают обратимые преобразования. Но при этом работают они в пространстве очень высокой размерности. Предлагается совместить 2 подхода: снижения размерности и генеративного моделирования. 
* **Данные:** Любой датасет изображений (MNIST/CIFAR10).
* **Новизна:** Понизив размерность, можно добиться существенного ускорения генеративных моделей, что позволит понизить сложность таких моделей.
* **Автор:** Роман Исаченко-->

<!--## Задача 123 (OLD)
* **Задача:** Анализ смещения распределения в задаче контрастного распределения.
* **Описание задачи:** Есть такая же задача, как обучение представлений (Representation learning). Один из самых популярных подходов для решения данной задачи контрастное обучение (Contrastive learning). При этом в данных, на которых мы учимся часто есть ошибки в разметке: false positive/false negative. Предлагается проанализировать различные способы устранения этих смещений, вызванных ошибками. А также исследовать свойства предложенных моделей.
* **Данные:** Любой датасет изображений (MNIST/CIFAR10).
* **Новизна:** Текущие модели очень чувствительны к ошибкам. Если получится учесть смещение в распределениях, многие методы ранжирования товаров сильно вырастут в качестве.
* **Автор:** Роман Исаченко-->

<!--## Задача 124 (OLD)
* **Задача:** Ускорение семлирования из диффузионных моделей с помощью состязательных сетей
* **Описание задачи:** Самая популярная генеративная модель на сегодняшний день - диффузионная модель. Главный ее недостаток - скорость семплдирования. Для семплирования 1 картинки нужно прогнать 1 нейросеть 100-1000 раз. Есть способы ускорения этого процесса. Один из таких способов - использование состязательных сетей. Предлагается развить данный метод и исследовать различные способы задания функционала для семплирования
* **Данные:** Любой датасет изображений (MNIST/CIFAR10).
* **Новизна:** Ускорив диффузионные модели, они станут еще более популярными и проще в использовании.
* **Автор:** Роман Исаченко-->

<!--## Задача 126 (OLD)
* **Название:** Детекция изменения стиля машинной генерации
* **Описание проблемы:** Требуется предложить метод детекции 
* **Данные:** Выборка для аппроксимации представлена в работе J. Berezutskay, в которой присутствуют различные типы параллельных сигналов.
* **Литература:**
	- G. Gritsay, A. Grabovoy, Y. Chekhovich. Automatic Detection of Machine Generated Texts: Need More Tokens // Ivannikov Memorial Workshop (IVMEM), 2022.
	- M. Kuznetsov, A. Motrenko, R. Kuznetsova, V. Strijov. Methods for intrinsic plagiarism detection and author diarization // Working Notes of CLEF, 2016, 1609 : 912-919.
	- [Конкурс RuATD.](https://www.dialog-21.ru/en/dialogue-evaluation/competitions/dialogue-evaluation-2022/ruatd-2022/)
* **Базовый алгоритм:**
	- Использование результатов конкурса RuATD в качестве базовых моделей для классификации предложений.
	- Использовать метод из работы Kuznetsov et all.
* **Новизна:** Предложить метод детекции машиносгенерированных фрагментов в тексте используя методы изменения стиля написания.
* **Авторы:** Эксперт Грабовой Андрей-->

<!--## Задача 129  (OLD)
* **Название:** Пространственно-временное прогнозирование с помощью сверточных сетей и тензорных разложений 
* **Описание проблемы:** Породить набор сверток по имеющимся данным и выбрать лучшую с помощью методов снижения порядка и размерности пространств. 
* **Данные:** Потребление и цена электроэнергии, океанические течения, движение барханов, воздушные потоки
* **Литература**
	- [http://irep.ntu.ac.uk/id/eprint/32719/1/PubSub10184_Sanei.pdf](Tensor-based Singular Spectrum Analysis for Automatic Scoring of Sleep EEG)
	- [https://ieeexplore.ieee.org/document/6661921](Tensor based singular spectrum analysis for nonstationary source separation)
* **Базовый алгоритм:** Гусеница, тензорная гусеница.
* **Решение:** Найти мультиериодический временной ряд, построить его тензорное представление, разложить в спектр, собрать, показать прогноз.
* **Новизна:** Показать, что мультилинейная модель является удобным способом построения сверток для измерений в пространстве и времени.
* **Авторы:** Надежда Алсаханова-->

<!--## Задача 130  (OLD)
* Название: Автоматическое выделение терминов для тематического моделирования
* Задача: Построить модель ATE (Automatic Term Extraction) для автоматического выделения словосочетаний, являющихся терминами предметной области, в текстах научных статей. Предполагается использовать эффективные методы выделения коллокаций (TopMine или более современные) и тематические модели для определения «тематичности» словосочетания. Модель должна обучаться без учителя (unsupervised).
* Данные: 
	- Коллекция научных статей в области машинного обучения.
        - Размеченные статьи с выделенными терминами для оценивания моделей.
* Литература:
    1. El-Kishky A., Song Y., Wang C., Voss C. R., Han J. Scalable topical phrase mining from text corpora // Proc. VLDB Endowment. _ 2014._ Vol. 8, no. 3._ Pp. 305_316.
    2. Воронцов К.В. "Вероятностное тематическое моделирование: теория, модели, алгоритмы и проект BigARTM" (http://www.machinelearning.ru/wiki/images/d/d5/Voron17survey-artm.pdf)
    3. Николай Шаталов. Методы обучения без учителя для автоматического выделения составных терминов в текстовых коллекциях. 2019. ВМК МГУ.
    4. Владимир Полушин. Тематические модели для ранжирования рекомендаций текстового контента. 2017. ВМК МГУ.
    5. Hanh Thi Hong Tran, Matej Martinc, Jaya Caporusso, Antoine Doucet, Senja Pollak. The Recent Advances in Automatic Term Extraction: A survey. 2023. https://arxiv.org/abs/2301.06767
* Базовые алгоритмы:
    • Метод поиска коллокаций TopMine 
    • Библиотека тематического моделирование BigARTM.
    • Современные методы на основе нейросетевых моделей языка
* Решение: Применение алгоритма поиска коллокаций TopMine с последующей фильтрацией по критерию тематичности. Подбор гиперпараметров тематической модели и критерия тематичности. Сравнение данного подхода с современными методами на основе нейросетевых моделей языка. 
* Новизна: Предшествующие исследования предлагаемого подхода показали хорошие результаты как по полноте, так и по вычислительной эффективности. Однако они до сих пор не сравнивались с нейросетевыми моделями. 
* **Авторы:** Полина Потапова, Воронцов К.В.-->

<!--## Задача 131  (OLD)
* **Название:** Итеративное улучшение тематической модели с обратной связью от пользователя
* **Задача:**  Тематическое моделирование широко используется в социо-гуманитарных исследованиях для понимания тематической структуры больших текстовых коллекций. Типичный сценарий использования предполагает, что пользователь оценивает темы как релевантные, нерелевантные и мусорные. Если количество мусорных тем слишком велико, то пользователь пытается построить другую модель. Задача в том, чтобы при каждом таком перестроении использовать пользовательскую разметку таким образом, чтобы релевантные темы сохранились, из нерелевантных и мусорных тем по возможности выделились новые релевантные, и мусорных тем стало как можно меньше. 
* **Данные:** В качестве данных подойдёт любая коллекция текстов на естественном языке, про которую известна тематическая структура (сколько примерно тем, сколько документов по разным темам). Например, можно взять коллекцию новостей Lenta, дамп Википедии, посты с Хабрахабра, 20 Newsgroups, Reuters, статьи с ПостНауки. 
Тематика коллекции должна быть интересна самому исследователю, чтобы была мотивация оценивать темы вручную.  
* **Литература:** *
*  Воронцов К.В. "Вероятностное тематическое моделирование: теория, модели, алгоритмы и проект BigARTM" (http://www.machinelearning.ru/wiki/images/d/d5/Voron17survey-artm.pdf)
* Alekseev V. et al. "TopicBank: Collection of coherent topics using multiple model training with their further use for topic model validation" (https://www.sciencedirect.com/science/article/pii/S0169023X21000483)
* **Решение:** Использование библиотеки тематического моделирование BigARTM. Использование регуляризаторов сглаживания и декоррелирования. Разработка способов инициализации при перестроении тематических моделей. Поиск готового инструмента или разработка простого, быстрого, удобного способа просмотра и разметки тем. 
* **Новизна:** Проблема неединственности и неустойчивости моделей до сих пор не имеет окончательного решения в вероятностном тематическом моделировании. Предлагаемое исследование является важным шагом к построению моделей с максимальным числом интерпретируемых тем, содержательно полезных с точки зрения гуманитарного исследования. 
* **Авторы:**  Василий Алексеев, Воронцов К.В.-->

<!--## Задача 132  (OLD)
* **Название:** Ранжирование научных статей для полуавтоматического реферирования
* **Задача:** Построить модель ранжирования, которая принимает на входе подборку текстов научных статей и выдаёт на выходе последовательность их упоминания в реферате. 
* **Данные:**
        - В качестве обучающей выборки используются обзорные разделы (например, Introduction и Related Work) статей из коллекции S2ORC (81.1M англоязычных статей). Объект обучающей выборки – это последовательность ссылок на статьи из списка литературы, упоминаемые в обзорных разделах. Для каждого документа есть набор мета данных - год публикации, журнал, число цитирований, число цитирований автора и др. Также, имеется abstract и, возможно, полный текст статьи.
        - В качестве метрики используется Коэффициент ранговой корреляции Кендалла.
* **Литература:**
	- [Крыжановская С. Ю. «Технология полуавтоматической суммаризации тематических подборок научных статей»](http://www.machinelearning.ru/wiki/images/e/ed/Kryzhanovskaya22msc.pdf)
	- [Власов А. В.  «Методы полуавтоматической суммаризации подборок научных статей»](http://www.machinelearning.ru/wiki/images/6/6d/Vlasov2020MSThesis.pdf)
	- [Крыжановская С. Ю., Воронцов К. В. «Технология полуавтоматической суммаризации тематических подборок научных статей»](http://www.machinelearning.ru/wiki/images/f/ff/Idp22.pdf, стр. 371)
	- [S2ORC: The Semantic Scholar Open Research Corpus](https://aclanthology.org/2020.acl-main.447.pdf)
* **Базовые алгоритмы:**
	- Попарные (pair-wise) методы ранжирования
	- Градиентный бустинг
* **Решение:** Простейшим решением является ранжирование статей в хронологическом порядке, по году их публикации. Для решения задачи предлагается построить модель ранжирования на основе градиентного бустинга. В качестве признаков можно использовать год публикации, цитируемость статьи, цитируемость её авторов, семантическая близость публикации к обзору, к его локальному контексту, и т. д. 
* **Новизна:** Задача является первым этапом для полуавтоматического реферирования тематических подборок научных публикаций (machine aided human summarization, MAHS). После того, как сценарий реферата построен, система генерирует для каждой статьи фразы-подсказки, из которых пользователь выбирает фразы для продолжения своего реферата. 
* **Автор:** Крыжановская Светлана, Константин Воронцов-->

<!-- ## Задача 133  (OLD)
* **Название**: Модели диффузии в задаче генерации структуры молекулы с оптимальной энергией
* **Задача**: Для органической маленькой молекулы (количество атомов меньше 100), знание только топологии молекулярного графа недостаточно для получения пространственной структуры. Молекула может иметь множество возможных конфигураций (конформеров), каждая из которых соответствует локальному минимумуму потенциала. На практике, наибольший интерес представляю наиболее устойчивые конформеры, которые имеют минимальную энергию. Исследования последних лет показывают успехи применнения моделей диффузии для генерации молекулярных структур. Данный подход показывает передовые результаты в задаче генерации молекул и их конформеров для малого количества тяжелых атомов (QM9 dataset - до 9 тяжелых атомов в молекуле), а также оценке связывания молекулы и белка. Предлагается построить модель генерации конформеров с минимальной энергией для молекул большего размера.    
* **Данные**: Базовый датасет - QM9 
* **Литература**: 
1) Разные теоретические подходы к модели диффузии: https://arxiv.org/abs/2011.13456
2) Диффузия в генерации молекул: https://arxiv.org/abs/2203.17003
3) Диффузия в задаче связывания белка и молекулы: https://arxiv.org/abs/2210.01776
4) Дифузия в задаче генерации конформеров: https://arxiv.org/abs/2203.02923
5) Туториал по эквиваринтным нейронным сетям: https://arxiv.org/abs/2207.09453
* **Базовой алгоритм**: GeoDiff[4]. 
* **Решение**: Реализовать генерацию конформера аналогично DiffDock[3] для QM9 датасета. Проверить работоспособность модели для молекул большего размера. 
* **Новизна**: Новизна работы заключается в дизайне модели для генерации комформеров большого размера, имеющее большое практического значение. 
* **Автор:** Филипп Никитин -->

<!--## Задача 135 (OLD, will ask for a new problem)
* **Название**: Меры близости в задачах self-supervised learning
* **Задача**: Идея self-supervised learning состоит в решении искусственно подобранной задачи для получения полезных представлений по данным без разметки. Один из наиболее популярных подходов - использование contrastive learning, в ходе которого модель обучается минимизировать расстояние между представлениями аугментированных копий одного и того же объекта. Цель задачи - исследовать качество получаемых представлений в зависимости от выбора меры близости (similarity measure), используемой при обучении, и предложить свой вариант измерения расстояния
* **Данные**: CIFAR-100
* **Литература**: 
	- [Решение с использованием квадрата евклидова расстояния](https://arxiv.org/abs/2105.04906)
	- [Решение с использованием косинусного сходства](https://arxiv.org/abs/2011.10566)
	- [Решение, основанное на информационном принципе](https://arxiv.org/abs/2103.03230)
* **Базовой алгоритм**: VicReg, Barlow Twins, SimSiam
* **Решение**: Одним из вариантов расстояния, которое можно предложить, является аналог метрики Васерштейна, который позволил бы учитывать зависимости между признаками. 
* **Новизна**: Предложить новый способ определения меры близости, который был бы теоретически обоснован/способствовал получению представлений с заданными свойствами
* **Авторы**: Полина Барабанщикова-->

<!--## Задача 136   (OLD)
* **Title:** Stochastic Newton with Arbitrary Sampling
* **Problem:** We analyze second order methods solving Empirical Risk Minimization problem of the form min f(x) in R^d. Here x is a parameter vector of some Machine Learning model, f_i(x) is a loss function on i-th training point (a_i,b_i). Our desire to solve it using Newton-type method that requires access to only one data point per iteration. We investigate different sampling strategies of index i_k on iteration k. See description in PDF.
* **Dataset:** It is proposed to use open SVM library as a data for experimental part of the work.
* **References:**
	- Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates
        - Parallel coordinate descent methods for big data optimization
* **Base algorithm:** As a base method it is proposed to use Algorithm 1 from the paper Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates.
* **Solution:** Is is proposed to adjust existing sampling strategies from Parallel coordinate descent methods for big data optimization in this work.
* **Novelty:** In the literature of Second Order methods there are a few works on incremental methods. The idea is to analyze the existing method by applying different sampling strategies. It is known that the proper sampling strategies may improve the performance of a method.
* **Authors:** Islamov Rustem--> 

<!--## Задача 137 (OLD, new problem will appear)
* **Задача:** Binary Neural Networks. Lossless picture quality for binary neural networks in pixel-level tasks. (Бинарные сети в img-to-img задачах)
* **Авторы:** Илья Жариков-->

<!--## Задача 138 (OLD)
* **Задача:** Post Training Quantization. Flexible continuous modification for SOTA post training quantization methods to make them lossless.
* **Авторы:** Илья Жариков-->

<!--## Задача 139 (OLD)
 * **Название**: Дистилляция моделей на многодоменных выборках.
 * **Задача**: Исследуется проблема понижения сложности аппроксимирующей модели при переносе на новые данные меньшей мощности.
 * **Данные**: Выборки MNIST, CIFAR-10, CIFAR-100, Amazon товары.
 * **Литература**: 
 - [Диплом: Камил Баязитов](https://github.com/kbayazitov/Distillation)
 * **Базовой алгоритм**: Базовое решение и эксперименты представлены в дипломе.
 * **Авторы:** Грабовой Андрей-->

<!--## Problem 141 (OLD)
* __Title__: Metric analysis of deep network space parameters
* __Problem__: The structure of a neural work is exhaustive. The dimensionality of the parameter space should be reduced. The autoencoder in the subject of the investigation. Due to the continuous-time nature of the data, we analyze several types of autoencoders. We reconstruct spatial-time data, minimizing the error. 
* __Data__: 
	* Synthetic data sine for 2D visualizaion of the parameter distributions
	* Accelerometer quasiperiodic data
	* Limb movement quasiperiodic data (if any)
	* Video periodic data (cartoon, walking persona)
	* Video, fMRI, ECoG from the s41597-022-01173-0 
* __References__: 
	* [SSA and Hankel matrix construction](http://strijov.com/papers/Grabovoy2019QuasiPeriodicTimeSeries.pdf) or in [wiki](https://en.wikipedia.org/wiki/Singular_spectrum_analysis)
	* [Open multimodal iEEG-fMRI dataset from naturalistic stimulation](https://www.nature.com/articles/s41597-022-01173-0)
	* [Variational autoencoders to estimate parameters](https://arxiv.org/pdf/1606.05908.pdf)
	* RNN in the [5G book](https://arxiv.org/abs/2104.13478)
	* [Neural CDE](https://bit.ly/NeuroCDE)
* __Baseline__: RNN-like variational autoencoder in the criteria: error vs. complexity (number of parameters)
* __Roadmap__:
	* Prepare data so that the reconstruction work on a basic model (like SSA)
	* Estimate expectation and covariance of parameters (using VAE or else, to be discussed)
	* Reduce dimensionality, plot the error/complexity, plot the covariance
	* Run RNN-like model, plot
	* Assign the expectation and covariation matrix to each neuron of the model
	* Plot the parameter space regarding covariance as its metric tensor (end of minimum part)
	* Suggest a dimensionality reduction algorithm (naive part)
	* Run Neuro ODE/CDE model and plot the parameter space
	* Analyse the data distribution as the normalized flow 
	* Suggest the parameter space modification in terms of  the normalized flow (paradoxical part, diffusion model is needed)
	* Compare all models according to the criterion error/complexity (max part)
	* Construct the decoder model for any pair of data like fMRI-ECoG tensor and neuro CDE (supermax part)
* __Proposed solution__: description of the idea to implement in the project
* __Novelty__: Continous-time models are supposed to be simple due to their periodic nature. Since they approximate the vector fields, these models are universal. The model selection for the continuous time is not considered now, but at the time, it is acute for wearable multimedia devices for metaverse and augmented reality. 
* __Supergoal__ To join two encoders in a signal decoding model to reveal the connection between video and fMRI, between fMRI and ECoG.
* __Authors__: Expert-->

## Problem template (EN)
## Problem 101
* __Title__: Title
* __Problem__: Problem description
* __Data__: Data description
* __Reference__: Links to the literature
* __Baseline__: baseline description
* __Proposed solution__: description of the idea to implement in the project
* __Novelty__: why the task is good and what does it bring to science?  (for editorial board and reviewers)
* __Authors__: supervisors, consultants, experts

## Шаблон задачи (RU)
## Задача 101
* __Название__: Название, под которым статья подается в журнал. 
* __Задача__: Описание или постановка задачи. Желательна постановка в виде задачи оптимизации (в формате argmin). Также возможна ссылка на классическую постановку задачи. 
* __Данные__: Краткое описание данных, используемых в вычислительном эксперименте, и ссылка на выборку. 
* __Литература__: Список научных работ, дополненный 1) формулировкой решаемой задачи, 2) ссылками на новые результаты, 3) основной информацией об исследуемой проблеме. 
* __Базовой алгоритм__: Ссылка на алгоритм, с которым проводится сравнение или на ближайшую по теме работу. 
* __Решение__: Предлагаемое решение задачи и способы проведения исследования. Способы представления и визуализации данных и проведения анализа ошибок, анализа качества алгоритма. 
* __Новизна__: Обоснование новизны и значимости идей (для редколлегии и рецензентов журнала). 
