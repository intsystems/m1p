# Problem list

## Задача 112  
* **Название:** Моделирование показания FMRI по видео показанному человеку
* **Описание проблемы:** Требуется построить модель зависимости показания датчиков FMRI и видеоряду, который в этот момент просматривает человек.
* **Данные:** Выборка для аппроксимации представлена в работе J. Berezutskay, в которой присутствуют различные типы параллельных сигналов.
* **Литература**
	- Berezutskaya J., et al Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film // Sci Data 9, 91, 2022.
	- [Код предшественников](https://github.com/intsystems/CreationOfIntelligentSystems_video_fMRI)
* **Базовый алгоритм:** Запуск кода на основе трансформер моделей.
* **Новизна:** Анализ зависимости между показаниями датчиков и восприятиям внешнего мира человеком. Требуется проверить гипотезу зависимости между данными, а также предложить метод апроксимации показаний FMRI по просматриваемому видеоряду.
* **Авторы:** Эксперт Грабовой Андрей

## Задача 113  
* **Название:** Моделирование показания FMRI по звуковому ряду, который слышит человек
* **Описание проблемы:** Требуется построить модель зависимости показания датчиков FMRI и звуковому сопровождению, который в этот момент прослушивает человек.
* **Данные:** Выборка для аппроксимации представлена в работе J. Berezutskay, в которой присутствуют различные типы параллельных сигналов.
* **Литература**
	- Berezutskaya J., et al Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film // Sci Data 9, 91, 2022.
	- [Код предшественников](https://github.com/intsystems/CreationOfIntelligentSystems_sound_fMRI)
* **Базовый алгоритм:** Запуск кода на основе трансформер моделей.
* **Новизна:** Анализ зависимости между показаниями датчиков и восприятиям внешнего мира человеком. Требуется проверить гипотезу зависимости между данными, а также предложить метод апроксимации показаний FMRI по прослушиваемому звуковому ряду.
* **Авторы:** Эксперт Грабовой Андрей

## Задача 114  
* **Название:** Моделирование динамики физических систем с помощью Physics-Informed Neural Networks
* **Описание проблемы:** Породить набор сверток по имеющимся данным и выбрать лучшую с помощью методов снижения порядка и размерности пространств. 
* **Данные:** Биомедицинские данные акселерометра и гироскопа, океанические течения, движение барханов, воздушные потоки
* **Литература**
	- [Базовая работа содержит ссылки.](https://github.com/severilov/master-thesis/blob/main/doc/Severilov2022MasterThesis_rus.pdf)
* **Базовый алгоритм:** Нейросетевой, лагранжевы нейросети.
* **Решение:** Нетерова нейросеть. 
* **Новизна:** Предложенная сеть учитывает симметрию
* **Авторы:** Эксперты - Северилов, Стрижов, консультант - Панченко

## Задача 115  
* **Название:** Дистилляция знаний в глубоких сетях и выравнивание структур моделей 
* **Описание проблемы:** Требуется построить сеть наиболее простой структуры, модель-ученик, используя модель-учитель высокого качества. Показать насколько изменяется точность и устойчивость ученика. Результатом эксперимента является график сложность-точность-устойчивость, где каждая модель является точной. 
* **Данные:** CIFAR-10. Предполагается, что учитель имеет открытую для анализа структуру с большим числом слоев. 
* **Литература**  
	- [Исходная работа Хинтона по дистилляции](https://arxiv.org/abs/1503.02531)
	- [Работы Андрея Грабового](https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=at&paperid=15892&option_lang=rus)
	- [Работа Марии Горпинич](https://link.springer.com/article/10.1134/S00051179220100071)* 
* **Базовый алгоритм:** для сравнения 
    - Обучение (моделей с заданной структурой управляемой сложности) без дистилляции.
    - Обучение (ditto) с дистилляцией Хинтона. 
    - Обучение с переносом по слоям.
    - Обучение с переносом по нейронам. 
* **Решение:** Как в п. 2, только по слоям. Построение пути наименьшей стоимости по нейронам. Считаем ковариационные матрицы каждого нейрона каждого слоя для учителя и для ученика. Предлагаем функцию ошибки, включающую цену пути наименьшей стоимости. Предлагаем способ построить путь наименьшей стоимости. Основная идея: перенос идет по парам нейронов и наиболее похожими распределениями (ожидание и ковариационная матрица) от учителя к ученику. 
* **Новизна:** Предложенный перенос существенно снижает сложность без потери точности и решает проблему взаимозаменяемости нейронов, идентифицируя их. 
* **Авторы:** Эксперт Бахтеев, Стрижов, консультант - Мария Горпинич

## Задача 116  
* **Название:** Нейронные дифференциальные уравнения для моделирования физической активности – выбор моделей
* **Авторы:** Эксперт, консультант - Эдуард Владимиров

## Задача 117  
* **Название:** Поиск зависимости и SSA, теорема Такенса
* **Авторы:** Эксперт Стрижов, консультант - Владимиров, Самохина

## Задача 118  
* **Авторы:** Эксперт Стрижов, консультант - Тихонов

## Задача 119  
* **Название** Анализ динамики многократного обучения 
* **Описание проблемы** Рассмотрим задачу многократного обучения с учителем, в которой обучающая выборка не фиксирована, а обновляется в зависимости от предсказаний обученной модели на тестовой выборке. Для процесса многократного обучения, предсказания и обновления выборки строим математическую модель и исследуем свойства этого процесса на основе построенной модели. Пусть f(x) - функция плотности распределения признаков, G - алгоритм обучения модели, формирования предсказаний на тестовой выборке и подмешивания предсказаний в обучающую выборку, в результате применения которого изменяется распределение признаков. Пусть задано пространство неотрицательных гладких функций F(x), интеграл которых на R^n равен единице Рассмотрим автономную (не зависит от времени явно) динамическую систему (есть выделенная переменная - номер шага, которая возрастает),  на шаге t и t+1 которой выполняется соотношение f_{t+1}(x) = G(f_{t})(x), где G(f) - оператор эволюции на пространстве указанных функций F и известна начальная функция f_0(x), Вообще говоря, G может быть произвольным оператором, не обязательно гладким и/или непрерывным.
    - **Вопрос 0.** Найти условия на оператор G, при которых образ G лежит в том же классе функций плотности распределений F. 
    В частности, должен ли G быть ограниченным, операторная норма ||G|| <= 1, для того, чтобы образ G(f) \in F также был функцией плотности распределения для любой f из F? Существует ли в пространстве F единица относительно оператора G и что будет единичной функцией f в таком F?
    - **Вопрос 1.** При каких условиях на G будет существовать такое t_0, что для всех t > t_0 хвост последовательности {f} будет ограничен?
    - **Вопрос 2.** При каких условиях оператор G будет иметь неподвижную точку?
* **Данные** В вычислительном эксперименте предлагается проверить существенность ограничения / значимость условий, при которых получен ответ на вопросы 0-2. Например, для задачи линейной регрессии и/или регрессии с многоуровневой полносвязной нейронной сетью при разных долях подмешиваемых в обучающую выборку предсказаниях на синтетических наборах данных.
* **Авторы** Ментор, эксперт - Хританков А.С., эксперт - Афанасьев А.П.
* **Литература**
    - Khritankov A., Hidden Feedback Loops in Machine Learning Systems: A Simulation Model and Preliminary Results, https://doi.org/10.1007/978-3-030-65854-0_5
    - Khritankov A.. Pilkevich A. Existence Conditions for Hidden Feedback Loops in Online Recommender Systems, https://doi.org/10.1007/978-3-030-91560-5_19
    - Каток А.Б., Хасселблат Б. Введение в современную теорию динамических систем.1999. 768 с. ISBN 5-88688-042-9.
    - Немыцкий В. В., Степанов В. В. Качественная теория дифференциальных уравнений, год изд.: 1974

## Задача 120  
* **Название:** Дифференцируемый алгоритм поиска ансамблей моделей глубокого обучения с контролем разнообразия
* **Задача:** Рассмотривается задача выбора ансамбля моделей. Требуется предложить метод контроля разнообразия базовых моделей на этапе применения.
* **Данные:** Fashion-MNIST, CIFAR-10, CIFAR-100 datasets
* **Литература:**
	- [Neural Architecture Search with Structure Complexity Control](https://easychair.org/publications/preprint/H5MC)
	- [Neural Ensemble Search via Bayesian Sampling](https://arxiv.org/pdf/2109.02533.pdf)
	- [DARTS: Differentiable Architecture Search](https://arxiv.org/pdf/1806.09055.pdf)
* **Базовой алгоритм:** В качестве базового алгоритма предлагается использовать DARTS [3]. 
* **Решение:** Для контроля разнообразия базовых моделей предлагается использовать гиперсеть [1], которая смещает структурные параметры в терминах дивергенции Йенсена—Шеннона. На этапе применения сэмплируются базовые архитектуры с заданным смещением для построения ансамбля.
* **Новизна:** Предложенный метод позволяет строить ансамбли с любым количеством базовых моделей без дополнительных вычислительных затрат относительно базового алгоритма.
* **Авторы:** К.Д. Яковлев, О.Ю. Бахтеев

## Задача 121  
* **Задача:** построение предиктивной аналитики для сенсоров загрязнений атмосферы.
* **Описание задачи** Есть данные по станциям мониторинга качества воздуха г. Москвы и Московской области (временные ряды). Задача состоит в том, чтобы проверить достижимую предиктивную способность прогноза временных рядов показаний станций по их истории + при подключении дополнительных фичей (учитывать станции в совокупности с учетом их расположения, время суток и выходной/рабочий день, историю и прогноз погоды (ветра))
* **Данные** Реальные данные и симуляции по Москве и МО
* **Авторы:** Артем Михайлов, Владимир Вановский

## Задача 122  
* **Задача:** Снижение размерности пространства в задаче генеративного моделирования с помощью обратимых моделей.
* **Описание задачи:** Пример задачи генеративного моделирования - генерация изображений. Некоторые виды новых моделей, такие как нормализационные потоки или диффузионные модели, задают обратимые преобразования. Но при этом работают они в пространстве очень высокой размерности. Предлагается совместить 2 подхода: снижения размерности и генеративного моделирования. 
* **Данные:** Любой датасет изображений (MNIST/CIFAR10).
* **Новизна:** Понизив размерность, можно добиться существенного ускорения генеративных моделей, что позволит понизить сложность таких моделей.
* **Автор:** Роман Исаченко

## Задача 123 
* **Задача:** Анализ смещения распределения в задаче контрастного распределения.
* **Описание задачи:** Есть такая же задача, как обучение представлений (Representation learning). Один из самых популярных подходов для решения данной задачи контрастное обучение (Contrastive learning). При этом в данных, на которых мы учимся часто есть ошибки в разметке: false positive/false negative. Предлагается проанализировать различные способы устранения этих смещений, вызванных ошибками. А также исследовать свойства предложенных моделей.
* **Данные:** Любой датасет изображений (MNIST/CIFAR10).
* **Новизна:** Текущие модели очень чувствительны к ошибкам. Если получится учесть смещение в распределениях, многие методы ранжирования товаров сильно вырастут в качестве.
* **Автор:** Роман Исаченко

## Задача 124
* **Задача:** Ускорение семлирования из диффузионных моделей с помощью состязательных сетей
* **Описание задачи:** Самая популярная генеративная модель на сегодняшний день - диффузионная модель. Главный ее недостаток - скорость семплдирования. Для семплирования 1 картинки нужно прогнать 1 нейросеть 100-1000 раз. Есть способы ускорения этого процесса. Один из таких способов - использование состязательных сетей. Предлагается развить данный метод и исследовать различные способы задания функционала для семплирования
* **Данные:** Любой датасет изображений (MNIST/CIFAR10).
* **Новизна:** Ускорив диффузионные модели, они станут еще более популярными и проще в использовании.
* **Автор:** Роман Исаченко

## Задача 125
* **Название:** Влияние локдауна на динамику распространения эпидемии
* **Задача:** Введение локдауна считается эффективной мерой по борьбе с эпидемией. Однако вопреки интуиции оказалось, что при определенных условиях локдаун может привести к росту эпидемии. Данный эффект отсутствует для классических моделей распространения эпидемии «в среднем», но был выявлен при моделировании эпидемии на графе контактов. Задача заключается в поиске формульных и количественных соотношений между параметрами, при которых локдаун может привести к росту эпидемии. Необходимо как выявить такие соотношения в моделях SEIRS/SEIR/SIS/etc на основе фреймворка распространения эпидений SEIRS+ (и ее модификации), так и теоретически обосновать соотношения, полученные из конкретных реализаций эпидении.
* **Данные:** Задача предполагает работу с модельными и синтетическими данными: имеются готовые данные, а также предполагается возможность генерации новых в процессе решения задачи. Данная задача относится к unsupervised обучению, поскольку реализация эпидемии на графе контактов имеет высокую долю случаныйх событий, а потому требует проводить анализ в среднем на многих синтетически сгенерированных реализациях эпидемии
* **Литература:** 
    - T. Harko, Francisco S. N. Lobo и M. Mak. «Exact analytical solutions of the
Susceptible-Infected-Recovered (SIR) epidemic model and of the SIR model with
equal death and birth rates»
    - [Код](https://github.com/ryansmcgee/seirsplus)
* **Авторы:** А.Ю. Бишук, А.В. Зухба

## Задача 126
* **Название:** Детекция изменения стиля машинной генерации
* **Описание проблемы:** Требуется предложить метод детекции 
* **Данные:** Выборка для аппроксимации представлена в работе J. Berezutskay, в которой присутствуют различные типы параллельных сигналов.
* **Литература:**
	- G. Gritsay, A. Grabovoy, Y. Chekhovich. Automatic Detection of Machine Generated Texts: Need More Tokens // Ivannikov Memorial Workshop (IVMEM), 2022.
	- M. Kuznetsov, A. Motrenko, R. Kuznetsova, V. Strijov. Methods for intrinsic plagiarism detection and author diarization // Working Notes of CLEF, 2016, 1609 : 912-919.
	- [Конкурс RuATD.](https://www.dialog-21.ru/en/dialogue-evaluation/competitions/dialogue-evaluation-2022/ruatd-2022/)
* **Базовый алгоритм:**
	- Использование результатов конкурса RuATD в качестве базовых моделей для классификации предложений.
	- Использовать метод из работы Kuznetsov et all.
* **Новизна:** Предложить метод детекции машиносгенерированных фрагментов в тексте используя методы изменения стиля написания.
* **Авторы:** Эксперт Грабовой Андрей

## Задача 127  
Егор Шульгин

## Задача 128  
* **Название:** Построение модели глубокого обучения в зависимости от данных задачи
* **Задача:** рассматривается задача оптимизации модели глубокого обучения для нового датасета. Требуется предложить метод оптимизации модели, позволяющий производить порождение новых моделей для нового датасета с небольшими вычислительными затратами.
* **Данные:** CIFAR10, CIFAR100
* **Литература:** 
	- [вариационный вывод для нейронных сетей](https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf)
	- [гиперсети](https://arxiv.org/abs/1609.09106)
	- [похожая работа заточенная под изменение модели в зависимости от заранее заданной сложности](https://www.mathnet.ru/links/71cd2117ce84018e028a939bcd0e1507/ia710.pdf)
* **Базовой алгоритм:** Переобучение модели напрямую. 
**Решение:** Предлагаемый метод заключается в представлении модели глубокого обучения в виде гиперсети (сети, которая генерирует параметры другой сети) с использованием байесовского подхода. Вводятся вероятностные предположения о параметрах моделей глубокого обучения, максимизируется вариационная нижняя оценка байесовской обоснованности модели. Вариационная оценка рассматривается как условная величина, зависящая от информации о данных задачи.
* **Новизна: ** предложенный метод позволяет порождать модели в режиме one-shot (практически без переподготовки) для требуемой задачи, что значительно снижает затраты на оптимизацию и дообучение.
* **Авторы:** Ольга Гребенькова и Олег Бахтеев

## Задача 129  
* **Название:** Пространственно-временное прогнозирование с помощью сверточных сетей и тензорных разложений 
* **Описание проблемы:** Породить набор сверток по имеющимся данным и выбрать лучшую с помощью методов снижения порядка и размерности пространств. 
* **Данные:** Потребление и цена электроэнергии, океанические течения, движение барханов, воздушные потоки
* **Литература**
	- [http://irep.ntu.ac.uk/id/eprint/32719/1/PubSub10184_Sanei.pdf](Tensor-based Singular Spectrum Analysis for Automatic Scoring of Sleep EEG)
	- [https://ieeexplore.ieee.org/document/6661921](Tensor based singular spectrum analysis for nonstationary source separation)
* **Базовый алгоритм:** Гусеница, тензорная гусеница.
* **Решение:** Найти мультиериодический временной ряд, построить его тензорное представление, разложить в спектр, собрать, показать прогноз.
* **Новизна:** Показать, что мультилинейная модель является удобным способом построения сверток для измерений в пространстве и времени.
* **Авторы:** Эксперт - В. В. Стрижов

## Задача 130  
* Название: (Ожидается уточнение) Кроссязычный поиск дубликатов
* Задача: Ставится задача кроссязычного поиска текстового плагиата. Поиск дубликатов оригинального текста осуществуляется среди текстов на 100 различных языках.
* Данные: В качестве обучающей выборки используется выборка научных статей из научной электронной библиотеки eLIBRARY.ru, а так же статьи из онлайн-энциклопедии Wikipedia. В качестве научных рубрикаторов рассматриваются Государственный рубрикатор научно-технической информации (ГРНТИ), Универсальный десятичный классификатор (УДК). В качестве метрик качества поиска используются: средняя частота – частота, усреднённая по контрольным языкам, с которой документ-запрос попадает в топ 10 % документов, среди которых осуществляется поиск средний процент – процент документов, усреднённый по контрольным языкам, попавших в топ 10 % документов-переводов, которые имеют такую же научную рубрику, как документ-запрос
* Литература: Воронцов К. В. Вероятностное тематическое моделирование: обзор моделей и аддитивная регуляризация [http://www.machinelearning.ru/wiki/images/d/d5/Voron17survey-artm.pdf]
* Базовые алгоритмы:
        - Иерархические тематические модели
        - Тематические модели с однопроходной векторизацией документов
* Решение: Для решения задачи поиска была построена мультимодальная тематическая модель. В качестве модальностей использовались 100 языков, а также научные рубрики, к которым относились статьи из обучающих данных. Была проведена серия экспериментов по улучшению метрик качества поиска, в том числе: подбор оптимального способа токенизации, добавление регуляризаторов, подбор функций сравнения тематических векторов, функций ранжирования и др.
* Новизна: В основе большинства систем поиска документов в больших коллекциях лежит векторизация документов коллекции и поискового документа тем или иным способом. Новейшие на данный момент способы векторизации документов обычно ограничиваются одним языком. В таком случае возникает проблема создания единообразной системы получения векторных эмбедингов мультиязыковой коллекции документов. Предложенный подход позволяет обучить тематическую модель кодирующую информацию о распределениях слов в тексте безотносительно их языковой принадлежности. Также на решение действуют ограничения по размеру модели и времени обучения, обусловленные возможностью практического использования описываемой модели.
* **Авторы:** Эксперт - Воронцов, консультант - Полина Потапова 

## Задача 131  
* **Авторы:** Эксперт - Воронцов, консультант - Василий Алексеев

## Задача 132  
* **Название:** Ранжирование научных статей для полуавтоматического реферирования
* **Задача:** Построить модель ранжирования, которая принимает на входе подборку текстов научных статей и выдаёт на выходе последовательность их упоминания в реферате. 
* **Данные:**
        - В качестве обучающей выборки используются обзорные разделы (например, Introduction и Related Work) статей из коллекции S2ORC (81.1M англоязычных статей). Объект обучающей выборки – это последовательность ссылок на статьи из списка литературы, упоминаемые в обзорных разделах. Для каждого документа есть набор мета данных - год публикации, журнал, число цитирований, число цитирований автора и др. Также, имеется abstract и, возможно, полный текст статьи.
        - В качестве метрики используется Коэффициент ранговой корреляции Кендалла.
* **Литература:**
	- [Крыжановская С. Ю. «Технология полуавтоматической суммаризации тематических подборок научных статей»](http://www.machinelearning.ru/wiki/images/e/ed/Kryzhanovskaya22msc.pdf)
	- [Власов А. В.  «Методы полуавтоматической суммаризации подборок научных статей»](http://www.machinelearning.ru/wiki/images/6/6d/Vlasov2020MSThesis.pdf)
	- [Крыжановская С. Ю., Воронцов К. В. «Технология полуавтоматической суммаризации тематических подборок научных статей»](http://www.machinelearning.ru/wiki/images/f/ff/Idp22.pdf, стр. 371)
	- [S2ORC: The Semantic Scholar Open Research Corpus](https://aclanthology.org/2020.acl-main.447.pdf)

* **Базовые алгоритмы:**
	- Попарные (pair-wise) методы ранжирования
	- Градиентный бустинг
* **Решение:** Простейшим решением является ранжирование статей в хронологическом порядке, по году их публикации. Для решения задачи предлагается построить модель ранжирования на основе градиентного бустинга. В качестве признаков можно использовать год публикации, цитируемость статьи, цитируемость её авторов, семантическая близость публикации к обзору, к его локальному контексту, и т. д. 
* **Новизна:** Задача является первым этапом для полуавтоматического реферирования тематических подборок научных публикаций (machine aided human summarization, MAHS). После того, как сценарий реферата построен, система генерирует для каждой статьи фразы-подсказки, из которых пользователь выбирает фразы для продолжения своего реферата. 
* **Автор:** Крыжановская Светлана, Константин Воронцов

## Задача 133  
* **Название**: Модели диффузии в задаче генерации структуры молекулы с оптимальной энергией
* **Задача**: Для органической маленькой молекулы (количество атомов меньше 100), знание только топологии молекулярного графа недостаточно для получения пространственной структуры. Молекула может иметь множество возможных конфигураций (конформеров), каждая из которых соответствует локальному минимумуму потенциала. На практике, наибольший интерес представляю наиболее устойчивые конформеры, которые имеют минимальную энергию. Исследования последних лет показывают успехи применнения моделей диффузии для генерации молекулярных структур. Данный подход показывает передовые результаты в задаче генерации молекул и их конформеров для малого количества тяжелых атомов (QM9 dataset - до 9 тяжелых атомов в молекуле), а также оценке связывания молекулы и белка. Предлагается построить модель генерации конформеров с минимальной энергией для молекул большего размера.    
* **Данные**: Базовый датасет - QM9 
* **Литература**: 
1) Разные теоретические подходы к модели диффузии: https://arxiv.org/abs/2011.13456
2) Диффузия в генерации молекул: https://arxiv.org/abs/2203.17003
3) Диффузия в задаче связывания белка и молекулы: https://arxiv.org/abs/2210.01776
4) Дифузия в задаче генерации конформеров: https://arxiv.org/abs/2203.02923
5) Туториал по эквиваринтным нейронным сетям: https://arxiv.org/abs/2207.09453
* **Базовой алгоритм**: GeoDiff[4]. 
* **Решение**: Реализовать генерацию конформера аналогично DiffDock[3] для QM9 датасета. Проверить работоспособность модели для молекул большего размера. 
* **Новизна**: Новизна работы заключается в дизайне модели для генерации комформеров большого размера, имеющее большое практического значение. 
* **Автор:** Филипп Никитин

## Задача 134  
  * **Название**: Объединение дистилляции моделей и данных
  * **Задача**: Дистилляция знаний - передача знаний из более содержательного представления в компактное сжатое представление. Существует два вида дистилляции знаний. Первый - дистилляция моделей. В этом случае большая модель передает знания (дистиллируется) в маленькую модель. Второй - дистилляция данных. В этом случае создается минимальный набор данных, на котором после обучения модели достигает  качество, сравнимого с обучением на полной выборке . На данный момент нет решения, которое может реализовать одновременную дистилляцию модели и знаний. Поэтому цель задачи предложить базовое решение по дистилляции моделей и сравнить с подходами по дистилляцией моделей и дистилляции данных
  * **Данные**: Выборка рукописных цифр MNIST, Выборка изображений CIFAR-10
  * **Литература**: 
	- [Собрание различных работ по дистилляции данных.](https://github.com/Guang000/Awesome-Dataset-Distillation)
	- [Обзор на методы дистилляции моделей.](https://arxiv.org/pdf/2006.05525.pdf)
	- [Базовое решение по дистилляции знаний](https://georgecazenavette.github.io/mtt-distillation/)
	- [Базовое решение по дистилляции моделей](https://arxiv.org/pdf/1503.02531.pdf)
  * **Базовой алгоритм**: 
	Базовое решение по дистилляции модели - дистилляция Хинтона
	Базовое решение по дистилляции датасетов - Dataset Distillation by Matching Training Trajectories
  * **Решение**: В качестве базового алгоритма предлагается реализовать дистилляцию данных. Затем обучить бОльшую модель на полученных данных и дистиллировать её в маленькую модель. Далее, провести сравнение с исходной моделью и моделью, обученной на дистиллированных данных.
  * **Новизна**: Новизна работы заключается в соединении двух подходов по дистилляции, что не было реализовано ранее
 * **Авторы**:  Андрей Филатов

## Задача 135
* **Название**: Меры близости в задачах self-supervised learning
* **Задача**: Идея self-supervised learning состоит в решении искусственно подобранной задачи для получения полезных представлений по данным без разметки. Один из наиболее популярных подходов - использование contrastive learning, в ходе которого модель обучается минимизировать расстояние между представлениями аугментированных копий одного и того же объекта. Цель задачи - исследовать качество получаемых представлений в зависимости от выбора меры близости (similarity measure), используемой при обучении, и предложить свой вариант измерения расстояния
* **Данные**: CIFAR-100
* **Литература**: 
	- [Решение с использованием квадрата евклидова расстояния](https://arxiv.org/abs/2105.04906)
	- [Решение с использованием косинусного сходства](https://arxiv.org/abs/2011.10566)
	- [Решение, основанное на информационном принципе](https://arxiv.org/abs/2103.03230)
* **Базовой алгоритм**: VicReg, Barlow Twins, SimSiam
* **Решение**: Одним из вариантов расстояния, которое можно предложить, является аналог метрики Васерштейна, который позволил бы учитывать зависимости между признаками. 
* **Новизна**: Предложить новый способ определения меры близости, который был бы теоретически обоснован/способствовал получению представлений с заданными свойствами
* **Авторы**: Полина Барабанщикова 

## Задача 136  
* **Title:** Stochastic Newton with Arbitrary Sampling
* **Problem:** We analyze second order methods solving Empirical Risk Minimization problem of the form min f(x) in R^d. Here x is a parameter vector of some Machine Learning model, f_i(x) is a loss function on i-th training point (a_i,b_i). Our desire to solve it using Newton-type method that requires access to only one data point per iteration. We investigate different sampling strategies of index i_k on iteration k. See description in PDF.
* **Dataset:** It is proposed to use open SVM library as a data for experimental part of the work.
* **References:**
	- Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates
        - Parallel coordinate descent methods for big data optimization
* **Base algorithm:** As a base method it is proposed to use Algorithm 1 from the paper Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates.
* **Solution:** Is is proposed to adjust existing sampling strategies from Parallel coordinate descent methods for big data optimization in this work.
* **Novelty:** In the literature of Second Order methods there are a few works on incremental methods. The idea is to analyze the existing method by applying different sampling strategies. It is known that the proper sampling strategies may improve the performance of a method.
* **Authors:** Islamov Rustem, Vadim Strijov

## Задача 137
* **Задача:** Binary Neural Networks. Lossless picture quality for binary neural networks in pixel-level tasks. (Бинарные сети в img-to-img задачах)
* **Авторы:** Илья Жариков

## Задача 138
* **Задача:** Post Training Quantization. Flexible continuous modification for SOTA post training quantization methods to make them lossless.
* **Авторы:** Илья Жариков

## Задача 139
* **Название**: Обучение с экспертом для выборки со многими доменами.
* **Задача**: Исследуется проблема понижения сложности аппроксимирующей модели при переносе на новые данные меньшей мощности.
* **Данные**: Выборки MNIST, CIFAR-10, CIFAR-100, Amazon товары.
* **Литература**: 
- [Диплом: Камил Баязитов](https://arxiv.org/pdf/1806.00258.pdf https://arxiv.org/pdf/1806.00258.pdf)
* **Базовой алгоритм**: Базовое решение и эксперименты представлены в дипломе.
* **Авторы:** Грабовой Андрей

## Problem template (EN)
## Problem 101
* __Title__: Title
* __Problem__: Problem description
* __Data__: Data description
* __Reference__: Links to the literature
* __Baseline__: baseline description
* __Proposed solution__: description of the idea to implement in the project
* __Novelty__: why the task is good and what does it bring to science?  (for editorial board and reviewers)
* __Authors__: supervisors, consultants, experts

## Шаблон задачи (RU)
## Задача 101
* __Название__: Название, под которым статья подается в журнал. 
* __Задача__: Описание или постановка задачи. Желательна постановка в виде задачи оптимизации (в формате argmin). Также возможна ссылка на классическую постановку задачи. 
* __Данные__: Краткое описание данных, используемых в вычислительном эксперименте, и ссылка на выборку. 
* __Литература__: Список научных работ, дополненный 1) формулировкой решаемой задачи, 2) ссылками на новые результаты, 3) основной информацией об исследуемой проблеме. 
* __Базовой алгоритм__: Ссылка на алгоритм, с которым проводится сравнение или на ближайшую по теме работу. 
* __Решение__: Предлагаемое решение задачи и способы проведения исследования. Способы представления и визуализации данных и проведения анализа ошибок, анализа качества алгоритма. 
* __Новизна__: Обоснование новизны и значимости идей (для редколлегии и рецензентов журнала). 
