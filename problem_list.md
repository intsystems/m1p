# Problem list

## Задача 112  
* **Название:** ECoG - video
* **Авторы:** Эксперт Грабовой, консультант - 

## Задача 113  
* **Название:** ECoG - audio
* **Авторы:** Эксперт Грабовой, консультант - 

## Задача 114  
* **Название:**Моделирование динамики физических систем с помощью Physics-Informed Neural Networks
* **Описание проблемы:** Породить набор сверток по имеющимся данным и выбрать лучшую с помощью методов снижения порядка и размерности пространств. 
* **Данные:** Биомедицинские данные акселерометра и гироскопа, океанические течения, движение барханов, воздушные потоки
* **Литература**  [https://github.com/severilov/master-thesis/blob/main/doc/Severilov2022MasterThesis_rus.pdf] (базовая работа содержит ссылки)
* **Базовый алгоритм:** Нейросетевой, лагранжевы нейросети.
* **Решение:** Нетерова нейросеть. 
* **Новизна:** Предложенная сеть учитывает симметрию
* **Авторы:** Эксперты - Северилов, Стрижов, консультант - Панченко

## Задача 115  
* **Название:** Дистилляция знаний в глубоких сетях и выравнивание структур моделей 
* **Описание проблемы:** Требуется построить сеть наиболее простой структуры, модель-ученик, используя модель-учитель высокого качества. Показать насколько изменяется точность и устойчивость ученика. Результатом эксперимента является график сложность-точность-устойчивость, где каждая модель является точной. 
* **Данные:** CIFAR-10. Предполагается, что учитель имеет открытую для анализа структуру с большим числом слоев. 
* **Литература**  
* [Исходная работа Хинтона по дистилляции](https://arxiv.org/abs/1503.02531)
* [Работы Андрея Грабового](https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=at&paperid=15892&option_lang=rus)
* [Работа Марии Горпинич](https://link.springer.com/article/10.1134/S00051179220100071)* 
* **Базовый алгоритм:** для сравнения 
    - Обучение (моделей с заданной структурой управляемой сложности) без дистилляции.
    - Обучение (ditto) с дистилляцией Хинтона. 
    - Обучение с переносом по слоям.
    - Обучение с переносом по нейронам. 
* **Решение:** Как в п. 2, только по слоям. Построение пути наименьшей стоимости по нейронам. Считаем ковариационные матрицы каждого нейрона каждого слоя для учителя и для ученика. Предлагаем функцию ошибки, включающую цену пути наименьшей стоимости. Предлагаем способ построить путь наименьшей стоимости. Основная идея: перенос идет по парам нейронов и наиболее похожими распределениями (ожидание и ковариационная матрица) от учителя к ученику. 
* **Новизна:** Предложенный перенос существенно снижает сложность без потери точности и решает проблему взаимозаменяемости нейронов, идентифицируя их. 
* **Авторы:** Эксперт Бахтеев, Стрижов, консультант - Мария Горпинич

## Задача 116  
* **Название:** Нейронные дифференциальные уравнения для моделирования физической активности – выбор моделей
* **Авторы:** Эксперт, консультант - Эдуард Владимиров

## Задача 117  
* **Название:** Поиск зависимости и SSA, теорема Такенса
* **Авторы:** Эксперт Стрижов, консультант - Владимиров, Самохина

## Задача 118  
* **Авторы:** Эксперт Стрижов, консультант - Тихонов

## Задача 119  
* **Название** Анализ динамики многократного обучения 
* ** Описание проблемы** Рассмотрим задачу многократного обучения с учителем, в которой обучающая выборка не фиксирована, а обновляется в зависимости от предсказаний обученной модели на тестовой выборке. Для процесса многократного обучения, предсказания и обновления выборки строим математическую модель и исследуем свойства этого процесса на основе построенной модели. 
* Пусть f(x) - функция плотности распределения признаков, G - алгоритм обучения модели, формирования предсказаний на тестовой выборке и подмешивания предсказаний в обучающую выборку, в результате применения которого изменяется распределение признаков. 
* Пусть задано пространство неотрицательных гладких функций F(x), интеграл которых на R^n равен единице. 
* Рассмотрим автономную (не зависит от времени явно) динамическую систему (есть выделенная переменная - номер шага, которая возрастает),  на шаге t и t+1 которой выполняется соотношение 
    f_{t+1}(x) = G(f_{t})(x), 
    где G(f) - оператор эволюции на пространстве указанных функций F и известна начальная функция f_0(x),
    Вообще говоря, G может быть произвольным оператором, не обязательно гладким и/или непрерывным.
    
    ** Вопрос 0.**
    Найти условия на оператор G, при которых образ G лежит в том же классе функций плотности распределений F. 
    В частности, должен ли G быть ограниченным, операторная норма ||G|| <= 1, для того, чтобы образ G(f) \in F также был функцией плотности распределения для любой f из F? 
    Существует ли в пространстве F единица относительно оператора G и что будет единичной функцией f в таком F?

    **Вопрос 1.**
    При каких условиях на G будет существовать такое t_0, что для всех t > t_0 хвост последовательности {f} будет ограничен?

    **Вопрос 2.**
    При каких условиях оператор G будет иметь неподвижную точку?

* **Данные** В вычислительном эксперименте предлагается проверить существенность ограничения / значимость условий, при которых получен ответ на вопросы 0-2. Например, для задачи линейной регрессии и/или регрессии с многоуровневой полносвязной нейронной сетью при разных долях подмешиваемых в обучающую выборку предсказаниях на синтетических наборах данных.

* **Авторы** Ментор, эксперт - Хританков А.С., эксперт - Афанасьев А.П.

* **Литература**
    * Khritankov A., Hidden Feedback Loops in Machine Learning Systems: A Simulation Model and Preliminary Results, https://doi.org/10.1007/978-3-030-65854-0_5
    * Khritankov A.. Pilkevich A. Existence Conditions for Hidden Feedback Loops in Online Recommender Systems, https://doi.org/10.1007/978-3-030-91560-5_19
    * Каток А.Б., Хасселблат Б. Введение в современную теорию динамических систем.1999. 768 с. ISBN 5-88688-042-9.
    * Немыцкий В. В., Степанов В. В. Качественная теория дифференциальных уравнений, год изд.: 1974


## Задача 120  
* **Название:** Дифференцируемый алгоритм поиска ансамблей моделей глубокого обучения с контролем разнообразия
* **Задача:** Рассмотривается задача выбора ансамбля моделей. Требуется предложить метод контроля разнообразия базовых моделей на этапе применения.
* **Данные:** Fashion-MNIST, CIFAR-10, CIFAR-100 datasets
* **Литература:**  [1] [Neural Architecture Search with Structure Complexity Control](https://easychair.org/publications/preprint/H5MC), [2] [Neural Ensemble Search via Bayesian Sampling](https://arxiv.org/pdf/2109.02533.pdf), [3] [DARTS: Differentiable Architecture Search](https://arxiv.org/pdf/1806.09055.pdf)
* **Базовой алгоритм:** В качестве базового алгоритма предлагается использовать DARTS [3]. 
* **Решение:** Для контроля разнообразия базовых моделей предлагается использовать гиперсеть [1], которая смещает структурные параметры в терминах дивергенции Йенсена—Шеннона. На этапе применения сэмплируются базовые архитектуры с заданным смещением для построения ансамбля.
* **Новизна:** Предложенный метод позволяет строить ансамбли с любым количеством базовых моделей без дополнительных вычислительных затрат относительно базового алгоритма.
* **Авторы:** К.Д. Яковлев, О.Ю. Бахтеев

## Задача 121  
* **Задача:** построение предиктивной аналитики для сенсоров загрязнений атмосферы. Требуется предсказать поеведение одного сенсора по значениям других сенсоров.
* **Авторы:** Владимир Вановский, Артем Михайлов.

## Задача 122  
**Автор:** Роман Исаченко

## Задача 123  
* **Название:** Влияние локдауна на динамику распространения эпидемии
* **Задача:** Введение локдауна считается эффективной мерой по борьбе с эпидемией. Однако вопреки интуиции оказалось, что при определенных условиях локдаун может привести к росту эпидемии. Данный эффект отсутствует для классических моделей распространения эпидемии «в среднем», но был выявлен при моделировании эпидемии на графе контактов. Задача заключается в поиске формульных и количественных соотношений между параметрами, при которых локдаун может привести к росту эпидемии. Необходимо как выявить такие соотношения в моделях SEIRS/SEIR/SIS/etc на основе фреймворка распространения эпидений SEIRS+ (и ее модификации), так и теоретически обосновать соотношения, полученные из конкретных реализаций эпидении.
* **Данные:** Задача предполагает работу с модельными и синтетическими данными: имеются готовые данные, а также предполагается возможность генерации новых в процессе решения задачи. Данная задача относится к unsupervised обучению, поскольку реализация эпидемии на графе контактов имеет высокую долю случаныйх событий, а потому требует проводить анализ в среднем на многих синтетически сгенерированных реализациях эпидемии
* **Литература:** 
    - T. Harko, Francisco S. N. Lobo и M. Mak. «Exact analytical solutions of the
Susceptible-Infected-Recovered (SIR) epidemic model and of the SIR model with
equal death and birth rates»
    - https://github.com/ryansmcgee/seirsplus
* **Авторы:** А.Ю. Бишук, А.В. Зухба

## Задача 125  
Грабовой, Антиплагиат и команда

## Задача 126  
Рустем Исламов

## Задача 127  
Егор Шульгин

## Задача 128  
**Название:** Построение модели глубокого обучения в зависимости от данных задачи
**Задача:** рассматривается задача оптимизации модели глубокого обучения для нового датасета. Требуется предложить метод оптимизации модели, позволяющий производить порождение новых моделей для нового датасета с небольшими вычислительными затратами.
**Данные:** CIFAR10, CIFAR100
**Литература:** 
[1] [вариационный вывод для нейронных сетей](https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf)
[2] [гиперсети](https://arxiv.org/abs/1609.09106)
[3][ похожая работа заточенная под изменение модели в зависимости от заранее заданной сложности](https://www.mathnet.ru/links/71cd2117ce84018e028a939bcd0e1507/ia710.pdf) (по запросу скинем более новую версию)
**Базовой алгоритм:** Переобучение модели напрямую. 
**Решение:** Предлагаемый метод заключается в представлении модели глубокого обучения в виде гиперсети (сети, которая генерирует параметры другой сети) с использованием байесовского подхода. Вводятся вероятностные предположения о параметрах моделей глубокого обучения, максимизируется вариационная нижняя оценка байесовской обоснованности модели. Вариационная оценка рассматривается как условная величина, зависящая от информации о данных задачи.
**Новизна: ** предложенный метод позволяет порождать модели в режиме one-shot (практически без переподготовки) для требуемой задачи, что значительно снижает затраты на оптимизацию и дообучение.
**Авторы:** Ольга Гребенькова и Олег Бахтеев

## Задача 129  
* **Название:**Пространственно-временное прогнозирование с помощью сверточных сетей и тензорных разложений 
* **Описание проблемы:** Породить набор сверток по имеющимся данным и выбрать лучшую с помощью методов снижения порядка и размерности пространств. 
* **Данные:** Потребление и цена электроэнергии, океанические течения, движение барханов, воздушные потоки
* **Литература**
*# [http://irep.ntu.ac.uk/id/eprint/32719/1/PubSub10184_Sanei.pdf](Tensor-based Singular Spectrum Analysis for Automatic Scoring of Sleep EEG)
*# [https://ieeexplore.ieee.org/document/6661921](Tensor based singular spectrum analysis for nonstationary source separation)
* **Базовый алгоритм:** Гусеница, тензорная гусеница.
* **Решение:** Найти мультиериодический временной ряд, построить его тензорное представление, разложить в спектр, собрать, показать прогноз.
* **Новизна:** Показать, что мультилинейная модель является удобным способом построения сверток для измерений в пространстве и времени.
* **Авторы:** Эксперт - В. В. Стрижов

## Задача 130  
* **Авторы:** Эксперт - Воронцов, консультант - Полина Потапова 
## Задача 131  
* **Авторы:** Эксперт - Воронцов, консультант - Василий Алексеев
## Задача 132  
**Название:** Ранжирование научных статей для полуавтоматического реферирования
**Задача:** Построить модель ранжирования, которая принимает на входе подборку текстов научных статей и выдаёт на выходе последовательность их упоминания в реферате. 
**Данные:**
        - В качестве обучающей выборки используются обзорные разделы (например, Introduction и Related Work) статей из коллекции S2ORC (81.1M англоязычных статей). Объект обучающей выборки – это последовательность ссылок на статьи из списка литературы, упоминаемые в обзорных разделах. Для каждого документа есть набор мета данных - год публикации, журнал, число цитирований, число цитирований автора и др. Также, имеется abstract и, возможно, полный текст статьи.
        - В качестве метрики используется Коэффициент ранговой корреляции Кендалла.
**Литература: **
 - [Крыжановская С. Ю. «Технология полуавтоматической суммаризации тематических подборок научных статей»](http://www.machinelearning.ru/wiki/images/e/ed/Kryzhanovskaya22msc.pdf)
 - [Власов А. В.  «Методы полуавтоматической суммаризации подборок научных статей»](http://www.machinelearning.ru/wiki/images/6/6d/Vlasov2020MSThesis.pdf)
 - [Крыжановская С. Ю., Воронцов К. В. «Технология полуавтоматической суммаризации тематических подборок научных статей»](http://www.machinelearning.ru/wiki/images/f/ff/Idp22.pdf, стр. 371)
 - [S2ORC: The Semantic Scholar Open Research Corpus](https://aclanthology.org/2020.acl-main.447.pdf)

**Базовые алгоритмы:**
 - Попарные (pair-wise) методы ранжирования
 - Градиентный бустинг
**Решение:** Простейшим решением является ранжирование статей в хронологическом порядке, по году их публикации. Для решения задачи предлагается построить модель ранжирования на основе градиентного бустинга. В качестве признаков можно использовать год публикации, цитируемость статьи, цитируемость её авторов, семантическая близость публикации к обзору, к его локальному контексту, и т. д. 
**Новизна:** Задача является первым этапом для полуавтоматического реферирования тематических подборок научных публикаций (machine aided human summarization, MAHS). После того, как сценарий реферата построен, система генерирует для каждой статьи фразы-подсказки, из которых пользователь выбирает фразы для продолжения своего реферата. 
**Автор:** Крыжановская Светлана, Константин Воронцов

## Задача 133  
**Автор:** Филипп Никитин

## Задача 134  
   **Название**: Объединение дистилляции моделей и данных
   **Задача**: Дистилляция знаний - передача знаний из более содержательного представления в компактное сжатое представление. Существует два вида дистилляции знаний. Первый - дистилляция моделей. В этом случае большая модель передает знания (дистиллируется) в маленькую модель. Второй - дистилляция данных. В этом случае создается минимальный набор данных, на котором после обучения модели достигает  качество, сравнимого с обучением на полной выборке . На данный момент нет решения, которое может реализовать одновременную дистилляцию модели и знаний. Поэтому цель задачи предложить базовое решение по дистилляции моделей и сравнить с подходами по дистилляцией моделей и дистилляции данных
   **Данные**: Выборка рукописных цифр MNIST, Выборка изображений CIFAR-10
   **Литература**: 
1) Собрание различных работ по дистилляции данных: https://github.com/Guang000/Awesome-Dataset-Distillation.
2) Обзор на методы дистилляции моделей: https://arxiv.org/pdf/2006.05525.pdf
3) Базовое решение по дистилляции знаний: https://georgecazenavette.github.io/mtt-distillation/
4) Базовое решение по дистилляции моделей: https://arxiv.org/pdf/1503.02531.pdf
   **Базовой алгоритм**: 
	Базовое решение по дистилляции модели - дистилляция Хинтона
	Базовое решение по дистилляции датасетов - Dataset Distillation by Matching Training Trajectories
   **Решение**: В качестве базового алгоритма предлагается реализовать дистилляцию данных. Затем обучить бОльшую модель на полученных данных и дистиллировать её в маленькую модель. Далее, провести сравнение с исходной моделью и моделью, обученной на дистиллированных данных.
   **Новизна**: Новизна работы заключается в соединении двух подходов по дистилляции, что не было реализовано ранее
 **Авторы**:  Андрей Филатов

## Задача 135
**Авторы**: Полина Барабанщикова 

## Problem template (EN)
## Problem 101
* __Title__: Title
* __Problem__: Problem description
* __Data__: Data description
* __Reference__: Links to the literature
* __Baseline__: baseline description
* __Proposed solution__: description of the idea to implement in the project
* __Novelty__: why the task is good and what does it bring to science?  (for editorial board and reviewers)
* __Authors__: supervisors, consultants, experts

## Шаблон задачи (RU)
## Задача 101
* __Название__: Название, под которым статья подается в журнал. 
* __Задача__: Описание или постановка задачи. Желательна постановка в виде задачи оптимизации (в формате argmin). Также возможна ссылка на классическую постановку задачи. 
* __Данные__: Краткое описание данных, используемых в вычислительном эксперименте, и ссылка на выборку. 
* __Литература__: Список научных работ, дополненный 1) формулировкой решаемой задачи, 2) ссылками на новые результаты, 3) основной информацией об исследуемой проблеме. 
* __Базовой алгоритм__: Ссылка на алгоритм, с которым проводится сравнение или на ближайшую по теме работу. 
* __Решение__: Предлагаемое решение задачи и способы проведения исследования. Способы представления и визуализации данных и проведения анализа ошибок, анализа качества алгоритма. 
* __Новизна__: Обоснование новизны и значимости идей (для редколлегии и рецензентов журнала). 
