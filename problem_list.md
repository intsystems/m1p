# Список задач по курсу Моя первая научная статья, весна 2024
## Задача 112  (template)
* **Название:**
* **Описание проблемы:**
* **Данные:**
* **Литература**
* **Базовый алгоритм:** 
* **Новизна:**  
* **Авторы:**

## Задача 125 (renewed problem)
* **Название:** Влияние локдауна на динамику распространения эпидемии
* **Задача:** Введение локдауна считается эффективной мерой по борьбе с эпидемией. Однако вопреки интуиции оказалось, что при определенных условиях локдаун может привести к росту эпидемии. Данный эффект отсутствует для классических моделей распространения эпидемии «в среднем», но был выявлен при моделировании эпидемии на графе контактов. Задача заключается в поиске формульных и количественных соотношений между параметрами, при которых локдаун может привести к росту эпидемии. Необходимо как выявить такие соотношения в моделях SEIRS/SEIR/SIS/etc на основе фреймворка распространения эпидений SEIRS+ (и ее модификации), так и теоретически обосновать соотношения, полученные из конкретных реализаций эпидении.
* **Данные:** Задача предполагает работу с модельными и синтетическими данными: имеются готовые данные, а также предполагается возможность генерации новых в процессе решения задачи. Данная задача относится к unsupervised обучению, поскольку реализация эпидемии на графе контактов имеет высокую долю случаныйх событий, а потому требует проводить анализ в среднем на многих синтетически сгенерированных реализациях эпидемии
* **Литература:** 
    - T. Harko, Francisco S. N. Lobo и M. Mak. «Exact analytical solutions of the
Susceptible-Infected-Recovered (SIR) epidemic model and of the SIR model with
equal death and birth rates»
    - [Код](https://github.com/ryansmcgee/seirsplus)
* **Авторы:** А.Ю. Бишук, А.В. Зухба

## Задача 118   (renew for 2024)
* __Название__ Непрерывное время при построении нейроинтерфейса BCI 
* __Задача__: В задачах декодирования сигнала данные представляются как многомерные временные ряды. При решении задач используется дискретное представление
времени. Однако недавние работы по нейронным обыкновенным дифференциальным уравнениям иллюстрируют возможность работать со скрытым состоянием рекуррентных нейронных сетей, как с решениями дифференциальных уравнений. Это позволяет рассматривать временные ряды как непрерывные по времени. 
* __Данные__: Для классификации:
  * датасет P300, по которому писали статью c Алиной 
  * похожий на него по формату записей датасет DEAPdataset. 
  * найти современный датасет, спросить в U.Grenoble-Alpes
* __Литература__: [Neural CDE](https://bit.ly/NeuroCDE)
* __Базовой алгоритм__: Алгоритм Алины Самохиной, S4, S5, D4 (порождающий)
* __Решение__: Использование вариаций  NeurODE для аппроксимации исходного сигнала. Сравнительный анализ существующих подходов к применению дифференциальных уравнений для классификации EEG. (***Энкодер-тензорное разложение, декодер NeuroCDE***)
* __Новизна__: предлагается способ построения непрервыного представления сигнала. Работа с функциональным пространством сигнала, а не его дискретным представлением. Использование параметров полученной функции в качестве признакового пространства результирующей модели.
* **Авторы:** Алина Самохина, Стрижов 

## Задача 159
* **Название:** Восстановление функциональных групп головного мозга с помощью графовых диффузных моделей
* **Описание проблемы:** Решается задача построения модели анализа активности головного мозга, учитывающей пространственную структуру сигнала. Данные об активности мозга представлены в виде многомерных временных рядов, считываемых
электродами, расположенными на голове испытуемого одним из универсальных стандартов размещения. Из-за отсутствия регулярного определения окрестности на сферической поверхности мозга классические сверточные нейронные
сети не могут быть эффективно применены для учета пространственной информации. Предлагается использовать графовое представление сигнала, что позволит выявить более сложные взаимосвязи различных областей активности в пространстве и провести нейробиологическую интерпретацию функциональных связей мозга. 
* **Данные:** Юлия Березуцкая, код загрузки у четвертого курса
* **Литература** Магистерская работа Наталии Вареник
* **Базовый алгоритм:** Graph Neural Diffusion: https://github.com/twitter-research/graph-neural-pde
* **Новизна:** Построить карту функциональных групп с изменением во времени в зависимости от внешнего воздействия (видео Пеппи)
* **Авторы:** Святослав Панченко, Стрижов 

## Задача 158 (Мухаммадшариф Набиев, назначена)
* **Название:** Прогнозирование деформаций и напряжений в биологических тканях и органах
* **Описание проблемы:** Требуется выбрать модель аппроксимации поля поле деформаций и напряжений в материале под разными условиями. Целью исследования является разработка модели и метода, которые по имеющимся ограниченным экспериментальным данным достоверно восстанавливает поля в материале.
* **Данные:** Измерения деформаций и напряжений, полученные с помощью сенсоров. Данные будут представлены в виде тензора, где каждый элемент соответствует определенному измерению деформации или напряжения в конкретном месте и времени.
* **Литература**
	- https://arxiv.org/abs/2302.06594
	- Что было опубликовано после этой работы вместе со словами Physics-informed learning + tensor analysis
* **Авторы:** Павел Северилов, Стрижов, Мухаммадшариф Набиев

## Задача 157 (Иван Папай, назначена)
* **Название:** Приближение временных рядов стохастическими дифференциальными уравнениями (вставить мотивацию)
* **Описание проблемы:** Требуется построить прогноз набора временных рядов (с высокой ковариацией), вплоть до редсказания аномалий (аномалия означает отказ от прогнозирования, она не относится ни к детерминированой, ни к стохастической составляющей). Требуется декомпозировать реализзцию случайного процесса (временного ряда) и восстановить модель случайного процесса (со сносом, скачками). Требуется выбать адекватную порождабщую нейросеть, указать способ прореживания числа слоёв и снижения их размерности.
* **Данные:**
Выборка: набор фазовых траекторий, реализаций с.п.  (было: Т х Т - временной диапазон х З - значение, которое случ. величина принимала в это время). Требуется построить прогноз. Критерий – минимум свертки мсв с функцией ошибки, адекватной гипотезе порождения данных, или построенной под прикладную задачу.
- EEG PyRiemann https://pyriemann.readthedocs.io
- LOBSTER Trades Quotes and Prices https://lobsterdata.com/tradesquotesandprices
* **Литература**
  - про Neural SDE: [SDE as a GAN](https://arxiv.org/pdf/2102.03657.pdf), [signature kernel scores](https://arxiv.org/pdf/2305.16274.pdf), [gradients](https://arxiv.org/pdf/2105.13493.pdf)
  - обсудить da Prato–Debussche trick (возмущения решений посредством порождающих моделей)
* **Базовый алгоритм:** 
* **Новизна:**  
* **Авторы:** Эдуард Владимиров, Стрижов, Иван Папай 

## Задача 158 (индустриальная)
* **Описание проблемы:**
	- Ранжирование риск-сигналов о признаках развития неправомерной торговой стратегии на финансовом рынке. Задача выявления ранних признаков аномального поведения участника торгов [Данные: набор агрегатов, построенных на обезличенных данных торгов].
	- Классификация субъектов ПНИИИМР, ПОД/ФТ, … . Выявление аномального поведения субъектов, классификация, кластеризация стратегий или профилей финансового поведения [Данные: набор агрегатов, построенных на обезличенных данных торгов].
* **Данные:** ВАЖНО! Требуется найти или синтезировать открытые данные
* **Авторы:** Андрей Сергеевич Инякин

## Задача 157 (индустриальная)
* **Описание проблемы:**
	- Использование (дообучение, «компрессия» / дистиляция, прунинг, квантизация) SOTA LLM/GAN (генеративных моделей) для формирования последовательности тестовых сценариев (тест-кейсов) по заданному набору функциональных требований [Данные: реестр требований, тест-кейсы].
	- Использование (дообучение, «компрессия» / дистиляция, прунинг, квантизация) SOTA LLM/GAN для формирования и актуализации реестра «атомарных» и непротиворечивых функциональных / нефункциональных требований к программной Системе на основе набора функциональных, технических заданий и иных документов [Данные: реестр требований, реестр ФЗ, реестр ТЗ].
* **Данные:** ВАЖНО! Требуется найти или синтезировать открытые данные
* **Авторы:** Андрей Сергеевич Инякин

## Задача 156
* **Название:** Optimal Gradient Methods with Relative Inexactness
* **Описание проблемы:** A description of your problem, its motivation  and goals. An optimization-style problem statement is welcome
Задача: построить оптимальный метод оптимизации первого порядка с помощью градиентов, зашумлённых относительным шумом. Для этого мы будем использовать технику Programming Performance Estimate, которая позволяет строить такого рода алгоритмы через анализ численного решения задачи Полуопределённого программирования. Необходимые знания: 1) Понимать, что такое выпуклые функции,  и как работаю градиентные методы для поиска их минимумов 2) Опыт Python с numpy
Ожидаемые результаты: Доказанные теоремы сходимости для предложенного методы и экспериментальное подтверждение результатов, публикация в журнал уровня Q1,Q2. Комментарий от автора: 
Конечно, тема не самая простая и результаты не факты, что будут, но я готов уверить, что будет релевантный опыт написание введения, анализа работ и  теории для статьи, постановки и презентации экспериментов и представления научной ценности. Вся необходимая помощь по материалу будет предоставлена.
* **Данные:** Synthetic data 
* **Литература** Рарерs with
	1-2) Основная статья, продолжения первой части которой мы будем писать https://arxiv.org/abs/2310.00506
	3) Введение в PEP https://francisbach.com/computer-aided-analyses/
* **Базовый алгоритм:**  https://github.com/Jhomanik/InterRel
* **Авторы:** Корнилов Никита
  
## Задача 155 
* **Название:** Identification of the relationship between labels using an algorithm based on one's own attention for the classification problem with multiple labels, justifying the connection with Hawkes processes.
* **Описание проблемы:** Most of the available user information can be represented as a sequence of events with timestamps. Each event is assigned a set of categorical labels, the future structure of which is of great interest. This is a temporal sets prediction problem for sequential data. Modern approaches focus on the transformation architecture for sequential data, introducing independent attention to the elements in the sequence. In this case, we take into account the temporal interactions of events, but lose information about the interdependencies of labels. Motivated by this disadvantage, we propose to use the mechanism of independent attention to the marks preceding the predicted step. Since our approach is a network of attention to labels, we call it a LANET.  We also justify this aggregation method, it affects the intensity of the event positively, assuming that the intensity is represented by the basic Hawkes process.
* **Данные:** A brief description of data in the computational experiment and. Links to the datasets. The datasets shall be open-source. The data shall be ready-to-model
	- https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data
	- https://www.kaggle.com/c/python-and-analyze-data-final-project/data
Based on the dataset data, we will compare the state of the art solutions in this area with our solution in this problem statement.
* **Литература** 1-2 Predicting Temporal Sets with Deep Neural Networks, Predicting Temporal Sets with Simplified Fully Connected Networks  3 Transformer Hawkes Process, The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process - introduction to idea with process Hawkes
* **Базовый алгоритм:**
	- https://github.com/yule-BUAA/SFCNTSP
	- https://github.com/yule-BUAA/DNNTSP The state of the art methods for temporal sets prediction
* **Новизна:** The idea of the proposed solution and methods for conducting the research. Ways of visualizing data and error analysis
Most of the transformer-related models used for temporal sets prediction use self-attention computation between consecutive input timestamps representations. The LANET instead uses the self-attention between label representations. So, it has the input that consists of K vectors. Below, we describe how to aggregate a sequence of size τ to K vectors via an Embedding layer. Then we define the Self-attention layer. To get the predictions, we apply a Prediction layer.  Also, to justify such aggregation by labels, instead of time dependence, we introduce the concept of Cox processes, which describe the probability of an event at a given time through the integral of the intensity function. And we can show that such aggregation can either not worsen it, or make a positive contribution to probability. LANET will train with the cross-entropy loss adapted for the multi-label task through
independent consideration of each label score. Comparison tables with basic approaches will be carried out, as well as visualization of quality changes from selected parameters, visualization of attention for analyzing the relationship of label representations, as well as tables comparing the effect of vector representations on the result of the model.
* **Авторы:**  Consultant - Andrey Grabovoy, Expert - Alexey Zaytsev, Author of research - Galina Boeva

## Задача 154
* **Название:** Декодирования сигналов головного мозга в аудиоданные
* **Описание проблемы:** Декодирования сигналов головного мозга в аудиоданные (чтение мыслей) на выбор из двух математических постановок:
1. Classification problem in a match-mismatch paradigm: the task of the model is to determine which of the input stimulus segments of audio corresponds to the EEG https://exporl.github.io/auditory-eeg-challenge-2024/task1/description/
2. Regression problem: to reconstruct the mel spectrogram from the EEG https://exporl.github.io/auditory-eeg-challenge-2024/task2/description/ 
* **Данные:** Датасет был собран на базе Лёвенского университета - https://rdr.kuleuven.be/dataset.xhtml?persistentId=doi:10.48804/K3VSND Для сбора датаесета были приглашены 85 человек без проблем со слухом и нервной системой, носители бельгийского голландского языка. Измерения производились в звуконепроницаемой лаборатории с помощью высокоточных приборов для снятия ЭЭГ с 64 электродами. Частота дискретезации даных 8192 Гц. Каждому учатнику предлагалось послушать отрывок подкаста или аудиокниги (случайно) длиной до 15 минут. Всего имеем 668 пар ЭЭГ-стимул (прослушанный отрывок) общей продолжительностью 9431 минута
* **Литература**
	1. Pre-LN FFT: Baseline решение с использованием иной архитектуры обработки аудио https://arxiv.org/pdf/2305.06806.pdf  (код https://github.com/jkyunnng/HappyQuokka_system_for_EEG_Challenge)
	2. Линейные модели, FCCN (2022) - https://www.researchgate.net/publication/361380348_Robust_decoding_of_the_speech_envelope_from_EEG_recordings_through_deep_neural_networks
	3. VLAAI (2023, Nature) https://www.nature.com/articles/s41598-022-27332-2
	4. Decoding speech perception from non-invasive brain recordings (2023) https://arxiv.org/pdf/2208.12266.pdf
* **Базовый алгоритм:** Описание baseline решения предоставлено на сайте соревнования: https://exporl.github.io/auditory-eeg-challenge-2024/task2/description/
* **Новизна:** Попробовать современные методы получения эмбеддингов аудиоданных для улучшения бейзлайн решения (например wav2vec, fastspeech2). Цель: показать, что модели, учитывающие законы физики/мира (как например fastspeech2) улучшают качество декодирования сигналов головного мозга в аудиоданные
* **Авторы:** Павел Северилов pseverilov@gmail.com, Стрижов

## Задача 117
* **Название:** Поиск зависимостей биомеханических системах и (Метод Convergence Cross-Mpping, теорема Такенса)
* **Задача**:  При прогнозировании сложноорганизованных временных рядов, зависящих от экзогенных факторов и имеющих множественную периодичность, требуется решить задачу выявления связанных пар рядов. Предполагается, что добавление этих рядов в модель повышает качество прогноза. В данной работе для обнаружения связей между временными рядами предлагается использовать метод сходящегося перекрестного отображения. При таком подходе два временных ряда связаны, если существуют их траекторные подпространства, проекции на которые связаны. В свою очередь, проекции рядов на траекторные подпространства связаны, если окрестность фазовой траектории одного ряда отображается в окрестность фазовой траектории другого ряда. Ставится задача отыскания траекторных подпространств, обнаруживающих связь рядов.  
* **Литература**
	- Все, что написал Сугихара в Science и в Nature (спросить коллекцию)
	- Усманова К.Р., Стрижов В.В. Обнаружение зависимостей во временных рядах в задачах построения прогностических моделей // Системы и средства информатики, 2019, 29(2) 
	- [Neural CDE](https://bit.ly/NeuroCDE)
* __Данные__:
	* Видео, его разметка и ECoG, EEG, движение, глаз из работы Nature, Березуцкая, люди смотрят фильм
* Решение 
	* Базовое в работе Карины
	* Наше построить Neural ODE для обеих сигналов и решить, относятся ли обе модели к одной динамической системе.
* **Авторы:** Денис Тихонов, Стрижов, Даниил Дорин

## Задача 153
* **Название:**  Погружение временных рядов с высокой волатильностью в метрическое пространство
* **Описание проблемы:**  Решается задача выбора оптимального порфеля финансовых инструментов по результатам прогноза наборов временных рядов. Проблема при постановке задачи выбора заключается в том, что оценка ковариационной матрицы пар временных рядов не имеет требуемых статистических свойств (устойчивость, состоятельность, несмещенность). Предлагается выполнить сравнительный анализ различных методов вычисления парных расстояний между временными рядами. 
* **Данные:**
	- Финансовые ряды https://www.cambridge.org/core/books/trades-quotes-and-prices/029A71078EE4C41C0D5D4574211AB1B5
	- Trades Quotes and Prices https://lobsterdata.com/tradesquotesandprices
* **Литература** 
	- Multi-Period Trading via Convex Optimization by Stephen Boyd  Enzo Busseti
	- диссертация А.А. Адуенко
 * **Базовый алгоритм:** Задача квадратичного программирования с расстоянием между временными рядами в виде ковариационной матрицы из книги Бойда. Прогностическая модель произвольна, начиная с линейной регрессии. 
* **Новизна:** Задача выбора метрики между сильно зашумленными временными рядами является открытой. Функицю выбора предлагается строить исходя из Бойдовского критерия, против свертки прогноза с функцией доходности. 
* **Авторы:**  Яковлев, Стрижов

## Задача 152
* **Название:**  Метрическое прогнозирование временных рядов с высокой ковариацией
* **Описание проблемы:**  Решается задача прогнозирования наборов временных рядов. Каждый ряд имеет высокую дисперсию, ряды имеют высокую ковариацию. Такие свойства рядов наблюдаются в сигналах головного мозга и в ценах биржевых активов. Предлагается построить пространство парных растояний (метрическую конфигурацию временных рядов), выполнить прогноз в пространстве парных расстояний, и вернуть прогноз в исходное пространство, используя метод многомерного шкалирования. 
* **Данные:**
	- EEG, Данные удобные для загрузки https://pyriemann.readthedocs.io/en/latest/auto_examples/ERP/plot_classify_EEG_tangentspace.html#sphx-glr-auto-examples-erp-plot-classify-eeg-tangentspace-py
	- Финансовые ряды https://www.cambridge.org/core/books/trades-quotes-and-prices/029A71078EE4C41C0D5D4574211AB1B5
	- Trades Quotes and Prices https://lobsterdata.com/tradesquotesandprices
* **Литература** 
	- Singular Spectrum Analysis
	- https://pyriemann.readthedocs.io/en/latest/index.html
	- Multidimensional scaling
* **Базовый алгоритм:** Базовый прогноз выполнятется методами Singular Spectrum Analysis, LSTM+attention, Transformer
* **Новизна:** Предлагаемое решение использует 1) Римановы модели но не для классификации, а для арегрессии, 2) Римановы генеративные диффузные модели. 
* **Авторы:**  Яковлев, Стрижов
  
## Задача 150
* **Название:** Tree-width Driven SDP for The Max-Cut Problem
* **Описание проблемы:** The Max Cut problem is computationally intractable (NP hard) over general graphs; however, for trees and graphs with small tree-width it is easy to solve exactly in polynomial time. Furthermore, the SDP or Lovász-Schrijver relaxations allows to approximate the Max-Cut value over general graphs. The contribution is to combine both the tree-width and relaxation approaches to improve (empirically) the Max-Cut approximation quality. 
* **Данные:**
	- [1] Texas Data Repository https://dataverse.tdl.org/dataset.xhtml?persistentId=doi:10.18738/T8/VLTIVC
	- [2] Biq Mac Library https://biqmac.aau.at/biqmaclib.html
* **Литература**
	- [1] Intro and Problem setup: https://medium.com/toshiba-sbm/benchmarking-the-max-cut-problem-on-the-simulated-bifurcation-machine-e26e1127c0b0
	- [2] The Lovasz-Schrijver https://home.ttic.edu/~madhurt/Papers/ls.pdf
	- [3] The SDP https://ocw.mit.edu/courses/15-084j-nonlinear-programming-spring-2004/a632b565602fd2eb3be574c537eea095_lec23_semidef_opt.pdf
	- [4] Treewidth https://www.cs.cmu.edu/~odonnell/toolkit13/lecture17.pdf
* **Базовый алгоритм:**
	(a) The SDP/Semi-Definite Programming relaxation (b) The Lovasz Schrijver relaxation
* **Новизна:**
	(c) find a matrix k-diagonal C dominating (in a spectral sence) the laplacian of the initial graph. Use the treewidth max-cut over a "dominating" graph
	(d*) Use graph sparsification [5] to create a nice approximation to the initial graph, but having lower number of edges and treewidth
	- [5] A nice course of 3 lectures on Graph sparsification: https://simons.berkeley.edu/graph-sparsification
* **Авторы:** Alex Bulkin
 
## Задача 149
* **Название:** The Optimal Binning Problem: A Statistical ViewPoint
* **Описание проблемы:** The Optimal Binning problem is the optimal discretization of a variable into bins given a discrete or continuous numeric target. Given a dataset of N samples in [0,1] we are looking for a binning on M bins maximizing the weight-of-evidence metric [1]. The latter metric allows to understand the predictive power of an independent variable. Weight-of-evidence helps to understand if a particular class of an independent variable has a higher distribution of good or bad. Our problem is for a sufficiently large N and constant M to find such an optimal binning.  [1] https://medium.com/mlearning-ai/weight-of-evidence-woe-and-information-value-iv-how-to-use-it-in-eda-and-model-building-3b3b98efe0e8
* **Данные:** the baseline experiment over simulated datasets or any of the Kaggle datasets would be ok
* **Литература**
  	- [0] http://www.c4st.org/images/hesa-2015/submissions/Weight-of-Evidence-A-Review-of-Concept-and-Methods-E.pdf
  	- [1] https://medium.com/mlearning-ai/weight-of-evidence-woe-and-information-value-iv-how-to-use-it-in-eda-and-model-building-3b3b98efe0e8
  	- [2] https://arxiv.org/pdf/2001.08025.pdf
  	- [3] https://iopscience.iop.org/article/10.1088/0266-5611/18/4/201/meta
* **Базовый алгоритм:** The baseline algorithms are (a) CART and similar techniques (b) convex relaxation
* **Новизна:** quantile splitting of the inverse transform to the empirical distribution; probably with some a-posteriori empirical tuning
* **Авторы:** Alex Bulkin
* 
## Задача 148
* **Название:** Средневзвешенная когерентность как мера интерпретируемости тематических моделей
* **Описание проблемы:** Тематическое моделирование широко используется в социо-гуманитарных исследованиях для понимания тематической структуры больших текстовых коллекций. Типичный сценарий предполагает, что пользователь сам разделяет найденные моделью темы на "хорошие" (интерпретируемые) и "плохие". Для упрощения этой работы можно использовать ряд автоматически вычисляемых критериев качества, один из которых — когерентность (мера "согласованности" слов темы). Однако проблема когерентности в том, что при её вычислении игнорируется бòльшая часть текста, что делает оценку качества темы по когерентности ненадёжной. Задача в том, чтобы проверить новый способ вычисления когерентности, обобщающий классический подход, но при этом учитывающий распределение темы во всём тексте.
* **Данные:** В качестве данных подойдёт любая коллекция текстов на естественном языке, про которую известна тематическая структура (сколько примерно тем, сколько документов по разным темам). Например, можно взять коллекцию статей с ПостНауки, новостей Lenta, дамп Википедии, посты с Хабрахабра, 20 Newsgroups, Reuters.
* **Литература**
  - Воронцов К. В. "Вероятностное тематическое моделирование: теория, модели, алгоритмы и проект BigARTM" (https://web.archive.org/web/20230520153443/http://machinelearning.ru/wiki/images/d/d5/Voron17survey-artm.pdf)
  - Воронцов К. В. "Оценивание качества тематических моделей" (из курса лекций "Вероятностные тематические модели"; https://web.archive.org/web/20230811052505/http://www.machinelearning.ru/wiki/images/a/a7/Voron23ptm-quality.pdf
  - Alekseev V. A., Bulatov V. G., Vorontsov K. V. Intra-text coherence as a measure of topic models' interpretability //Komp'juternaja Lingvistika i Intellektual'nye Tehnologii. – 2018. – С. 1-13 (https://www.dialog-21.ru/media/4281/alekseevva.pdf)
  - Newman D. et al. Automatic evaluation of topic coherence //Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics. – 2010. – С. 100-108. (https://aclanthology.org/N10-1012.pdf)
* **Базовый алгоритм:** Когерентность Ньюмана по топ словам, внутритекстовая когерентность
* **Новизна:** Использование библиотек тематического моделирование BigARTM и TopicNet. Разработка нового способа вычисления когерентности тем. Предложение и реализация методики измерения интерпретируемости тем (чтобы проверить "адекватность" новой когерентности: в самом ли деле для заведомо хороших тем она показывает качество выше, чем для плохих тем).
* **Авторы:** Василий Алексеев, Константин Воронцов

## Задача 147
* **Название:** Нижние оценки для min max задач с разной размерностью блоков переменных (Проект 1.)
* **Описание проблемы:** Для задач малоразмерной выпуклой оптимизации нижние оценки получаются с помощью сопротивляющегося оракула https://www2.isye.gatech.edu/~nemirovs/Lect_EMCO.pdf (3 Methods with linear convergence, II, но начать лучше прямо с самого первого раздела Lecture 1 - на одномерном случае все попонятнее). В то время как для задач большой размерности -  c помощью “худшей в мире функции” - см., например, указания к упражнения 1.3 и 2.1 пособия МЦНМО https://opt.mipt.ru/posobie.pdf В работе https://arxiv.org/pdf/2010.02280.pdf, исследуются задачи min max, в которых одна из групп min переменных имеет небольшую размерность, а другая группа, напротив, большую. Получены верхние оценки. Интересно было бы попробовать получить нижние оценки, путем комбинации двух конструкций. Кажется, что в математическом плане пример построения нижней оценки будет содержать новые интересные идеи.
В развитие этого проекта интересно было бы подумать и о нижних оценках для min min задач, в которых по одной из групп переменных (негладких) имеется малая размерность. Верхние оценки имеются в работах https://arxiv.org/pdf/2102.00584.pdf и  https://arxiv.org/pdf/2103.00434.pdf
* **Авторы:** Александр Владимирович Гасников

## Задача 146
* **Название:** Слайдинг с редукцией дисперсии (Проект 2.)
* **Описание проблемы:** Направление градиентного слайдинга стало популярно в последнее время. Активно в этом направлении работает Джордж Лан https://arxiv.org/pdf/1406.0919.pdf https://arxiv.org/pdf/1609.04905.pdf https://arxiv.org/pdf/2111.00996.pdf Некоторые результаты по слайдингу есть и у нас https://arxiv.org/pdf/1906.03620.pdf https://arxiv.org/pdf/2002.02706.pdf https://arxiv.org/pdf/2103.09344.pdf https://arxiv.org/pdf/2205.15136.pdf https://arxiv.org/pdf/2307.12946.pdf стали появляться и статьи других коллективов https://arxiv.org/pdf/2201.01169.pdf.
Однако, полноценного слайдинга для задач вида f(x) + g(x), где g(x) имеет структуру суммы и для g(x) используется инкрементальный оракул (рандомизация суммы m слагаемых) не известно. Все что известно, описано тут https://arxiv.org/pdf/1912.11632.pdf
Задача заключается в обосновании оценки числа вызовов градиента f \tilde{O}(\sqrt{L_f/\mu}), и инкрементального оракула (градиентов слагаемых) g \tilde{O}(m+\sqrt{mL_{g}/\mu}). В общем случае такая оценка не получена до сих пор.
P.S. Были надежды на редукцию дисперсии с importance sampling с правильным выбором вероятностей, но пока они не оправдались…
* **Авторы:** Александр Владимирович Гасников 

## Задача 145 
  * **Название**: Объединение дистилляции моделей и данных TODO
  * **Задача**: TODO
  * **Данные**: Выборка рукописных цифр MNIST, Выборка изображений CIFAR-10
  * **Литература**: 
	- [Собрание различных работ по дистилляции данных.](https://github.com/Guang000/Awesome-Dataset-Distillation)
	- [Обзор на методы дистилляции моделей.](https://arxiv.org/pdf/2006.05525.pdf)
	- [Базовое решение по дистилляции знаний](https://georgecazenavette.github.io/mtt-distillation/)
	- [Базовое решение по дистилляции моделей](https://arxiv.org/pdf/1503.02531.pdf)
  * **Базовой алгоритм**: 	
  * **Решение**: 
  * **Новизна**: 
 * **Авторы**: Андрей Филатов

## Задача 144
* **Название:** Многократное обучение в рекомендательных системах
* **Описание проблемы:** Метрики качества рекомендательных систем P@k, NDCG, MRR и пр. обычно учитывают, насколько хорошие рекомендации были даны для рассматриваемого пользователя, при этом текущее и долгосрочное влияние применяемых алгоритмов на окружающую аудиторию потребителей и ассортимент товаров не учитывается. Предлагается рассмотреть рекомендательную систему, в которой товары W и потребители C меняются со временем, как процесс многократного машинного обучения. Пусть заданы начальные плотности распределений признаков f_0(с) и f_0(w) на X = C U W. Рассмотрим динамическую систему вида f_{t+1}(x) = D_t(f_t)(x) с оператором эволюции D_t [4], где переход к шагу t+1 состоит в формировании рекомендации потребителю (c, z) ~ f(c) алгоритмом товара w = h(c, f(c), f(w)), заключением сделки потребителем с вероятностью ~ u(c,w,z), обновлении f(c), f(w) и h(…) по истории предложений и сделок. В частности выяснить  1) При каких условиях в такой системе при t к бесконечности возникает вырождение аудитории, ассортимента или распределения сделок, и как зависит такое вырождение от алгоритма обучения и вида рекомендательной модели? 2) Предложить такой обучаемый рекомендательный алгоритм, который улучшает любую из стандартных метрик без вырождения C и G при t к бесконечности, или доказать, что это невозможно.
* **Данные:** Для инициализации используем синтетические данные, созданные по аналогии со статьей [1] или MovieLens 100K Dataset по аналогии со статьей [2]
* **Литература**
	- [1] Debiasing the Human-Recommender System Feedback Loop in Collaborative Filtering
	- [2] Breaking Feedback Loops in Recommender Systems with Causal Inference
	- [3] Positive feedback loops lead to concept drift in machine learning systems
	- [4] Задача 119, Моя первая научная статья 2023
	- [5] Python LibMF
* **Базовый алгоритм:** Исходный код - доработанная версия кода из [3] при решении [4], базовые алгоритмы рекомендаций - TopPop, Random, Oracle, SGD MF [5]
* **Новизна:** Предлагается исследовать свойства оператора D в зависимости от возможных предположений и ограничений, и применить ранее полученные в [4] результаты в предлагаемой постановке. Затем для простых моделей рекомендаций провести вычислительный эксперимент с симуляцией работы системы во времени с использованием базового кода для сравнения теоретических результатов по вопросам 1)-2) с фактическими наблюдениями.
* **Авторы:**  консультант - Веприков А.С., эксперт - Хританков А.С.

## Задача 143 (индустриальная)
* **Название:** Интерпретируемая иерархическая кластеризация объектов
* **Описание проблемы:** Требуется на обучающей выборке физических лиц (ФЛ) с меткой принадлежности классу построить такую иерархическую кластеризацию данных, что для каждого кластера можно наиболее полно интерпретировать причину его попадания в конкретный кластер.
* **Данные:** Для исследования предлагается использовать данные из соревнования Kaggle по классификации мошенников в автостраховании на основе их признакового описания. https://www.kaggle.com/datasets/khusheekapoor/vehicle-insurance-fraud-detection?resource=download 
* **Литература** 
	- https://habr.com/ru/companies/otus/articles/782862/
	- https://paperswithcode.com/paper/interpretable-clustering-on-dynamic-graphs 
	- https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.53
* **Базовый алгоритм:** Базовый алгоритм решения задачи состоит в применении методов из пакетов sklearn, umap языка Python. Интерпретируемым алгоритмом классификации является дерево решений (sklearn.tree.DecisionTree).
* **Новизна:** Индустриальная задача
* **Авторы:** Старожилец Всеволод Михайлович

## Задача 142 (индустриальная)
* ** **Название:**  Классификация товаров по ОКПД2 кодам.
* **Описание проблемы:**  Требуется по краткому текстовому описанию товара классифицировать товар по кодам ОКПД2 классификатора. Предполагается исследование зависимости точности и полноты классификации от рассматриваемой глубины ОКПД2 классификатора (например, для двух первых чисел классификатора задача практически тривиальна). 
* **Данные:** Около 40% данных госзакупок из открытых источников за 2022 год. https://drive.google.com/file/d/1bg1wdpsxv797dB8RBWaq8JLp_qQp_uc0/view?usp=drive_link
* **Литература** Рарерs with 1) the formulation of the problem, 2) baseline and new results, 3) fast introduction https://www.kaggle.com/competitions/ndsc-beginner/overview
* **Базовый алгоритм:** Построение текстовых эмбеддингов каким-либо открытым переобученным пакетом. Например, spaCy (у него есть русский язык). Далее решается задача классификации.
* **Новизна:** Индустриальная задача
* **Авторы:** Старожилец Всеволод Михайлович

# Старые задачи

## Задача 112  (OLD)
* **Название:** Моделирование показания FMRI по видео показанному человеку
* **Описание проблемы:** Требуется построить модель зависимости показания датчиков FMRI и видеоряду, который в этот момент просматривает человек.
* **Данные:** Выборка для аппроксимации представлена в работе J. Berezutskay, в которой присутствуют различные типы параллельных сигналов.
* **Литература**
	- Berezutskaya J., et al Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film // Sci Data 9, 91, 2022.
	- [Код предшественников](https://github.com/intsystems/CreationOfIntelligentSystems_video_fMRI)
* **Базовый алгоритм:** Запуск кода на основе трансформер моделей.
* **Новизна:** Анализ зависимости между показаниями датчиков и восприятиям внешнего мира человеком. Требуется проверить гипотезу зависимости между данными, а также предложить метод апроксимации показаний FMRI по просматриваемому видеоряду.
* **Авторы:** Эксперт Грабовой Андрей

## Задача 113  (OLD)
* **Название:** Моделирование показания FMRI по звуковому ряду, который слышит человек 
* **Описание проблемы:** Требуется построить модель зависимости показания датчиков FMRI и звуковому сопровождению, который в этот момент прослушивает человек.
* **Данные:** Выборка для аппроксимации представлена в работе J. Berezutskay, в которой присутствуют различные типы параллельных сигналов.
* **Литература**
	- Berezutskaya J., et al Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film // Sci Data 9, 91, 2022.
	- [Код предшественников](https://github.com/intsystems/CreationOfIntelligentSystems_sound_fMRI)
* **Базовый алгоритм:** Запуск кода на основе трансформер моделей.
* **Новизна:** Анализ зависимости между показаниями датчиков и восприятиям внешнего мира человеком. Требуется проверить гипотезу зависимости между данными, а также предложить метод апроксимации показаний FMRI по прослушиваемому звуковому ряду.
* **Авторы:** Эксперт Грабовой Андрей

<!--## Задача 114   (OLD)
* **Название:** Моделирование динамики физических систем с помощью Physics-Informed Neural Networks
* **Задача** Решается задача выбора оптимальной модели предсказания динамики физической системы. Под динамикой системы понимается изменение во времени параметров системы. Нейронные сети не имеют априорных знаний о моделируемой системе, что не позволяет получить оптимальные параметры, учитывающие физические законы. Лагранжева нейронная сеть учитывает закон сохранения энергии при моделировании динамики. В данной работе предлагается Нётеровская агранжева нейронная сеть, учитывающая законы сохранения импульса и момента импульса в дополнение к закону сохранения энергии. Показано, что для данной задачи Нётеровская Лагранжева нейронная сеть является оптимальной среди полносвязной модели нейронной сети, нейронной сети с долговременной кратковременной памятью и Лагранжевой нейронной сетью. Сравнение моделирования проводилось на искусственно сгенерированных данных для системы двойного маятника, которая является простейшей хаотической системой. Результаты экспериментов подтверждают гипотезу, что внесение априорного знания о физике системы повышает качество модели.
* **Описание проблемы:** Породить набор сверток по имеющимся данным и выбрать лучшую с помощью методов снижения порядка и размерности пространств. 
* **Данные:** Биомедицинские данные акселерометра и гироскопа, океанические течения, движение барханов, воздушные потоки
* **Литература**
	- [Базовая работа содержит ссылки.](https://github.com/severilov/master-thesis/blob/main/doc/Severilov2022MasterThesis_rus.pdf)
* **Базовый алгоритм:** Нейросетевой, лагранжевы нейросети.
* **Решение:** Нетерова нейросеть. 
* **Новизна:** Предложенная сеть учитывает симметрию
* **Авторы:** Эксперты - Северилов, консультант - Панченко-->

<!--## Задача 116   (OLD)
* **Название:** Нейронные дифференциальные уравнения для моделирования физической активности – выбор и порождение математических моделей
* '''Задача:'''  Задача выбора оптимальной мат. модели как задача генетической оптимизации. Критерий оптимальности определяется в терминах точности, сложности и устойчивости модели. Сама процедура семплирования состоит из двух шагов: порождение новой структуры и отклонение этой структуры, если она не удовлетворяет критерию оптимальности. Требуется на данных 'маятникого' типа – акселоерометр, миограмма, пульсовая волна – выбрать оптимальную модель.
* '''Данные:''' : WISDM, своя коллекция биомедицинских данных
* '''Литература:''' [Neural CDE](https://bit.ly/NeuroCDE)
* '''Базовой алгоритм:''' Neuro ODE/CDE на двухслойной нейросети.
* '''Решение:''' Сейчас уже выполнен ряд экспериментов, где семплирования производится генетическим алгоритмом. Получены приемлемые результаты. Предлагается их проанализировать и улучшить. 
* '''Решение:''' Алгоритм порождения математических моделей в виде обыкновенных дифференциальных уравнений. Сравнение моделей и солверов на биомедицинских данных.
* **Авторы:** Эксперт – Стрижов, консультант - Эдуард Владимиров -->

<!--## Задача 119   (OLD)
* **Название** Анализ динамики многократного обучения 
* **Описание проблемы** Рассмотрим задачу многократного обучения с учителем, в которой обучающая выборка не фиксирована, а обновляется в зависимости от предсказаний обученной модели на тестовой выборке. Для процесса многократного обучения, предсказания и обновления выборки строим математическую модель и исследуем свойства этого процесса на основе построенной модели. Пусть f(x) - функция плотности распределения признаков, G - алгоритм обучения модели, формирования предсказаний на тестовой выборке и подмешивания предсказаний в обучающую выборку, в результате применения которого изменяется распределение признаков. Пусть задано пространство неотрицательных гладких функций F(x), интеграл которых на R^n равен единице Рассмотрим автономную (не зависит от времени явно) динамическую систему (есть выделенная переменная - номер шага, которая возрастает),  на шаге t и t+1 которой выполняется соотношение f_{t+1}(x) = G(f_{t})(x), где G(f) - оператор эволюции на пространстве указанных функций F и известна начальная функция f_0(x), Вообще говоря, G может быть произвольным оператором, не обязательно гладким и/или непрерывным.
    - **Вопрос 0.** Найти условия на оператор G, при которых образ G лежит в том же классе функций плотности распределений F. 
    В частности, должен ли G быть ограниченным, операторная норма ||G|| <= 1, для того, чтобы образ G(f) \in F также был функцией плотности распределения для любой f из F? Существует ли в пространстве F единица относительно оператора G и что будет единичной функцией f в таком F?
    - **Вопрос 1.** При каких условиях на G будет существовать такое t_0, что для всех t > t_0 хвост последовательности {f} будет ограничен?
    - **Вопрос 2.** При каких условиях оператор G будет иметь неподвижную точку?
* **Данные** В вычислительном эксперименте предлагается проверить существенность ограничения / значимость условий, при которых получен ответ на вопросы 0-2. Например, для задачи линейной регрессии и/или регрессии с многоуровневой полносвязной нейронной сетью при разных долях подмешиваемых в обучающую выборку предсказаниях на синтетических наборах данных.
* **Авторы** Ментор, эксперт - Хританков А.С., эксперт - Афанасьев А.П.
* **Литература**
    - Khritankov A., Hidden Feedback Loops in Machine Learning Systems: A Simulation Model and Preliminary Results, https://doi.org/10.1007/978-3-030-65854-0_5
    - Khritankov A.. Pilkevich A. Existence Conditions for Hidden Feedback Loops in Online Recommender Systems, https://doi.org/10.1007/978-3-030-91560-5_19
    - Каток А.Б., Хасселблат Б. Введение в современную теорию динамических систем.1999. 768 с. ISBN 5-88688-042-9.
    - Немыцкий В. В., Степанов В. В. Качественная теория дифференциальных уравнений, год изд.: 1974-->

<!--## Задача 122  (OLD)
* **Задача:** Снижение размерности пространства в задаче генеративного моделирования с помощью обратимых моделей.
* **Описание задачи:** Пример задачи генеративного моделирования - генерация изображений. Некоторые виды новых моделей, такие как нормализационные потоки или диффузионные модели, задают обратимые преобразования. Но при этом работают они в пространстве очень высокой размерности. Предлагается совместить 2 подхода: снижения размерности и генеративного моделирования. 
* **Данные:** Любой датасет изображений (MNIST/CIFAR10).
* **Новизна:** Понизив размерность, можно добиться существенного ускорения генеративных моделей, что позволит понизить сложность таких моделей.
* **Автор:** Роман Исаченко-->

<!--## Задача 123 (OLD)
* **Задача:** Анализ смещения распределения в задаче контрастного распределения.
* **Описание задачи:** Есть такая же задача, как обучение представлений (Representation learning). Один из самых популярных подходов для решения данной задачи контрастное обучение (Contrastive learning). При этом в данных, на которых мы учимся часто есть ошибки в разметке: false positive/false negative. Предлагается проанализировать различные способы устранения этих смещений, вызванных ошибками. А также исследовать свойства предложенных моделей.
* **Данные:** Любой датасет изображений (MNIST/CIFAR10).
* **Новизна:** Текущие модели очень чувствительны к ошибкам. Если получится учесть смещение в распределениях, многие методы ранжирования товаров сильно вырастут в качестве.
* **Автор:** Роман Исаченко-->

<!--## Задача 124 (OLD)
* **Задача:** Ускорение семлирования из диффузионных моделей с помощью состязательных сетей
* **Описание задачи:** Самая популярная генеративная модель на сегодняшний день - диффузионная модель. Главный ее недостаток - скорость семплдирования. Для семплирования 1 картинки нужно прогнать 1 нейросеть 100-1000 раз. Есть способы ускорения этого процесса. Один из таких способов - использование состязательных сетей. Предлагается развить данный метод и исследовать различные способы задания функционала для семплирования
* **Данные:** Любой датасет изображений (MNIST/CIFAR10).
* **Новизна:** Ускорив диффузионные модели, они станут еще более популярными и проще в использовании.
* **Автор:** Роман Исаченко-->

## Задача 126 (OLD)
* **Название:** Детекция изменения стиля машинной генерации
* **Описание проблемы:** Требуется предложить метод детекции 
* **Данные:** Выборка для аппроксимации представлена в работе J. Berezutskay, в которой присутствуют различные типы параллельных сигналов.
* **Литература:**
	- G. Gritsay, A. Grabovoy, Y. Chekhovich. Automatic Detection of Machine Generated Texts: Need More Tokens // Ivannikov Memorial Workshop (IVMEM), 2022.
	- M. Kuznetsov, A. Motrenko, R. Kuznetsova, V. Strijov. Methods for intrinsic plagiarism detection and author diarization // Working Notes of CLEF, 2016, 1609 : 912-919.
	- [Конкурс RuATD.](https://www.dialog-21.ru/en/dialogue-evaluation/competitions/dialogue-evaluation-2022/ruatd-2022/)
* **Базовый алгоритм:**
	- Использование результатов конкурса RuATD в качестве базовых моделей для классификации предложений.
	- Использовать метод из работы Kuznetsov et all.
* **Новизна:** Предложить метод детекции машиносгенерированных фрагментов в тексте используя методы изменения стиля написания.
* **Авторы:** Эксперт Грабовой Андрей

## Задача 129  (OLD)
* **Название:** Пространственно-временное прогнозирование с помощью сверточных сетей и тензорных разложений 
* **Описание проблемы:** Породить набор сверток по имеющимся данным и выбрать лучшую с помощью методов снижения порядка и размерности пространств. 
* **Данные:** Потребление и цена электроэнергии, океанические течения, движение барханов, воздушные потоки
* **Литература**
	- [http://irep.ntu.ac.uk/id/eprint/32719/1/PubSub10184_Sanei.pdf](Tensor-based Singular Spectrum Analysis for Automatic Scoring of Sleep EEG)
	- [https://ieeexplore.ieee.org/document/6661921](Tensor based singular spectrum analysis for nonstationary source separation)
* **Базовый алгоритм:** Гусеница, тензорная гусеница.
* **Решение:** Найти мультиериодический временной ряд, построить его тензорное представление, разложить в спектр, собрать, показать прогноз.
* **Новизна:** Показать, что мультилинейная модель является удобным способом построения сверток для измерений в пространстве и времени.
* **Авторы:** Эксперт - В. В. Стрижов, консультант - Надежда Алсаханова

<!--## Задача 130  (OLD)
* Название: Автоматическое выделение терминов для тематического моделирования
* Задача: Построить модель ATE (Automatic Term Extraction) для автоматического выделения словосочетаний, являющихся терминами предметной области, в текстах научных статей. Предполагается использовать эффективные методы выделения коллокаций (TopMine или более современные) и тематические модели для определения «тематичности» словосочетания. Модель должна обучаться без учителя (unsupervised).
* Данные: 
	- Коллекция научных статей в области машинного обучения.
        - Размеченные статьи с выделенными терминами для оценивания моделей.
* Литература:
    1. El-Kishky A., Song Y., Wang C., Voss C. R., Han J. Scalable topical phrase mining from text corpora // Proc. VLDB Endowment. _ 2014._ Vol. 8, no. 3._ Pp. 305_316.
    2. Воронцов К.В. "Вероятностное тематическое моделирование: теория, модели, алгоритмы и проект BigARTM" (http://www.machinelearning.ru/wiki/images/d/d5/Voron17survey-artm.pdf)
    3. Николай Шаталов. Методы обучения без учителя для автоматического выделения составных терминов в текстовых коллекциях. 2019. ВМК МГУ.
    4. Владимир Полушин. Тематические модели для ранжирования рекомендаций текстового контента. 2017. ВМК МГУ.
    5. Hanh Thi Hong Tran, Matej Martinc, Jaya Caporusso, Antoine Doucet, Senja Pollak. The Recent Advances in Automatic Term Extraction: A survey. 2023. https://arxiv.org/abs/2301.06767
* Базовые алгоритмы:
    • Метод поиска коллокаций TopMine 
    • Библиотека тематического моделирование BigARTM.
    • Современные методы на основе нейросетевых моделей языка
* Решение: Применение алгоритма поиска коллокаций TopMine с последующей фильтрацией по критерию тематичности. Подбор гиперпараметров тематической модели и критерия тематичности. Сравнение данного подхода с современными методами на основе нейросетевых моделей языка. 
* Новизна: Предшествующие исследования предлагаемого подхода показали хорошие результаты как по полноте, так и по вычислительной эффективности. Однако они до сих пор не сравнивались с нейросетевыми моделями. 
* **Авторы:** Полина Потапова, Воронцов К.В.-->

<!--## Задача 131  (OLD)
* **Название:** Итеративное улучшение тематической модели с обратной связью от пользователя
* **Задача:**  Тематическое моделирование широко используется в социо-гуманитарных исследованиях для понимания тематической структуры больших текстовых коллекций. Типичный сценарий использования предполагает, что пользователь оценивает темы как релевантные, нерелевантные и мусорные. Если количество мусорных тем слишком велико, то пользователь пытается построить другую модель. Задача в том, чтобы при каждом таком перестроении использовать пользовательскую разметку таким образом, чтобы релевантные темы сохранились, из нерелевантных и мусорных тем по возможности выделились новые релевантные, и мусорных тем стало как можно меньше. 
* **Данные:** В качестве данных подойдёт любая коллекция текстов на естественном языке, про которую известна тематическая структура (сколько примерно тем, сколько документов по разным темам). Например, можно взять коллекцию новостей Lenta, дамп Википедии, посты с Хабрахабра, 20 Newsgroups, Reuters, статьи с ПостНауки. 
Тематика коллекции должна быть интересна самому исследователю, чтобы была мотивация оценивать темы вручную.  
* **Литература:** *
*  Воронцов К.В. "Вероятностное тематическое моделирование: теория, модели, алгоритмы и проект BigARTM" (http://www.machinelearning.ru/wiki/images/d/d5/Voron17survey-artm.pdf)
* Alekseev V. et al. "TopicBank: Collection of coherent topics using multiple model training with their further use for topic model validation" (https://www.sciencedirect.com/science/article/pii/S0169023X21000483)
* **Решение:** Использование библиотеки тематического моделирование BigARTM. Использование регуляризаторов сглаживания и декоррелирования. Разработка способов инициализации при перестроении тематических моделей. Поиск готового инструмента или разработка простого, быстрого, удобного способа просмотра и разметки тем. 
* **Новизна:** Проблема неединственности и неустойчивости моделей до сих пор не имеет окончательного решения в вероятностном тематическом моделировании. Предлагаемое исследование является важным шагом к построению моделей с максимальным числом интерпретируемых тем, содержательно полезных с точки зрения гуманитарного исследования. 
* **Авторы:**  Василий Алексеев, Воронцов К.В.-->

## Задача 132  (OLD)
* **Название:** Ранжирование научных статей для полуавтоматического реферирования
* **Задача:** Построить модель ранжирования, которая принимает на входе подборку текстов научных статей и выдаёт на выходе последовательность их упоминания в реферате. 
* **Данные:**
        - В качестве обучающей выборки используются обзорные разделы (например, Introduction и Related Work) статей из коллекции S2ORC (81.1M англоязычных статей). Объект обучающей выборки – это последовательность ссылок на статьи из списка литературы, упоминаемые в обзорных разделах. Для каждого документа есть набор мета данных - год публикации, журнал, число цитирований, число цитирований автора и др. Также, имеется abstract и, возможно, полный текст статьи.
        - В качестве метрики используется Коэффициент ранговой корреляции Кендалла.
* **Литература:**
	- [Крыжановская С. Ю. «Технология полуавтоматической суммаризации тематических подборок научных статей»](http://www.machinelearning.ru/wiki/images/e/ed/Kryzhanovskaya22msc.pdf)
	- [Власов А. В.  «Методы полуавтоматической суммаризации подборок научных статей»](http://www.machinelearning.ru/wiki/images/6/6d/Vlasov2020MSThesis.pdf)
	- [Крыжановская С. Ю., Воронцов К. В. «Технология полуавтоматической суммаризации тематических подборок научных статей»](http://www.machinelearning.ru/wiki/images/f/ff/Idp22.pdf, стр. 371)
	- [S2ORC: The Semantic Scholar Open Research Corpus](https://aclanthology.org/2020.acl-main.447.pdf)
* **Базовые алгоритмы:**
	- Попарные (pair-wise) методы ранжирования
	- Градиентный бустинг
* **Решение:** Простейшим решением является ранжирование статей в хронологическом порядке, по году их публикации. Для решения задачи предлагается построить модель ранжирования на основе градиентного бустинга. В качестве признаков можно использовать год публикации, цитируемость статьи, цитируемость её авторов, семантическая близость публикации к обзору, к его локальному контексту, и т. д. 
* **Новизна:** Задача является первым этапом для полуавтоматического реферирования тематических подборок научных публикаций (machine aided human summarization, MAHS). После того, как сценарий реферата построен, система генерирует для каждой статьи фразы-подсказки, из которых пользователь выбирает фразы для продолжения своего реферата. 
* **Автор:** Крыжановская Светлана, Константин Воронцов

<!-- ## Задача 133  (OLD)
* **Название**: Модели диффузии в задаче генерации структуры молекулы с оптимальной энергией
* **Задача**: Для органической маленькой молекулы (количество атомов меньше 100), знание только топологии молекулярного графа недостаточно для получения пространственной структуры. Молекула может иметь множество возможных конфигураций (конформеров), каждая из которых соответствует локальному минимумуму потенциала. На практике, наибольший интерес представляю наиболее устойчивые конформеры, которые имеют минимальную энергию. Исследования последних лет показывают успехи применнения моделей диффузии для генерации молекулярных структур. Данный подход показывает передовые результаты в задаче генерации молекул и их конформеров для малого количества тяжелых атомов (QM9 dataset - до 9 тяжелых атомов в молекуле), а также оценке связывания молекулы и белка. Предлагается построить модель генерации конформеров с минимальной энергией для молекул большего размера.    
* **Данные**: Базовый датасет - QM9 
* **Литература**: 
1) Разные теоретические подходы к модели диффузии: https://arxiv.org/abs/2011.13456
2) Диффузия в генерации молекул: https://arxiv.org/abs/2203.17003
3) Диффузия в задаче связывания белка и молекулы: https://arxiv.org/abs/2210.01776
4) Дифузия в задаче генерации конформеров: https://arxiv.org/abs/2203.02923
5) Туториал по эквиваринтным нейронным сетям: https://arxiv.org/abs/2207.09453
* **Базовой алгоритм**: GeoDiff[4]. 
* **Решение**: Реализовать генерацию конформера аналогично DiffDock[3] для QM9 датасета. Проверить работоспособность модели для молекул большего размера. 
* **Новизна**: Новизна работы заключается в дизайне модели для генерации комформеров большого размера, имеющее большое практического значение. 
* **Автор:** Филипп Никитин -->

<!--## Задача 135 (OLD, will ask for a new problem)
* **Название**: Меры близости в задачах self-supervised learning
* **Задача**: Идея self-supervised learning состоит в решении искусственно подобранной задачи для получения полезных представлений по данным без разметки. Один из наиболее популярных подходов - использование contrastive learning, в ходе которого модель обучается минимизировать расстояние между представлениями аугментированных копий одного и того же объекта. Цель задачи - исследовать качество получаемых представлений в зависимости от выбора меры близости (similarity measure), используемой при обучении, и предложить свой вариант измерения расстояния
* **Данные**: CIFAR-100
* **Литература**: 
	- [Решение с использованием квадрата евклидова расстояния](https://arxiv.org/abs/2105.04906)
	- [Решение с использованием косинусного сходства](https://arxiv.org/abs/2011.10566)
	- [Решение, основанное на информационном принципе](https://arxiv.org/abs/2103.03230)
* **Базовой алгоритм**: VicReg, Barlow Twins, SimSiam
* **Решение**: Одним из вариантов расстояния, которое можно предложить, является аналог метрики Васерштейна, который позволил бы учитывать зависимости между признаками. 
* **Новизна**: Предложить новый способ определения меры близости, который был бы теоретически обоснован/способствовал получению представлений с заданными свойствами
* **Авторы**: Полина Барабанщикова-->

<!--## Задача 136   (OLD)
* **Title:** Stochastic Newton with Arbitrary Sampling
* **Problem:** We analyze second order methods solving Empirical Risk Minimization problem of the form min f(x) in R^d. Here x is a parameter vector of some Machine Learning model, f_i(x) is a loss function on i-th training point (a_i,b_i). Our desire to solve it using Newton-type method that requires access to only one data point per iteration. We investigate different sampling strategies of index i_k on iteration k. See description in PDF.
* **Dataset:** It is proposed to use open SVM library as a data for experimental part of the work.
* **References:**
	- Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates
        - Parallel coordinate descent methods for big data optimization
* **Base algorithm:** As a base method it is proposed to use Algorithm 1 from the paper Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates.
* **Solution:** Is is proposed to adjust existing sampling strategies from Parallel coordinate descent methods for big data optimization in this work.
* **Novelty:** In the literature of Second Order methods there are a few works on incremental methods. The idea is to analyze the existing method by applying different sampling strategies. It is known that the proper sampling strategies may improve the performance of a method.
* **Authors:** Islamov Rustem--> 

<!--## Задача 137 (OLD, new problem will appear)
* **Задача:** Binary Neural Networks. Lossless picture quality for binary neural networks in pixel-level tasks. (Бинарные сети в img-to-img задачах)
* **Авторы:** Илья Жариков-->

<!--## Задача 138 (OLD)
* **Задача:** Post Training Quantization. Flexible continuous modification for SOTA post training quantization methods to make them lossless.
* **Авторы:** Илья Жариков-->

## Задача 139 (OLD)
 * **Название**: Дистилляция моделей на многодоменных выборках.
 * **Задача**: Исследуется проблема понижения сложности аппроксимирующей модели при переносе на новые данные меньшей мощности.
 * **Данные**: Выборки MNIST, CIFAR-10, CIFAR-100, Amazon товары.
 * **Литература**: 
 - [Диплом: Камил Баязитов](https://github.com/kbayazitov/Distillation)
 * **Базовой алгоритм**: Базовое решение и эксперименты представлены в дипломе.
 * **Авторы:** Грабовой Андрей

## Задача 140 
* **Название**: Адаптация архитектуры модели глубокого обучения с контролем эксплуатационных характеристик
*  **Задача:** рассматривается задача адаптация структуры обученной модели глубокого обучения для ограниченных вычислителньых ресурсов. Предполагается, что полученная архитектура (или несколько архитектур) должны работать эффективно на нескольких типах вычислительных серверов (например, на разных моделях GPU или различных мобильных устройствах). Требуется предложить метод поиска модели, позволяющий контролировать её сложность учетом целевых эксплуатационных характеристик.
*  **Данные:** MNIST, CIFAR
*  **Литература:**
    * [Гребенькова О.С., Бахтеев О., Стрижов В.В. Вариационная оптимизация модели глубокого обучения с контролем сложности // Информатика и ее применения, 2021, 15(2). PDF](https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=ia&paperid=710&option_lang=rus)
    * [Yakovlev K. D. et al. Neural Architecture Search with Structure Complexity Control //Recent Trends in Analysis of Images, Social Networks and Texts: 10th International Conference, AIST 2021, Tbilisi, Georgia, December 16–18, 2021, Revised Selected Papers. – Cham : Springer International Publishing, 2022. – С. 207-219.](https://yahootechpulse.easychair.org/publications/preprint_download/H5MC)
    * [FBNet: выбор архитектуры модели с учетом целевых характеристик](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_FBNet_Hardware-Aware_Efficient_ConvNet_Design_via_Differentiable_Neural_Architecture_Search_CVPR_2019_paper.pdf)  
*  **Базовый алгоритм:** FBNet и случайный поиск подструктуры модели
*  **Решение:** Предлагаемый метод заключается в использовании дифференцируемого алгоритма поиска архитектуры нейросети(FBNet) с контролем сложности параметров при помощи гиперсети. Гиперсеть - это модель, порождающая структуру модели в зависимости от входных параметров. В качестве параметров гиперсети предлагается использовать нормированное время работы базовых операций на целевых вычислительных ресурсах. Таким образом, полученная модель позволит адаптировать архитектуру модели для произвольного устройства.
Новизна: предложенный метод позволяет контролировать сложность модели, в процессе поиска архитектуры без дополнительных эвристик.
*  **Авторы:** Константин Яковлев, Олег Бахтеев
*  
<!--## Problem 141 (OLD)
* __Title__: Metric analysis of deep network space parameters
* __Problem__: The structure of a neural work is exhaustive. The dimensionality of the parameter space should be reduced. The autoencoder in the subject of the investigation. Due to the continuous-time nature of the data, we analyze several types of autoencoders. We reconstruct spatial-time data, minimizing the error. 
* __Data__: 
	* Synthetic data sine for 2D visualizaion of the parameter distributions
	* Accelerometer quasiperiodic data
	* Limb movement quasiperiodic data (if any)
	* Video periodic data (cartoon, walking persona)
	* Video, fMRI, ECoG from the s41597-022-01173-0 
* __References__: 
	* [SSA and Hankel matrix construction](http://strijov.com/papers/Grabovoy2019QuasiPeriodicTimeSeries.pdf) or in [wiki](https://en.wikipedia.org/wiki/Singular_spectrum_analysis)
	* [Open multimodal iEEG-fMRI dataset from naturalistic stimulation](https://www.nature.com/articles/s41597-022-01173-0)
	* [Variational autoencoders to estimate parameters](https://arxiv.org/pdf/1606.05908.pdf)
	* RNN in the [5G book](https://arxiv.org/abs/2104.13478)
	* [Neural CDE](https://bit.ly/NeuroCDE)
* __Baseline__: RNN-like variational autoencoder in the criteria: error vs. complexity (number of parameters)
* __Roadmap__:
	* Prepare data so that the reconstruction work on a basic model (like SSA)
	* Estimate expectation and covariance of parameters (using VAE or else, to be discussed)
	* Reduce dimensionality, plot the error/complexity, plot the covariance
	* Run RNN-like model, plot
	* Assign the expectation and covariation matrix to each neuron of the model
	* Plot the parameter space regarding covariance as its metric tensor (end of minimum part)
	* Suggest a dimensionality reduction algorithm (naive part)
	* Run Neuro ODE/CDE model and plot the parameter space
	* Analyse the data distribution as the normalized flow 
	* Suggest the parameter space modification in terms of  the normalized flow (paradoxical part, diffusion model is needed)
	* Compare all models according to the criterion error/complexity (max part)
	* Construct the decoder model for any pair of data like fMRI-ECoG tensor and neuro CDE (supermax part)
* __Proposed solution__: description of the idea to implement in the project
* __Novelty__: Continous-time models are supposed to be simple due to their periodic nature. Since they approximate the vector fields, these models are universal. The model selection for the continuous time is not considered now, but at the time, it is acute for wearable multimedia devices for metaverse and augmented reality. 
* __Supergoal__ To join two encoders in a signal decoding model to reveal the connection between video and fMRI, between fMRI and ECoG.
* __Authors__: Expert-->
   
## Problem template (EN)
## Problem 101
* __Title__: Title
* __Problem__: Problem description
* __Data__: Data description
* __Reference__: Links to the literature
* __Baseline__: baseline description
* __Proposed solution__: description of the idea to implement in the project
* __Novelty__: why the task is good and what does it bring to science?  (for editorial board and reviewers)
* __Authors__: supervisors, consultants, experts

## Шаблон задачи (RU)
## Задача 101
* __Название__: Название, под которым статья подается в журнал. 
* __Задача__: Описание или постановка задачи. Желательна постановка в виде задачи оптимизации (в формате argmin). Также возможна ссылка на классическую постановку задачи. 
* __Данные__: Краткое описание данных, используемых в вычислительном эксперименте, и ссылка на выборку. 
* __Литература__: Список научных работ, дополненный 1) формулировкой решаемой задачи, 2) ссылками на новые результаты, 3) основной информацией об исследуемой проблеме. 
* __Базовой алгоритм__: Ссылка на алгоритм, с которым проводится сравнение или на ближайшую по теме работу. 
* __Решение__: Предлагаемое решение задачи и способы проведения исследования. Способы представления и визуализации данных и проведения анализа ошибок, анализа качества алгоритма. 
* __Новизна__: Обоснование новизны и значимости идей (для редколлегии и рецензентов журнала). 
